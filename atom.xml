<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jack Wang Blog</title>
  <subtitle>Goals determine what you are going to be</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.yfshare.vip/"/>
  <updated>2019-01-28T14:59:28.049Z</updated>
  <id>http://www.yfshare.vip/</id>
  
  <author>
    <name>Jack Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s集群水平扩展(HPA)</title>
    <link href="http://www.yfshare.vip/2019/01/28/k8s%E9%9B%86%E7%BE%A4%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95-HPA/"/>
    <id>http://www.yfshare.vip/2019/01/28/k8s集群水平扩展-HPA/</id>
    <published>2019-01-28T14:51:38.000Z</published>
    <updated>2019-01-28T14:59:28.049Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Horizontal Pod Autoscaling，简称HPA，是Kubernetes中实现POD水平自动伸缩的功能。<br><a id="more"></a></p>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>K8S集群可以通过Replication Controller的scale机制完成服务的扩容或缩容，实现具有伸缩性的服务。<br>K8S自动伸缩分为：  </p>
<ul>
<li>sacle手动伸缩。见<a href="http://www.yfshare.vip/2018/05/28/k8s%E6%BB%9A%E5%8A%A8%E5%8D%87%E7%BA%A7-RollingUpdate/">k8s滚动升级(RollingUpdate)</a>  </li>
<li>autoscale自动伸缩，见<a href="http://www.yfshare.vip/2019/01/28/k8s%E9%9B%86%E7%BE%A4%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95-HPA/#autoscaling-v2beta2">HPA</a>。  </li>
</ul>
<p>自动扩展主要分为两种：  </p>
<ul>
<li>水平扩展(scale out)，针对于实例数目的增减。  </li>
<li>垂直扩展(scal up)，即单个实例可以使用的资源的增减, 比如增加cpu和增大内存。  </li>
</ul>
<p>HPA属于前者。它可以根据CPU使用率或应用自定义metrics自动扩展Pod数量(支持 replication controller、deployment 和 replica set)。<br><img src="https://note.youdao.com/yws/api/personal/file/F521C66C09BD4EA0A0D7B1C02AF49CE9?method=download&amp;shareKey=fb8d078ed30b70f326f6d6e37c1b3cc3" alt="HPA架构">  </p>
<p>获取metrics的两种方式：  </p>
<ul>
<li>Heapster：heapster提供metrics服务，但是在v1(autoscaling/v1)版本中仅支持以CPU作为扩展度量指标。而其他比如：内存，网络流量，qps等目前处于beta阶段(autoscaling/v2beta1)。  </li>
<li>Cousom：同样处于beta阶段(autoscaling/v2beta1)，但是涉及到自定义的REST API的开发，复杂度会大一些，并且当需要从自定义的监控中获取数据时，只能设置绝对值，无法设置使用率。  </li>
</ul>
<h4 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h4><ul>
<li>创建HPA资源，设定目标CPU使用率限额，以及最大/最小实例数，一定要设置Pod的资源限制参数: <code>request</code>，否则HPA不会工作。  </li>
<li>控制管理器每隔30s(在<code>kube-controller-manager.service</code>中可以通过<code>–horizontal-pod-autoscaler-sync-period</code>修改)查询metrics的资源使用情况。  </li>
<li>然后与创建时设定的值和指标做对比(平均值之和/限额)，求出目标调整的实例个数。  </li>
<li>目标调整的实例数不能超过第一条中设定的最大/最小实例数。如果没有超过，则扩容；超过，则扩容至最大的实例个数。  </li>
<li>重复第2-4步。  </li>
</ul>
<h4 id="自动伸缩算法"><a href="#自动伸缩算法" class="headerlink" title="自动伸缩算法"></a>自动伸缩算法</h4><p>HPA Controller会通过调整副本数量使得CPU使用率尽量向期望值靠近，而且不是完全相等。另官方考虑到自动扩展的决策可能需要一段时间才会生效：例如当pod所需要的CPU负荷过大，从而在创建一个新pod的过程中，系统的CPU使用量可能会同样在有一个攀升的过程。所以在每一次作出决策后的一段时间内，将不再进行扩展决策。对于扩容而言，这个时间段为3分钟，缩容为5分钟(可以通过<code>--horizontal-pod-autoscaler-downscale-delay</code>，<code>--horizontal-pod-autoscaler-upscale-delay</code>进行调整)。  </p>
<ul>
<li>HPA Controller中有一个tolerance（容忍力）的概念，它允许一定范围内的使用量的不稳定，现在默认为0.1，这也是出于维护系统稳定性的考虑。例如设定HPA调度策略为cpu使用率高于50%触发扩容，那么只有当使用率大于55%或者小于45%才会触发伸缩活动，HPA会尽力把Pod的使用率控制在这个范围之间。  </li>
<li>具体的每次扩容或者缩容的多少Pod的算法为：Ceil(前采集到的使用率 / 用户自定义的使用率) * Pod数量)。  </li>
<li>每次最大扩容pod数量不会超过当前副本数量的2倍。  </li>
</ul>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><table>
<thead>
<tr>
<th>角色</th>
<th>IP</th>
<th>操作系统版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>192.168.1.201</td>
<td>centos 7.4</td>
</tr>
<tr>
<td>etcd1</td>
<td>192.168.1.201</td>
<td>centos 7.4</td>
</tr>
<tr>
<td>etcd2</td>
<td>192.168.1.202</td>
<td>centos 7.4</td>
</tr>
<tr>
<td>etcd3</td>
<td>192.168.1.203</td>
<td>centos 7.4</td>
</tr>
<tr>
<td>node1</td>
<td>192.168.1.204</td>
<td>centos 7.4</td>
</tr>
<tr>
<td>node2</td>
<td>192.168.1.205</td>
<td>centos 7.4</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>环境</th>
<th>软件版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>kubectl server</td>
<td>v1.9.2</td>
</tr>
<tr>
<td>kubectl client</td>
<td>v1.9.2</td>
</tr>
<tr>
<td>Go</td>
<td>go1.9.2</td>
</tr>
<tr>
<td>etcdctl</td>
<td>3.2.15</td>
</tr>
<tr>
<td>etcd</td>
<td>3.2.15</td>
</tr>
<tr>
<td>flanneld</td>
<td>v0.10.0</td>
</tr>
<tr>
<td>cfssl</td>
<td>1.2.0</td>
</tr>
<tr>
<td>docker</td>
<td>18.09.1-beta1</td>
</tr>
</tbody>
</table>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl cluster-info</span></div><div class="line">Kubernetes master is running at https://192.168.1.201:6443</div><div class="line">Heapster is running at https://192.168.1.201:6443/api/v1/namespaces/kube-system/services/heapster/proxy</div><div class="line">monitoring-grafana is running at https://192.168.1.201:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</div><div class="line">monitoring-influxdb is running at https://192.168.1.201:6443/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy</div><div class="line">To further debug and diagnose cluster problems, use <span class="string">'kubectl cluster-info dump'</span>.</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment">#  kubectl -s http://192.168.1.201:8080 get componentstatuses </span></div><div class="line">NAME                 STATUS    MESSAGE              ERROR</div><div class="line">controller-manager   Healthy   ok                   </div><div class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">scheduler            Healthy   ok                   </div><div class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get nodes</span></div><div class="line">NAME            STATUS    ROLES     AGE       VERSION</div><div class="line">192.168.1.204   Ready     &lt;none&gt;    21h       v1.9.2</div><div class="line">192.168.1.205   Ready     &lt;none&gt;    21h       v1.9.2</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="部署HPA"><a href="#部署HPA" class="headerlink" title="部署HPA"></a>部署HPA</h4><p>先准备一套K8S集群环境，<a href="http://www.yfshare.vip/2018/02/23/%E9%83%A8%E7%BD%B2TLS-k8s/">环境部署</a>略。  </p>
<h5 id="创建Deployment-POD应用nginx"><a href="#创建Deployment-POD应用nginx" class="headerlink" title="创建Deployment POD应用nginx"></a>创建Deployment POD应用nginx</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># cat nginx.yml </span></div><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: nginx</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: nginx-hpa</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: nginx</div><div class="line">        image: nginx:latest</div><div class="line">        ports:</div><div class="line">        - containerPort: 80</div><div class="line">          name: http</div><div class="line">          protocol: TCP</div><div class="line">        resources:</div><div class="line">          requests:</div><div class="line">            cpu: 0.01</div><div class="line">            memory: 25Mi</div><div class="line">          limits:</div><div class="line">            cpu: 0.05</div><div class="line">            memory: 60Mi</div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: nginx</div><div class="line">  labels:</div><div class="line">    app: nginx-hpa</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    app: nginx-hpa</div><div class="line">  <span class="built_in">type</span>: NodePort</div><div class="line">  ports:</div><div class="line">  - name: http</div><div class="line">    protocol: TCP</div><div class="line">    port: 80</div><div class="line">    targetPort: 80</div><div class="line">    nodePort: 30080</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl apply -f nginx.yml</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide</span></div><div class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-5dcf548595-bk9cr   1/1       Running   1          14h       172.30.94.2   192.168.1.205</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="创建nginx应用的HPA"><a href="#创建nginx应用的HPA" class="headerlink" title="创建nginx应用的HPA"></a>创建nginx应用的HPA</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># cat nginx-hpa-cpu.yml </span></div><div class="line">apiVersion: autoscaling/v1</div><div class="line">kind: HorizontalPodAutoscaler</div><div class="line">metadata:</div><div class="line">  name: nginx-hpa</div><div class="line">spec:</div><div class="line">  scaleTargetRef:</div><div class="line">    apiVersion: extensions/v1beta1</div><div class="line">    kind: Deployment</div><div class="line">    name: nginx</div><div class="line">  minReplicas: 1</div><div class="line">  maxReplicas: 5</div><div class="line">  targetCPUUtilizationPercentage: 70</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl apply -f nginx-hpa-cpu.yml</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   &lt;unknown&gt; / 70%   1         5         1          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h6 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h6><p>这时发现nginx-hpa获取不到当前的CPU情况（TARGETS）。等待几分钟后执行<code>kubectl describe hpa</code>发现HPA报错信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl describe hpa</span></div><div class="line">Name:                                                  nginx-hpa</div><div class="line">Namespace:                                             default</div><div class="line">Labels:                                                &lt;none&gt;</div><div class="line">Annotations:                                           kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:<span class="string">"au</span></div><div class="line">toscaling/v1",<span class="string">"kind"</span>:<span class="string">"HorizontalPodAutoscaler"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"nginx-hpa"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"maxReplic...</span></div><div class="line">CreationTimestamp:                                     Sat, 26 Jan 2019 22:23:08 +0800</div><div class="line">Reference:                                             Deployment/nginx</div><div class="line">Metrics:                                               ( current / target )</div><div class="line">  resource cpu on pods  (as a percentage of request):  &lt;unknown&gt; / 70%</div><div class="line">Min replicas:                                          1</div><div class="line">Max replicas:                                          5</div><div class="line">Conditions:</div><div class="line">  Type           Status  Reason                   Message</div><div class="line">  ----           ------  ------                   -------</div><div class="line">  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale</div><div class="line">  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)</div><div class="line">Events:</div><div class="line">  Type     Reason                        Age               From                       Message</div><div class="line">  ----     ------                        ----              ----                       -------</div><div class="line">  Warning  FailedComputeMetricsReplicas  1m (x12 over 3m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)</div><div class="line">  Warning  FailedGetResourceMetric       1m (x13 over 3m) horizontal-pod-autoscaler  unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)</div><div class="line">[root@master ~]#</div></pre></td></tr></table></figure></p>
<p>大概意思是HPA无法通过API获取到metrics值。<br>解决办法：<br>在<code>/etc/systemd/system/kube-controller-manager.service</code>配置文件中新增<code>--horizontal-pod-autoscaler-use-rest-clients=false</code>配置参数。然后重启kube-controller-manager服务即可。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">kube-controller-manager<span class="string">'s parameter --horizontal-pod-autoscaler-use-rest-clients in k8s 1.9.0 default value is true , while in k8s 1.8.x is false</span></div><div class="line">change it to false and it works.</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># cat /etc/systemd/system/kube-controller-manager.service </span></div><div class="line">[Unit]</div><div class="line">Description=Kubernetes Controller Manager</div><div class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"></div><div class="line">[Service]</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/k8s/bin/kube-controller-manager \</div><div class="line">  --address=127.0.0.1 \</div><div class="line">  --master=http://192.168.1.201:8080 \</div><div class="line">  --allocate-node-cidrs=<span class="literal">true</span> \</div><div class="line">  --service-cluster-ip-range=172.16.0.0/16 \</div><div class="line">  --cluster-cidr=172.30.0.0/16 \</div><div class="line">  --cluster-name=kubernetes \</div><div class="line">  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \</div><div class="line">  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</div><div class="line">  --root-ca-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --leader-elect=<span class="literal">true</span> \</div><div class="line">  --horizontal-pod-autoscaler-use-rest-clients=<span class="literal">false</span> \</div><div class="line">  --v=2</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># systemctl daemon-reload</span></div><div class="line">[root@master ~]<span class="comment"># systemctl restart kube-controller-manager</span></div></pre></td></tr></table></figure>
<h6 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h6><p>配置并重启完成kube-controller-manager服务后，执行<code>kubectl delete -f nginx-hpa-cpu.yml</code>和<code>kubectl apply -f nginx-hpa-cpu.yml</code>重新创建服务后，发现出现新的错误，信息如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl describe hpa</span></div><div class="line">Name:                                                  nginx-hpa</div><div class="line">Namespace:                                             default</div><div class="line">Labels:                                                &lt;none&gt;</div><div class="line">Annotations:                                           kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:<span class="string">"au</span></div><div class="line">scaling/v1",<span class="string">"kind"</span>:<span class="string">"HorizontalPodAutoscaler"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"nginx-hpa"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec&#123;"</span>maxRepl...</div><div class="line">CreationTimestamp:                                     Sun, 27 Jan 2019 00:18:02 +0800</div><div class="line">Reference:                                             Deployment/nginx</div><div class="line">Metrics:                                               ( current / target )</div><div class="line">  resource cpu on pods  (as a percentage of request):  &lt;unknown&gt; / 70%</div><div class="line">M<span class="keyword">in</span> replicas:                                          1</div><div class="line">Max replicas:                                          5</div><div class="line">Conditions:</div><div class="line">  Type           Status  Reason                   Message</div><div class="line">  ----           ------  ------                   -------</div><div class="line">  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target<span class="string">'s current scale</span></div><div class="line">  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: unable to get metrics r resource cpu: failed to get pod resource metrics: an error on the server ("Error: 'dial tcp 172.30.9.4:8082: getsockoptconnection timed out<span class="string">'\nTrying to reach: '</span>http://172.30.9.4:8082/apis/metrics/v1alpha1/namespaces/default/pods?labelSelect=app%3Dnginx-hpa<span class="string">'") has prevented the request from succeeding (get services http:heapster:)</span></div><div class="line">Events:</div><div class="line">  Type     Reason                        Age               From                       Message</div><div class="line">  ----     ------                        ----              ----                       -------</div><div class="line">  Warning  FailedUpdateStatus            2m                horizontal-pod-autoscaler  Operation cannot be fulfilled on hozontalpodautoscalers.autoscaling "nginx-hpa": the object has been modified; please apply your changes to the latest versi and try again</div><div class="line">  Warning  FailedGetResourceMetric       24s (x3 over 4m)  horizontal-pod-autoscaler  unable to get metrics for resource u: failed to get pod resource metrics: an error on the server ("Error: 'dial tcp 172.30.9.4:8082: getsockopt: connection med out<span class="string">'\nTrying to reach: '</span>http://172.30.9.4:8082/apis/metrics/v1alpha1/namespaces/default/pods?labelSelector=app%3Dnginhpa<span class="string">'") has prevented the request from succeeding (get services http:heapster:)</span></div><div class="line">  Warning  FailedComputeMetricsReplicas  24s (x3 over 4m)  horizontal-pod-autoscaler  failed to get cpu utilization: unab to get metrics for resource cpu: failed to get pod resource metrics: an error on the server ("Error: 'dial tcp 172.30.9.4:8082: getsockopt: connection timed out<span class="string">'\nTrying to reach: '</span>http://172.30.9.4:8082/apis/metrics/v1alpha1/namespaces/defaulpods?labelSelector=app%3Dnginx-hpa<span class="string">'") has prevented the request from succeeding (get services http:heapster:)</span></div><div class="line">[root@master ~]#</div></pre></td></tr></table></figure></p>
<p>意思是HPA无法连接heapster服务。于是检查heapster服务是否异常。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide -n kube-system</span></div><div class="line">NAME                                   READY     STATUS    RESTARTS   AGE         IP           NODE</div><div class="line">heapster-6d5c495969-2rgcr              1/1       Running   2          20h         172.30.9.4   192.168.1.204</div><div class="line">kubernetes-dashboard-cbbf9945c-bkvbk   1/1       Running   2          20h         172.30.9.3   192.168.1.204</div><div class="line">monitoring-grafana-67d68bf9c6-zv928    1/1       Running   2          20h         172.30.9.2   192.168.1.204</div><div class="line">monitoring-influxdb-7c4c46745f-kbxgb   1/1       Running   0          &lt;invalid&gt;   172.30.9.5   192.168.1.204</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>访问kube-dashboard发现POD是可以通过heapster获取到CPU内存的信息的。如下，说明heapster工作正常。<br><img src="https://note.youdao.com/yws/api/personal/file/D33884AB9908421C9276A7C46836CF58?method=download&amp;shareKey=24be8fb4d86c0390d173cac851934a4c" alt="kube-dashboard"><br>于是到node节点手动curl访问连接异常的URL。经测试在node1节点上访问正常。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># curl 'http://172.30.9.4:8082/apis/metrics/v1alpha1/namespaces/default/pods?labelSelector=app%3Dnginx-hpa'</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"metadata"</span>: &#123;&#125;,</div><div class="line">  <span class="string">"items"</span>: [</div><div class="line">   &#123;</div><div class="line">    <span class="string">"metadata"</span>: &#123;</div><div class="line">     <span class="string">"name"</span>: <span class="string">"nginx-5dcf548595-bk9cr"</span>,</div><div class="line">     <span class="string">"namespace"</span>: <span class="string">"default"</span>,</div><div class="line">     <span class="string">"creationTimestamp"</span>: <span class="string">"2019-01-27T07:29:43Z"</span></div><div class="line">    &#125;,</div><div class="line">    <span class="string">"timestamp"</span>: <span class="string">"2019-01-27T07:29:00Z"</span>,</div><div class="line">    <span class="string">"window"</span>: <span class="string">"1m0s"</span>,</div><div class="line">    <span class="string">"containers"</span>: [</div><div class="line">     &#123;</div><div class="line">      <span class="string">"name"</span>: <span class="string">"nginx"</span>,</div><div class="line">      <span class="string">"usage"</span>: &#123;</div><div class="line">       <span class="string">"cpu"</span>: <span class="string">"0"</span>,</div><div class="line">       <span class="string">"memory"</span>: <span class="string">"2820Ki"</span></div><div class="line">      &#125;</div><div class="line">     &#125;</div><div class="line">    ]</div><div class="line">   &#125;</div><div class="line">  ]</div><div class="line"> &#125;</div><div class="line"> [root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>于是到kube-master上访问测试，发现HPA无法访问到heapster。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># curl 'http://172.30.9.4:8082/apis/metrics/v1alpha1/namespaces/default/pods?labelSelector=app%3Dnginx-hpa'</span></div><div class="line">curl: (7) Failed connect to 172.30.9.4:8082; Connection timed out</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>接下来我们来测试下网络情况，发现kube-master无法Ping通heapster的POD地址。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># ping 172.30.9.4</span></div><div class="line">PING 172.30.9.4 (172.30.9.4) 56(84) bytes of data.</div><div class="line">^C</div><div class="line">--- 172.30.9.4 ping statistics ---</div><div class="line">2 packets transmitted, 0 received, 100% packet loss, time 1002ms</div><div class="line"></div><div class="line">[root@master ~]<span class="comment"># telnet 172.30.9.4 8082</span></div><div class="line">Trying 172.30.9.4...</div><div class="line">telnet: connect to address 172.30.9.4: Connection timed out</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>测试发现是网络不通导致的。解决办法是在kube-master上安装flannel网络。<br>如果flannel网络的IP地址丢失，重启flannel网卡<code>systemctl restart flanneld</code>即可解决。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># ip a</span></div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 ::1/128 scope host </div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</div><div class="line">    link/ether 00:0c:29:48:f6:1d brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.201/24 brd 192.168.1.255 scope global ens33</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 fe80::22d8:9dda:6705:ec09/64 scope link </div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </div><div class="line">    link/ether 6e:05:c0:9c:34:3f brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 172.30.13.0/32 scope global flannel.1</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet6 fe80::6c05:c0ff:fe9c:343f/64 scope link </div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>再测试下kube-master到heapster POD的网络情况：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># ping 172.30.9.4 -c 4</span></div><div class="line">PING 172.30.9.4 (172.30.9.4) 56(84) bytes of data.</div><div class="line">64 bytes from 172.30.9.4: icmp_seq=1 ttl=63 time=2.15 ms</div><div class="line">64 bytes from 172.30.9.4: icmp_seq=2 ttl=63 time=1.27 ms</div><div class="line">64 bytes from 172.30.9.4: icmp_seq=3 ttl=63 time=1.30 ms</div><div class="line">64 bytes from 172.30.9.4: icmp_seq=4 ttl=63 time=1.66 ms</div><div class="line"></div><div class="line">--- 172.30.9.4 ping statistics ---</div><div class="line">4 packets transmitted, 4 received, 0% packet loss, time 3003ms</div><div class="line">rtt min/avg/max/mdev = 1.277/1.599/2.150/0.354 ms</div><div class="line">[root@master ~]<span class="comment"># telnet 172.30.9.4 8082</span></div><div class="line">Trying 172.30.9.4...</div><div class="line">telnet: connect to address 172.30.9.4: Connection refused</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>重新导入nginx-hpa-cpu.yml文件，然后等待几分钟…<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl delete -f nginx-hpa-cpu.yml </span></div><div class="line">horizontalpodautoscaler <span class="string">"nginx-hpa"</span> deleted</div><div class="line">[root@localhost ~]<span class="comment"># </span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl apply -f nginx-hpa-cpu.yml </span></div><div class="line">horizontalpodautoscaler <span class="string">"nginx-hpa"</span> created</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>OK，HPA连接heapster成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   0% / 70%   1         5         1          39s</div><div class="line">[root@localhost ~]<span class="comment"># </span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl describe hpa</span></div><div class="line">Name:                                                  nginx-hpa</div><div class="line">Namespace:                                             default</div><div class="line">Labels:                                                &lt;none&gt;</div><div class="line">Annotations:                                           kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:<span class="string">"au</span></div><div class="line">toscaling/v1",<span class="string">"kind"</span>:<span class="string">"HorizontalPodAutoscaler"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"nginx-hpa"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"maxRepl...</span></div><div class="line">CreationTimestamp:                                     Sun, 27 Jan 2019 01:04:25 +0800</div><div class="line">Reference:                                             Deployment/nginx</div><div class="line">Metrics:                                               ( current / target )</div><div class="line">  resource cpu on pods  (as a percentage of request):  0% (0) / 70%</div><div class="line">Min replicas:                                          1</div><div class="line">Max replicas:                                          5</div><div class="line">Conditions:</div><div class="line">  Type            Status  Reason            Message</div><div class="line">  ----            ------  ------            -------</div><div class="line">  AbleToScale     True    ReadyForNewScale  the last scale time was sufficiently old as to warrant a new scale</div><div class="line">  ScalingActive   True    ValidMetricFound  the HPA was able to succesfully calculate a replica count from cpu resource utilization (percentage of request)</div><div class="line">  ScalingLimited  True    TooFewReplicas    the desired replica count is increasing faster than the maximum scale rate</div><div class="line">Events:           &lt;none&gt;</div><div class="line">[root@localhost ~]#</div></pre></td></tr></table></figure></p>
<h4 id="HPA测试"><a href="#HPA测试" class="headerlink" title="HPA测试"></a>HPA测试</h4><p>截至目前，HPA支持的API版本有三个。分别是<code>autoscaling/v1</code>，<code>autoscaling/v2beta1</code>，<code>autoscaling/v2beta2</code>。其中<code>autoscaling/v1</code>只支持CPU一种伸缩指标；在<code>autoscaling/v2beta1</code>中增加支持custom metrics；在<code>autoscaling/v2beta2</code>中增加支持external metrics。<br>详细说明参考：  </p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="external">官方接口文档</a>  </li>
<li><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#appendix-horizontal-pod-autoscaler-status-conditions" target="_blank" rel="external">autoscaling/v2beta2示例</a>  </li>
<li><a href="https://yq.aliyun.com/articles/672398?spm=a2c4e.11153940.blogcont673889.15.4a0226a9rLnJ2p" target="_blank" rel="external">Kubernetes 弹性伸缩全场景解析</a>  </li>
</ul>
<p>官方说明，在k8s 1.11版本，HPA将不再从heapster上获取指标。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">The HorizontalPodAutoscaler normally fetches metrics from a series of aggregated APIs (metrics.k8s.io, custom.metrics.k8s.io, and external.metrics.k8s.io). The metrics.k8s.io API is usually provided by metrics-server, <span class="built_in">which</span> needs to be launched separately. See metrics-server <span class="keyword">for</span> instructions. The HorizontalPodAutoscaler can also fetch metrics directly from Heapster.</div><div class="line"></div><div class="line">Note:</div><div class="line">FEATURE STATE: Kubernetes 1.11 deprecated</div><div class="line">Fetching metrics from Heapster is deprecated as of Kubernetes 1.11.</div></pre></td></tr></table></figure></p>
<h5 id="autoscaling-v1"><a href="#autoscaling-v1" class="headerlink" title="autoscaling/v1"></a>autoscaling/v1</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># cat nginx-hpa-cpu.yml </span></div><div class="line">apiVersion: autoscaling/v1</div><div class="line">kind: HorizontalPodAutoscaler</div><div class="line">metadata:</div><div class="line">  name: nginx-hpa</div><div class="line">spec:</div><div class="line">  scaleTargetRef:</div><div class="line">    apiVersion: extensions/v1beta1</div><div class="line">    kind: Deployment</div><div class="line">    name: nginx</div><div class="line">  minReplicas: 1</div><div class="line">  maxReplicas: 5</div><div class="line">  targetCPUUtilizationPercentage: 70</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>这里只针对CPU的HPA 压力测试。<br>压测命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># cat test.sh </span></div><div class="line"><span class="keyword">while</span> <span class="literal">true</span></div><div class="line"><span class="keyword">do</span></div><div class="line">	wget -q -O- http://192.168.1.204:30080</div><div class="line"><span class="keyword">done</span></div><div class="line">[root@node1 ~]<span class="comment"># sh test.sh</span></div></pre></td></tr></table></figure></p>
<p>观察HPA当前负载和POD的情况<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   0% / 70%   1         5         1          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS     MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   14% / 70%   1         5         1          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>当负载飙升时，HPA会按照定义的规则开始创建新的POD副本(定义POD的CPU阈值为70%)。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS      MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   180% / 70%   1         5         3          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide</span></div><div class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-5dcf548595-bk9cr   1/1       Running   1          15h       172.30.94.2   192.168.1.205</div><div class="line">nginx-5dcf548595-pdndb   1/1       Running   0          1m        172.30.94.4   192.168.1.205</div><div class="line">nginx-5dcf548595-z9d6h   1/1       Running   0          1m        172.30.94.3   192.168.1.205</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>继续压测，会发现POD副本数量继续增加（REPLICAS从3到5）。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS      MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   139% / 70%   1         5         5          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide</span></div><div class="line">NAME                     READY     STATUS              RESTARTS   AGE       IP            NODE</div><div class="line">nginx-5dcf548595-9gmqf   0/1       ContainerCreating   0          39s       &lt;none&gt;        192.168.1.204</div><div class="line">nginx-5dcf548595-bk9cr   1/1       Running             1          15h       172.30.94.2   192.168.1.205</div><div class="line">nginx-5dcf548595-pdndb   1/1       Running             0          10m       172.30.94.4   192.168.1.205</div><div class="line">nginx-5dcf548595-r7n4b   1/1       Running             0          39s       172.30.94.5   192.168.1.205</div><div class="line">nginx-5dcf548595-z9d6h   1/1       Running             0          10m       172.30.94.3   192.168.1.205</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>当REPLICAS达到定义的上限时，即使当前CPU的压力仍然很大，REPLICAS也不会再增加了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS     MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   112% / 70%   1         5         5          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide</span></div><div class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-5dcf548595-9gmqf   1/1       Running   0          2m        172.30.9.6    192.168.1.204</div><div class="line">nginx-5dcf548595-bk9cr   1/1       Running   1          15h       172.30.94.2   192.168.1.205</div><div class="line">nginx-5dcf548595-pdndb   1/1       Running   0          12m       172.30.94.4   192.168.1.205</div><div class="line">nginx-5dcf548595-r7n4b   1/1       Running   0          2m        172.30.94.5   192.168.1.205</div><div class="line">nginx-5dcf548595-z9d6h   1/1       Running   0          12m       172.30.94.3   192.168.1.205</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>停止压测，当CPU负载降低时，HPA会自动减少POD的数量。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS     MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   40% / 70%   1         5         3          14h</div><div class="line">[root@master ~]<span class="comment">#</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide</span></div><div class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-5dcf548595-pdndb   1/1       Running   0          16m       172.30.94.4   192.168.1.205</div><div class="line">nginx-5dcf548595-r7n4b   1/1       Running   0          6m        172.30.94.5   192.168.1.205</div><div class="line">nginx-5dcf548595-z9d6h   1/1       Running   0          16m       172.30.94.3   192.168.1.205</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>慢慢的，HPA会减少POD的数量，直到降低到最小POD数(MINPODS)。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa</span></div><div class="line">NAME        REFERENCE          TARGETS      MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   0% / 70%   1         5         1          15h</div><div class="line">[root@master ~]<span class="comment">#</span></div><div class="line">[root@master ~]<span class="comment"># kubectl get pod -o wide</span></div><div class="line">NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-5dcf548595-z9d6h   1/1       Running   0          1h        172.30.94.3   192.168.1.205</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>通过kube-dashboard观察这个过程的变化。<br><img src="https://note.youdao.com/yws/api/personal/file/F629DAA40F784576824CBD41240103B0?method=download&amp;shareKey=aef82ab7b8bf8633212a7f4679686de2" alt="kube-dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/3BE7F7C0A7DD4E8C841D3F3E9A4BCB08?method=download&amp;shareKey=09d40ca0da02667398ba75864d50e837" alt="kube-dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/FE3468620FB64868BF110C6AE9FAE2E5?method=download&amp;shareKey=97d1289d99000d823e02495f6cf1057f" alt="kube-dashboard"><br>通过HPA的日志信息查看到它伸缩的过程。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl describe hpa</span></div><div class="line">Name:                                                  nginx-hpa</div><div class="line">Namespace:                                             default</div><div class="line">Labels:                                                &lt;none&gt;</div><div class="line">Annotations:                                           kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:<span class="string">"au</span></div><div class="line">toscaling/v1",<span class="string">"kind"</span>:<span class="string">"HorizontalPodAutoscaler"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"nginx-hpa"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"maxRepl...CreationTimestamp:                                     Sun, 27 Jan 2019 01:04:25 +0800</span></div><div class="line">Reference:                                             Deployment/nginx</div><div class="line">Metrics:                                               ( current / target )</div><div class="line">  resource cpu on pods  (as a percentage of request):  0% (0) / 70%</div><div class="line">Min replicas:                                          1</div><div class="line">Max replicas:                                          5</div><div class="line">Conditions:</div><div class="line">  Type            Status  Reason            Message</div><div class="line">  ----            ------  ------            -------</div><div class="line">  AbleToScale     False   BackoffDownscale  the time since the previous scale is still within the downscale forbidden window</div><div class="line">  ScalingActive   True    ValidMetricFound  the HPA was able to succesfully calculate a replica count from cpu resource utilization (percentage of request)  ScalingLimited  True    TooFewReplicas    the desired replica count is increasing faster than the maximum scale rate</div><div class="line">Events:</div><div class="line">  Type    Reason             Age               From                       Message</div><div class="line">  ----    ------             ----              ----                       -------</div><div class="line">  Normal  SuccessfulRescale  41m (x2 over 1h)  horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target</div><div class="line">  Normal  SuccessfulRescale  29m (x2 over 1h)  horizontal-pod-autoscaler  New size: 3; reason: All metrics below target</div><div class="line">  Normal  SuccessfulRescale  17m               horizontal-pod-autoscaler  New size: 2; reason: All metrics below target</div><div class="line">  Normal  SuccessfulRescale  8m (x2 over 1h)   horizontal-pod-autoscaler  New size: 3; reason: cpu resource utilization (percentage of request) above target</div><div class="line">  Normal  SuccessfulRescale  3m (x2 over 12m)  horizontal-pod-autoscaler  New size: 1; reason: All metrics below target</div><div class="line">[root@master ~]#</div></pre></td></tr></table></figure></p>
<h5 id="autoscaling-v2beta1"><a href="#autoscaling-v2beta1" class="headerlink" title="autoscaling/v2beta1"></a>autoscaling/v2beta1</h5><p><code>autoscaling/v2beta1</code>中增加支持custom metrics。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># cat nginx-hpa-v2beta1.yml </span></div><div class="line">apiVersion: autoscaling/v2beta1</div><div class="line">kind: HorizontalPodAutoscaler</div><div class="line">metadata:</div><div class="line">  name: nginx-hpa</div><div class="line">spec:</div><div class="line">  scaleTargetRef:</div><div class="line">    apiVersion: extensions/v1beta1</div><div class="line">    kind: Deployment</div><div class="line">    name: nginx</div><div class="line">  minReplicas: 1</div><div class="line">  maxReplicas: 5</div><div class="line">  metrics:</div><div class="line">    - <span class="built_in">type</span>: Resource</div><div class="line">      resource:</div><div class="line">        name: memory</div><div class="line">        targetAverageUtilization: 70</div><div class="line">    - <span class="built_in">type</span>: Resource</div><div class="line">      resource:</div><div class="line">        name: cpu</div><div class="line">        targetAverageUtilization: 70</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl apply -f nginx-hpa-v2beta1.yml</span></div></pre></td></tr></table></figure>
<p>等待几分钟后…<br>观察发现前面10%是内存的使用百分比，后面0%是CPU的使用百分比。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa nginx-hpa</span></div><div class="line">NAME        REFERENCE          TARGETS               MINPODS   MAXPODS   REPLICAS   AGE</div><div class="line">nginx-hpa   Deployment/nginx   10% / 70%, 0% / 70%   1         5         1          51s</div><div class="line">[root@master ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl describe hpa nginx-hpa</span></div><div class="line">Name:                                                     nginx-hpa</div><div class="line">Namespace:                                                default</div><div class="line">Labels:                                                   &lt;none&gt;</div><div class="line">Annotations:                                              kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:</div><div class="line"><span class="string">"autoscaling/v2beta1"</span>,<span class="string">"kind"</span>:<span class="string">"HorizontalPodAutoscaler"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"nginx-hpa"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"ma...CreationTimestamp:                                        Mon, 28 Jan 2019 22:22:01 +0800</span></div><div class="line">Reference:                                                Deployment/nginx</div><div class="line">Metrics:                                                  ( current / target )</div><div class="line">  resource memory on pods  (as a percentage of request):  10% (2670592) / 70%</div><div class="line">  resource cpu on pods  (as a percentage of request):     0% (0) / 70%</div><div class="line">Min replicas:                                             1</div><div class="line">Max replicas:                                             5</div><div class="line">Conditions:</div><div class="line">  Type            Status  Reason              Message</div><div class="line">  ----            ------  ------              -------</div><div class="line">  AbleToScale     True    ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale</div><div class="line">  ScalingActive   True    ValidMetricFound    the HPA was able to succesfully calculate a replica count from memory resou</div><div class="line">rce utilization (percentage of request)  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range</div><div class="line">Events:           &lt;none&gt;</div><div class="line"></div><div class="line">[root@master ~]#</div></pre></td></tr></table></figure>
<h5 id="autoscaling-v2beta2"><a href="#autoscaling-v2beta2" class="headerlink" title="autoscaling/v2beta2"></a>autoscaling/v2beta2</h5><p><code>autoscaling/v2beta2</code>测试发现目前k8s 1.9.2暂不支持这个API版本。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master ~]<span class="comment"># kubectl get hpa.v2beta2.autoscaling -o yaml</span></div><div class="line">the server doesn<span class="string">'t have a resource type "hpa" in group "v2beta2.autoscaling"</span></div><div class="line">[root@master ~]#</div></pre></td></tr></table></figure></p>
<p>参考：<br><a href="http://blog.51cto.com/ylw6006/2113848" target="_blank" rel="external">http://blog.51cto.com/ylw6006/2113848</a><br><a href="https://blog.frognew.com/2017/01/kubernetes-pod-scale.html" target="_blank" rel="external">https://blog.frognew.com/2017/01/kubernetes-pod-scale.html</a><br><a href="https://k8smeetup.github.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/" target="_blank" rel="external">https://k8smeetup.github.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</a><br><a href="https://blog.csdn.net/qq_17016649/article/details/79297796" target="_blank" rel="external">https://blog.csdn.net/qq_17016649/article/details/79297796</a><br><a href="https://github.com/kubernetes/kubernetes/issues/57673" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/issues/57673</a>  </p>
<p>附件：<br><a href="https://note.youdao.com/yws/api/personal/file/9D8378F87D324B679D38A854BE9E812A?method=download&amp;shareKey=67aef289e4b6d60a0a47d38c1a6d2554" target="_blank" rel="external">HPA测试配置文件.zip</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2019/01/28/k8s集群水平扩展-HPA/">http://www.yfshare.vip/2019/01/28/k8s集群水平扩展-HPA/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Horizontal Pod Autoscaling，简称HPA，是Kubernetes中实现POD水平自动伸缩的功能。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="k8s" scheme="http://www.yfshare.vip/tags/k8s/"/>
    
      <category term="docker" scheme="http://www.yfshare.vip/tags/docker/"/>
    
      <category term="HPA" scheme="http://www.yfshare.vip/tags/HPA/"/>
    
      <category term="cpu" scheme="http://www.yfshare.vip/tags/cpu/"/>
    
      <category term="memory" scheme="http://www.yfshare.vip/tags/memory/"/>
    
  </entry>
  
  <entry>
    <title>Grafana日志聚合工具Loki</title>
    <link href="http://www.yfshare.vip/2019/01/20/Grafana%E6%97%A5%E5%BF%97%E8%81%9A%E5%90%88%E5%B7%A5%E5%85%B7Loki/"/>
    <id>http://www.yfshare.vip/2019/01/20/Grafana日志聚合工具Loki/</id>
    <published>2019-01-20T08:22:01.000Z</published>
    <updated>2019-01-20T08:38:31.159Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，为 Prometheus和 Kubernetes用户做了相关优化。项目受 Prometheus 启发，类似于 Prometheus 的日志系统。<br><a id="more"></a></p>
<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Loki初体验<br>与其他日志聚合系统相比， Loki具有下面的一些特性：  </p>
<ul>
<li>不对日志进行全文索引。通过存储压缩非结构化日志和仅索引元数据，Loki 操作起来会更简单，更省成本。  </li>
<li>通过使用与 Prometheus 相同的标签记录流对日志进行索引和分组，这使得日志的扩展和操作效率更高。  </li>
<li>特别适合储存 Kubernetes Pod 日志; 诸如 Pod 标签之类的元数据会被自动删除和编入索引。  </li>
<li>受 Grafana 原生支持。  </li>
</ul>
<p>Loki 由以下3个部分组成：  </p>
<ul>
<li>loki是主服务器，负责存储日志和处理查询。  </li>
<li>promtail是代理，负责收集日志并将其发送给 loki。  </li>
<li>Grafana 用于 UI 展示。  </li>
</ul>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><table>
<thead>
<tr>
<th>环境</th>
<th>版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>centos</td>
<td>v7.4</td>
</tr>
<tr>
<td>docker</td>
<td>v18.09.0</td>
</tr>
<tr>
<td>docker-compose</td>
<td>v1.23.2</td>
</tr>
<tr>
<td>grafana</td>
<td>v5.5.0</td>
</tr>
<tr>
<td>node-exporter</td>
<td>v0.17.0</td>
</tr>
<tr>
<td>prometheus</td>
<td>v2.5.0</td>
</tr>
</tbody>
</table>
<h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p>clone loki源码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@1-206 ~]<span class="comment"># curl -L "https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose</span></div><div class="line">[root@1-206 ~]<span class="comment"># yum install -y git</span></div><div class="line">[root@1-206 ~]<span class="comment"># git clone https://github.com/grafana/loki.git</span></div><div class="line">[root@1-206 ~]<span class="comment"># cd loki/production/</span></div></pre></td></tr></table></figure></p>
<p>可以直接运行官方的docker-compose.yml文件，执行命令为：<code>docker-compose -f docker-compose.yaml up -d</code><br>docker-compose.yml 文件如下，这里使用的 grafana 镜像为自己编译的，官方的镜像为：<code>image: grafana/grafana:master</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">[root@1-206 production]<span class="comment"># cat docker-compose.yaml </span></div><div class="line">version: <span class="string">"3"</span></div><div class="line">networks:</div><div class="line">  loki:</div><div class="line">services:</div><div class="line">  loki:</div><div class="line">    image: grafana/loki:master</div><div class="line">    ports:</div><div class="line">      - <span class="string">"3100:3100"</span></div><div class="line">    <span class="built_in">command</span>: -config.file=/etc/loki/<span class="built_in">local</span>-config.yaml</div><div class="line">    networks:</div><div class="line">      - loki</div><div class="line"></div><div class="line">  promtail:</div><div class="line">    image: grafana/promtail:master</div><div class="line">    volumes:</div><div class="line">      - /var/<span class="built_in">log</span>:/var/<span class="built_in">log</span></div><div class="line">    <span class="built_in">command</span>: </div><div class="line">      -config.file=/etc/promtail/docker-config.yaml</div><div class="line">    networks:</div><div class="line">      - loki</div><div class="line">      </div><div class="line">  grafana:</div><div class="line">    image: yfshare/grafana:5.5.0_beta1</div><div class="line">    ports:</div><div class="line">      - <span class="string">"3000:3000"</span></div><div class="line">    environment:</div><div class="line">      GF_EXPLORE_ENABLED: <span class="string">"true"</span></div><div class="line">    networks:</div><div class="line">      - loki</div><div class="line">[root@1-206 production]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@1-206 production]<span class="comment"># docker-compose -f docker-compose.yaml up -d</span></div><div class="line">[root@1-206 production]<span class="comment"># docker-compose ps</span></div><div class="line">        Name                       Command               State               Ports             </div><div class="line">-----------------------------------------------------------------------------------------------</div><div class="line">production_grafana_1    /docker-entrypoint.sh            Up      22/tcp, 0.0.0.0:3000-&gt;3000/tcp</div><div class="line">production_loki_1       /bin/loki -config.file=/et ...   Up      0.0.0.0:3100-&gt;3100/tcp, 80/tcp</div><div class="line">production_promtail_1   /usr/bin/promtail -config. ...   Up                                    </div><div class="line">[root@1-206 production]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="体验"><a href="#体验" class="headerlink" title="体验"></a>体验</h4><p>部署完成后，访问grafana：<a href="http://ip:3000，默认密码为`admin:admin`" target="_blank" rel="external">http://ip:3000，默认密码为`admin:admin`</a><br>Grafana v5.5.0+以上的版本才支持Loki<br><img src="https://note.youdao.com/yws/api/personal/file/26D9B679EB99481FA1C1EDBF165EB96C?method=download&amp;shareKey=2b739c0af348ab7f270255386e300f3c" alt="Grafana_Plugins"><br>添加数据源Prometheus和Loki<br><img src="https://note.youdao.com/yws/api/personal/file/1F579EED73B449309CB3163035F783DF?method=download&amp;shareKey=7a275eedbdd684e387ca38e38d6457fc" alt="Grafana_Add_Data_Sources"><br><img src="https://note.youdao.com/yws/api/personal/file/C06C1D8CEAB0467FB20D5054C4EE3315?method=download&amp;shareKey=8ff3e7543b97fff40b9a9506f03369ff" alt="Grafana_Prometheus"><br><img src="https://note.youdao.com/yws/api/personal/file/8137BBD8D6014847BCDFDD735F287454?method=download&amp;shareKey=0188720a3a63bb63af7ef233d4e7bf62" alt="Grafana_Loki"><br><img src="https://note.youdao.com/yws/api/personal/file/BA3128B97536448ABE61DF845D0817C0?method=download&amp;shareKey=0146ff6587169a25d36e0d3ca42a78fa" alt="Grafana_Data_Sources"><br>根据系统标签来查看Loki日志<br>测试发现当promtail定义读取的日志路径为<code>/var/log</code>时，即使日志路径为<code>/var/log/grafana/grafana.log</code>也不能读取。<br><img src="https://note.youdao.com/yws/api/personal/file/EF3D0ECA3532497B934D2BE81C5A7D9E?method=download&amp;shareKey=ed99699f094fd25c5de0e26297c86716" alt="Grafana_Loki_Log"><br><img src="https://note.youdao.com/yws/api/personal/file/23299AE377984D1AB5F71B446D2C0FC6?method=download&amp;shareKey=3b5375e6c45f4f13b1b1e0f09a633782" alt="Loki_Labels"><br>这里的job varlogs是在promtail的配置文件里定义的，promtail配置文件如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@1-206 production]<span class="comment"># docker exec -it production_promtail_1 /bin/sh</span></div><div class="line">/ <span class="comment"># cat /etc/promtail/docker-config.yaml</span></div><div class="line">server:</div><div class="line">  http_listen_port: 0</div><div class="line">  grpc_listen_port: 0</div><div class="line">positions:</div><div class="line">  filename: /tmp/positions.yaml</div><div class="line">client:</div><div class="line">  url: http://loki:3100/api/prom/push</div><div class="line">scrape_configs:</div><div class="line">- job_name: system</div><div class="line">  entry_parser: raw</div><div class="line">  static_configs:</div><div class="line">  - targets:</div><div class="line">      - localhost</div><div class="line">    labels:</div><div class="line">      job: varlogs</div><div class="line">      __path__: /var/<span class="built_in">log</span></div><div class="line">/ <span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>Loki也可以查看Prometheus的item<br><img src="https://note.youdao.com/yws/api/personal/file/7E5F70739BB94E09AE316125E4592BD4?method=download&amp;shareKey=aaf8df3108eec2660ffc43cc009e8f7e" alt="Grafana_Prometheus_Item">  </p>
<p>参考：<br><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247484023&amp;idx=1&amp;sn=0ea2c44046edf5365e201714edbbc6ba&amp;chksm=fdb90d6acace847c290efbe4c86599110610286cddc53f9917407ae7c41c522971414f2a0cca&amp;mpshare=1&amp;scene=23&amp;srcid=121488q6r3xYl39pZbdhZ22l#rd" target="_blank" rel="external">Grafana 日志聚合工具 Loki</a><br><a href="https://grafana.com/loki#about" target="_blank" rel="external">Loki官网</a><br><a href="https://github.com/grafana/loki/tree/master/production/ksonnet" target="_blank" rel="external">Deploy Loki to Kubernetes</a><br><a href="https://github.com/grafana/loki/blob/master/docs/usage.md" target="_blank" rel="external">Configuring the Loki Datasource in Grafana</a><br><a href="https://github.com/grafana/loki/blob/master/docs/api.md" target="_blank" rel="external">Loki API</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2019/01/20/Grafana日志聚合工具Loki/">http://www.yfshare.vip/2019/01/20/Grafana日志聚合工具Loki/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Loki是 Grafana Labs 团队最新的开源项目，是一个水平可扩展，高可用性，多租户的日志聚合系统。它的设计非常经济高效且易于操作，因为它不会为日志内容编制索引，而是为每个日志流编制一组标签，为 Prometheus和 Kubernetes用户做了相关优化。项目受 Prometheus 启发，类似于 Prometheus 的日志系统。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Grafana" scheme="http://www.yfshare.vip/tags/Grafana/"/>
    
      <category term="Loki" scheme="http://www.yfshare.vip/tags/Loki/"/>
    
      <category term="Docker" scheme="http://www.yfshare.vip/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Python十行代码爬抖音</title>
    <link href="http://www.yfshare.vip/2018/11/06/Python%E5%8D%81%E8%A1%8C%E4%BB%A3%E7%A0%81%E7%88%AC%E6%8A%96%E9%9F%B3/"/>
    <id>http://www.yfshare.vip/2018/11/06/Python十行代码爬抖音/</id>
    <published>2018-11-05T16:07:32.000Z</published>
    <updated>2018-11-05T18:18:12.426Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>今天看到一篇文章介绍怎么用python来爬抖音的小视频和音乐。于是兴趣大增，先来试试能不能玩，哈哈。<br><a id="more"></a><br>先来看个效果图，嗯嗯…  </p>
<h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p><img src="https://note.youdao.com/yws/api/personal/file/39A6E0DC37EA4772BFFFE2FB749455C3?method=download&amp;shareKey=0f9dc13147fe991223234ff80d468afe" alt="py_video">  </p>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　　python 3.7.1<br>　　　centos 7.4<br>　　　pip 10.0.1  </p>
<h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># python3.7 --version</span></div><div class="line">Python <span class="number">3.7</span><span class="number">.1</span></div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># pip3 install douyin</span></div></pre></td></tr></table></figure>
<p><img src="https://note.youdao.com/yws/api/personal/file/6C5A7B9EE298427E8EB44FCBB98C773D?method=download&amp;shareKey=ded7cd2ee3b179a0fc1f7830eb827fc4" alt="py_install_douyin"><br>有时候因为网络原因会安装失败，这时重新执行上面的命令即可，直到安装完成。  </p>
<p>导入douyin模块<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># python3.7</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> douyin</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure></p>
<p>导入如果报错的话，可能douyin模块没有安装成功。  </p>
<p>下面我们开始爬…爬抖音小视频和音乐咯<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost douyin]<span class="comment"># python3.7 dou.py</span></div></pre></td></tr></table></figure></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/D3CE0E55A29F4CDE95B30643D179053B?method=download&amp;shareKey=e17a3c7a177e531471edfbc4c571bcf2" alt="py_download"><br><img src="https://note.youdao.com/yws/api/personal/file/04D69EAF98CB47098B498897383F693F?method=download&amp;shareKey=6f37956f071cc65b949c67ea8c715358" alt="py_download">  </p>
<p>几分钟后…我们来看看爬的成果<br>可以看到视频配的音乐被存储成了 mp3 格式的文件，抖音视频存储成了 mp4 文件。<br><img src="https://note.youdao.com/yws/api/personal/file/77B9BD11581D4D61A825B0AAD07CD12F?method=download&amp;shareKey=9d5eb61e94e2d418f1ebf1315075134f" alt="py_download">  </p>
<p>嗯…不错，哈哈。  </p>
<h4 id="py脚本"><a href="#py脚本" class="headerlink" title="py脚本"></a>py脚本</h4><p>作者说，能爬抖音上所有热门话题和音乐下的相关视频都爬取到，并且将爬到的视频下载下来，同时还要把视频所配的音乐也单独下载下来，不仅如此，所有视频的相关信息如发布人、点赞数、评论数、发布时间、发布人、发布地点等等信息都需要爬取下来，并存储到 MongoDB 数据库。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> douyin</div><div class="line"><span class="keyword">from</span> douyin.structures <span class="keyword">import</span> Topic, Music</div><div class="line"></div><div class="line"><span class="comment"># 定义视频下载、音频下载、MongoDB 存储的处理器</span></div><div class="line">video_file_handler = douyin.handlers.VideoFileHandler(folder=<span class="string">'./videos'</span>)</div><div class="line">music_file_handler = douyin.handlers.MusicFileHandler(folder=<span class="string">'./musics'</span>)</div><div class="line"><span class="comment">#mongo_handler = douyin.handlers.MongoHandler()</span></div><div class="line"><span class="comment"># 定义下载器，并将三个处理器当做参数传递</span></div><div class="line"><span class="comment">#downloader = douyin.downloaders.VideoDownloader([mongo_handler, video_file_handler, music_file_handler])</span></div><div class="line">downloader = douyin.downloaders.VideoDownloader([video_file_handler, music_file_handler])</div><div class="line"><span class="comment"># 循环爬取抖音热榜信息并下载存储</span></div><div class="line"><span class="keyword">for</span> result <span class="keyword">in</span> douyin.hot.trend():</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> result.data:</div><div class="line">        <span class="comment"># 爬取热门话题和热门音乐下面的所有视频，每个话题或音乐最多爬取 10 个相关视频。</span></div><div class="line">        downloader.download(item.videos(max=<span class="number">10</span>))</div></pre></td></tr></table></figure></p>
<p>由于我这里没有mongodb所以，把这mongodb相关的配置给注释掉了。<br>作者github地址：<a href="https://github.com/Python3WebSpider/DouYin" target="_blank" rel="external">https://github.com/Python3WebSpider/DouYin</a>  </p>
<p>====以下摘自作者====</p>
<h4 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h4><p>本库依赖的其他库有：  </p>
<ul>
<li>aiohttp：利用它可以完成异步数据下载，加快下载速度  </li>
<li>dateparser：利用它可以完成任意格式日期的转化  </li>
<li>motor：利用它可以完成异步 MongoDB 存储，加快存储速度  </li>
<li>requests：利用它可以完成最基本的 HTTP 请求模拟  </li>
<li>tqdm：利用它可以进行进度条的展示  </li>
</ul>
<p><strong>数据结构定义</strong><br>如果要做一个库的话，一个很重要的点就是对一些关键的信息进行结构化的定义，使用面向对象的思维对某些对象进行封装，抖音的爬取也不例外。  </p>
<p>在抖音中，其实有很多种对象，比如视频、音乐、话题、用户、评论等等，它们之间通过某种关系联系在一起，例如视频中使用了某个配乐，那么视频和音乐就存在使用关系；比如用户发布了视频，那么用户和视频就存在发布关系，我们可以使用面向对象的思维对每个对象进行封装，比如视频的话，就可以定义成如下结构：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Video</span><span class="params">(Base)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        init video object</div><div class="line">        :param kwargs:</div><div class="line">        """</div><div class="line">        super().__init__()</div><div class="line">        self.id = kwargs.get(<span class="string">'id'</span>)</div><div class="line">        self.desc = kwargs.get(<span class="string">'desc'</span>)</div><div class="line">        self.author = kwargs.get(<span class="string">'author'</span>)</div><div class="line">        self.music = kwargs.get(<span class="string">'music'</span>)</div><div class="line">        self.like_count = kwargs.get(<span class="string">'like_count'</span>)</div><div class="line">        self.comment_count = kwargs.get(<span class="string">'comment_count'</span>)</div><div class="line">        self.share_count = kwargs.get(<span class="string">'share_count'</span>)</div><div class="line">        self.hot_count = kwargs.get(<span class="string">'hot_count'</span>)</div><div class="line">        ...</div><div class="line">        self.address = kwargs.get(<span class="string">'address'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        video to str</div><div class="line">        :return: str</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> <span class="string">'&lt;Video: &lt;%s, %s&gt;&gt;'</span> % (self.id, self.desc[:<span class="number">10</span>].strip() <span class="keyword">if</span> self.desc <span class="keyword">else</span> <span class="keyword">None</span>)</div></pre></td></tr></table></figure></p>
<p>这里将一些关键的属性定义成 Video 类的一部分，包括 id 索引、desc 描述、author 发布人、music 配乐等等，其中 author 和 music 并不是简单的字符串的形式，它也是单独定义的数据结构，比如 author 就是 User 类型的对象，而 User 的定义又是如下结构：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span><span class="params">(Base)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        init user object</div><div class="line">        :param kwargs:</div><div class="line">        """</div><div class="line">        super().__init__()</div><div class="line">        self.id = kwargs.get(<span class="string">'id'</span>)</div><div class="line">        self.gender = kwargs.get(<span class="string">'gender'</span>)</div><div class="line">        self.name = kwargs.get(<span class="string">'name'</span>)</div><div class="line">        self.create_time = kwargs.get(<span class="string">'create_time'</span>)</div><div class="line">        self.birthday = kwargs.get(<span class="string">'birthday'</span>)</div><div class="line">        ...</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        user to str</div><div class="line">        :return:</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> <span class="string">'&lt;User: &lt;%s, %s&gt;&gt;'</span> % (self.alias, self.name)</div></pre></td></tr></table></figure></p>
<p>所以说，通过属性之间的关联，我们就可以将不同的对象关联起来，这样显得逻辑架构清晰，而且我们也不用一个个单独维护字典来存储了，其实这就和 Scrapy 里面的 Item 的定义是类似的。  </p>
<p><strong>请求和重试</strong><br>实现爬取的过程就不必多说了，这里面其实用到的就是最简单的抓包技巧，使用 Charles 直接进行抓包即可。抓包之后便可以观察到对应的接口请求，然后进行模拟即可。  </p>
<p>所以问题就来了，难道我要一个接口写一个请求方法吗？另外还要配置 Headers、超时时间等等的内容，那岂不是太费劲了，所以，我们可以将请求的方法进行单独的封装，这里我定义了一个 fetch 方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fetch</span><span class="params">(url, **kwargs)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    fetch api response</div><div class="line">    :param url: fetch url</div><div class="line">    :param kwargs: other requests params</div><div class="line">    :return: json of response</div><div class="line">    """</div><div class="line">    response = requests.get(url, **kwargs)</div><div class="line">    <span class="keyword">if</span> response.status_code != <span class="number">200</span>:</div><div class="line">        <span class="keyword">raise</span> requests.ConnectionError(<span class="string">'Expected status code 200, but got &#123;&#125;'</span>.format(response.status_code))</div><div class="line">    <span class="keyword">return</span> response.json()</div></pre></td></tr></table></figure></p>
<p>这个方法留了一个必要参数，即 url，另外其他的配置我留成了 kwargs，也就是可以任意传递，传递之后，它会依次传递给 requests 的请求方法，然后这里还做了异常处理，如果成功请求，即可返回正常的请求结果。  </p>
<p>定义了这个方法，在其他的调用方法里面我们只需要单独调用这个 fetch 方法即可，而不需要再去关心异常处理，返回类型了。  </p>
<p>好，那么定义好了请求之后，如果出现了请求失败怎么办呢？按照常规的方法，我们可能就会在外面套一层方法，然后记录调用 fetch 方法请求失败的次数，然后重新调用 fetch 方法进行重试，但这里可以告诉大家一个更好用的库，叫做 retrying，使用它我们可以通过定义一个装饰器来完成重试的操作。  </p>
<p>比如我可以使用 retry 装饰器这么装饰 fetch 方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</div><div class="line"></div><div class="line"><span class="meta">@retry(stop_max_attempt_number=retry_max_number, wait_random_min=retry_min_random_wait,</span></div><div class="line">           wait_random_max=retry_max_random_wait, retry_on_exception=need_retry)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fetch</span><span class="params">(url, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure></p>
<p>这里使用了装饰器的四个参数：  </p>
<ul>
<li>stop_max_attempt_number：最大重试次数，如果重试次数达到该次数则放弃重试  </li>
<li>wait_random_min：下次重试之前随机等待时间的最小值  </li>
<li>wait_random_max：下次重试之前随机等待时间的最大值  </li>
<li>retry_on_exception：判断出现了怎样的异常才重试  </li>
</ul>
<p>这里 retry_on_exception 参数指定了一个方法，叫做 need_retry，方法定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">need_retry</span><span class="params">(exception)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    need to retry</div><div class="line">    :param exception:</div><div class="line">    :return:</div><div class="line">    """</div><div class="line">    result = isinstance(exception, (requests.ConnectionError, requests.ReadTimeout))</div><div class="line">    <span class="keyword">if</span> result:</div><div class="line">        print(<span class="string">'Exception'</span>, type(exception), <span class="string">'occurred, retrying...'</span>)</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure></p>
<p>这里判断了如果是 requests 的 ConnectionError 和 ReadTimeout 异常的话，就会抛出异常进行重试，否则不予重试。  </p>
<p>所以，这样我们就实现了请求的封装和自动重试，是不是非常 Pythonic？  </p>
<p><strong>下载处理器的设计</strong><br>为了下载视频，我们需要设计一个下载处理器来下载已经爬取到的视频链接，所以下载处理器的输入就是一批批的视频链接，下载器接收到这些链接，会将其进行下载处理，并将视频存储到对应的位置，另外也可以完成一些信息存储操作。  </p>
<ul>
<li>在设计时，下载处理器的要求有两个，一个是保证高速的下载，另一个就是可扩展性要强，下面我们分别来针对这两个特点进行设计：<br>高速下载，为了实现高速的下载，要么可以使用多线程或多进程，要么可以用异步下载，很明显，后者是更有优势的。</li>
<li>扩展性强，下载处理器要能下载音频、视频，另外还可以支持数据库等存储，所以为了解耦合，我们可以将视频下载、音频下载、数据库存储的功能独立出来，下载处理器只负责视频链接的主要逻辑处理和分配即可。  </li>
</ul>
<p>为了实现高速下载，这里我们可以使用 aiohttp 库来完成，另外异步下载我们也不能一下子下载太多，不然网络波动太大，所以我们可以设置 batch 式下载，可以避免同时大量的请求和网络拥塞，主要的下载函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(self, inputs)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    download video or video lists</div><div class="line">    :param data:</div><div class="line">    :return:</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> isinstance(inputs, types.GeneratorType):</div><div class="line">        temps = []</div><div class="line">        <span class="keyword">for</span> result <span class="keyword">in</span> inputs:</div><div class="line">            print(<span class="string">'Processing'</span>, result, <span class="string">'...'</span>)</div><div class="line">            temps.append(result)</div><div class="line">            <span class="keyword">if</span> len(temps) == self.batch:</div><div class="line">                self.process_items(temps)</div><div class="line">                temps = []</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        inputs = inputs <span class="keyword">if</span> isinstance(inputs, list) <span class="keyword">else</span> [inputs]</div><div class="line">        self.process_items(inputs)</div></pre></td></tr></table></figure></p>
<p>这个 download 方法设计了多种数据接收类型，可以接收一个生成器，也可以接收单个或列表形式的视频对象数据，接着调用了 process_items 方法进行了异步下载，其方法实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_items</span><span class="params">(self, objs)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    process items</div><div class="line">    :param objs: objs</div><div class="line">    :return:</div><div class="line">    """</div><div class="line">    <span class="comment"># define progress bar</span></div><div class="line">    <span class="keyword">with</span> tqdm(total=len(objs)) <span class="keyword">as</span> self.bar:</div><div class="line">        <span class="comment"># init event loop</span></div><div class="line">        loop = asyncio.get_event_loop()</div><div class="line">        <span class="comment"># get num of batches</span></div><div class="line">        total_step = int(math.ceil(len(objs) / self.batch))</div><div class="line">        <span class="comment"># for every batch</span></div><div class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(total_step):</div><div class="line">            start, end = step * self.batch, (step + <span class="number">1</span>) * self.batch</div><div class="line">            print(<span class="string">'Processing %d-%d of files'</span> % (start + <span class="number">1</span>, end))</div><div class="line">            <span class="comment"># get batch of objs</span></div><div class="line">            objs_batch = objs[start: end]</div><div class="line">            <span class="comment"># define tasks and run loop</span></div><div class="line">            tasks = [asyncio.ensure_future(self.process_item(obj)) <span class="keyword">for</span> obj <span class="keyword">in</span> objs_batch]</div><div class="line">            <span class="keyword">for</span> task <span class="keyword">in</span> tasks:</div><div class="line">                task.add_done_callback(self.update_progress)</div><div class="line">            loop.run_until_complete(asyncio.wait(tasks))</div></pre></td></tr></table></figure></p>
<p>这里使用了 asyncio 实现了异步处理，并通过对视频链接进行分批处理保证了流量的稳定性，另外还使用了 tqdm 实现了进度条的显示。  </p>
<p>我们可以看到，真正的处理下载的方法是 process_item，这里面会调用视频下载、音频下载、数据库存储的一些组件来完成处理，由于我们使用了 asyncio 进行了异步处理，所以 process_item 也需要是一个支持异步处理的方法，定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, obj)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    process item</div><div class="line">    :param obj: single obj</div><div class="line">    :return:</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> isinstance(obj, Video):</div><div class="line">        print(<span class="string">'Processing'</span>, obj, <span class="string">'...'</span>)</div><div class="line">        <span class="keyword">for</span> handler <span class="keyword">in</span> self.handlers:</div><div class="line">            <span class="keyword">if</span> isinstance(handler, Handler):</div><div class="line">                <span class="keyword">await</span> handler.process(obj)</div></pre></td></tr></table></figure></p>
<p>这里我们可以看到，真正的处理逻辑都在一个个 handler 里面，我们将每个单独的功能进行了抽离，定义成了一个个 Handler，这样可以实现良好的解耦合，如果我们要增加和关闭某些功能，只需要配置不同的 Handler 即可，而不需要去改动代码，这也是设计模式的一个解耦思想，类似工厂模式。  </p>
<p><strong>Handler 的设计</strong><br>刚才我们讲了，Handler 就负责一个个具体功能的实现，比如视频下载、音频下载、数据存储等等，所以我们可以将它们定义成不同的 Handler，而视频下载、音频下载又都是文件下载，所以又可以利用继承的思想设计一个文件下载的 Handler，定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join, exists</div><div class="line"><span class="keyword">from</span> os <span class="keyword">import</span> makedirs</div><div class="line"><span class="keyword">from</span> douyin.handlers <span class="keyword">import</span> Handler</div><div class="line"><span class="keyword">from</span> douyin.utils.type <span class="keyword">import</span> mime_to_ext</div><div class="line"><span class="keyword">import</span> aiohttp</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FileHandler</span><span class="params">(Handler)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, folder)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        init save folder</div><div class="line">        :param folder:</div><div class="line">        """</div><div class="line">        super().__init__()</div><div class="line">        self.folder = folder</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> exists(self.folder):</div><div class="line">            makedirs(self.folder)</div><div class="line"></div><div class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">_process</span><span class="params">(self, obj, **kwargs)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        download to file</div><div class="line">        :param url: resource url</div><div class="line">        :param name: save name</div><div class="line">        :param kwargs:</div><div class="line">        :return:</div><div class="line">        """</div><div class="line">        print(<span class="string">'Downloading'</span>, obj, <span class="string">'...'</span>)</div><div class="line">        kwargs.update(&#123;<span class="string">'ssl'</span>: <span class="keyword">False</span>&#125;)</div><div class="line">        kwargs.update(&#123;<span class="string">'timeout'</span>: <span class="number">10</span>&#125;)</div><div class="line">        <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</div><div class="line">            <span class="keyword">async</span> <span class="keyword">with</span> session.get(obj.play_url, **kwargs) <span class="keyword">as</span> response:</div><div class="line">                <span class="keyword">if</span> response.status == <span class="number">200</span>:</div><div class="line">                    extension = mime_to_ext(response.headers.get(<span class="string">'Content-Type'</span>))</div><div class="line">                    full_path = join(self.folder, <span class="string">'%s.%s'</span> % (obj.id, extension))</div><div class="line">                    <span class="keyword">with</span> open(full_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">                        f.write(<span class="keyword">await</span> response.content.read())</div><div class="line">                    print(<span class="string">'Downloaded file to'</span>, full_path)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    print(<span class="string">'Cannot download %s, response status %s'</span> % (obj.id, response.status))</div><div class="line"></div><div class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, obj, **kwargs)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        process obj</div><div class="line">        :param obj:</div><div class="line">        :param kwargs:</div><div class="line">        :return:</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> <span class="keyword">await</span> self._process(obj, **kwargs)</div></pre></td></tr></table></figure></p>
<p>这里我们还是使用了 aiohttp，因为在下载处理器中需要 Handler 支持异步操作，这里下载的时候就是直接请求了文件链接，然后判断了文件的类型，并完成了文件保存。  </p>
<p>视频下载的 Handler 只需要继承当前的 FileHandler 即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> douyin.handlers <span class="keyword">import</span> FileHandler</div><div class="line"><span class="keyword">from</span> douyin.structures <span class="keyword">import</span> Video</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">VideoFileHandler</span><span class="params">(FileHandler)</span>:</span></div><div class="line"></div><div class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, obj, **kwargs)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        process video obj</div><div class="line">        :param obj:</div><div class="line">        :param kwargs:</div><div class="line">        :return:</div><div class="line">        """</div><div class="line">        <span class="keyword">if</span> isinstance(obj, Video):</div><div class="line">            <span class="keyword">return</span> <span class="keyword">await</span> self._process(obj, **kwargs)</div></pre></td></tr></table></figure></p>
<p>这里其实就是加了类别判断，确保数据类型的一致性，当然音频下载也是一样的。  </p>
<p><strong>异步 MongoDB 存储</strong><br>上面介绍了视频和音频处理的 Handler，另外还有一个存储的 Handler 没有介绍，那就是 MongoDB 存储，平常我们可能习惯使用 PyMongo 来完成存储，但这里我们为了加速，需要支持异步操作，所以这里有一个可以实现异步 MongoDB 存储的库，叫做 Motor，其实使用的方法差不太多，MongoDB 的连接对象不再是 PyMongo 的 MongoClient 了，而是 Motor 的 AsyncIOMotorClient，其他的配置基本类似。  </p>
<p>在存储时使用的是 update_one 方法并开启了 upsert 参数，这样可以做到存在即更新，不存在即插入的功能，保证数据的不重复性。  </p>
<p>整个 MongoDB 存储的 Handler 定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> douyin.handlers <span class="keyword">import</span> Handler</div><div class="line"><span class="keyword">from</span> motor.motor_asyncio <span class="keyword">import</span> AsyncIOMotorClient</div><div class="line"><span class="keyword">from</span> douyin.structures <span class="keyword">import</span> *</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoHandler</span><span class="params">(Handler)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conn_uri=None, db=<span class="string">'douyin'</span>)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        init save folder</div><div class="line">        :param folder:</div><div class="line">        """</div><div class="line">        super().__init__()</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> conn_uri:</div><div class="line">            conn_uri = <span class="string">'localhost'</span></div><div class="line">        self.client = AsyncIOMotorClient(conn_uri)</div><div class="line">        self.db = self.client[db]</div><div class="line"></div><div class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, obj, **kwargs)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        download to file</div><div class="line">        :param url: resource url</div><div class="line">        :param name: save name</div><div class="line">        :param kwargs:</div><div class="line">        :return:</div><div class="line">        """</div><div class="line">        collection_name = <span class="string">'default'</span></div><div class="line">        <span class="keyword">if</span> isinstance(obj, Video):</div><div class="line">            collection_name = <span class="string">'videos'</span></div><div class="line">        <span class="keyword">elif</span> isinstance(obj, Music):</div><div class="line">            collection_name = <span class="string">'musics'</span></div><div class="line">        collection = self.db[collection_name]</div><div class="line">        <span class="comment"># save to mongodb</span></div><div class="line">        print(<span class="string">'Saving'</span>, obj, <span class="string">'to mongodb...'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">await</span> collection.update_one(&#123;<span class="string">'id'</span>: obj.id&#125;, &#123;<span class="string">'$set'</span>: obj.json()&#125;, upsert=<span class="keyword">True</span>):</div><div class="line">            print(<span class="string">'Saved'</span>, obj, <span class="string">'to mongodb successfully'</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Error occurred while saving'</span>, obj)</div></pre></td></tr></table></figure></p>
<p>可以看到我们在类中定义了 AsyncIOMotorClient 对象，并暴露了 conn_uri 连接字符串和 db 数据库名称，可以在声明 MongoHandler 类的时候指定 MongoDB 的链接地址和数据库名。<br>同样的 process 方法，这里使用 await 修饰了 update_one 方法，完成了异步 MongoDB 存储。<br>好，以上便是 douyin 库的所有的关键部分介绍，这部分内容可以帮助大家理解这个库的核心部分实现，另外可能对设计模式、面向对象思维以及一些实用库的使用有一定的帮助。  </p>
<p>参考：<a href="https://github.com/Python3WebSpider/DouYin" target="_blank" rel="external">https://github.com/Python3WebSpider/DouYin</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI5NDY1MjQzNA==&amp;mid=2247488007&amp;idx=2&amp;sn=476479988cd21e86783c2bd729e0b162&amp;chksm=ec5ecd7adb29446c376cf8b3cdb7925be6759b199c05ba26fefbd029e5ba73a2840ac27edeb3&amp;mpshare=1&amp;scene=1&amp;srcid=1103OLXZhYrY7PEiUvxL57HI#rd" target="_blank" rel="external">不到 10 行代码爬抖音</a>  </p>
<p>附件：<br><a href="https://note.youdao.com/yws/api/personal/file/D319D842BB8C4D6A8F797978C25BC14E?method=download&amp;shareKey=728953d9df759a2ad43fc196e6b27489" target="_blank" rel="external">douyin.py</a><br><a href="https://note.youdao.com/yws/api/personal/file/F075863D139D4644A6D77D564BAD14AA?method=download&amp;shareKey=536691dc6533368dee6579a3b6b7f299" target="_blank" rel="external">Python-3.7.1.tar.xz</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/11/06/Python十行代码爬抖音/">http://www.yfshare.vip/2018/11/06/Python十行代码爬抖音/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天看到一篇文章介绍怎么用python来爬抖音的小视频和音乐。于是兴趣大增，先来试试能不能玩，哈哈。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.yfshare.vip/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.yfshare.vip/tags/Python/"/>
    
      <category term="douyin" scheme="http://www.yfshare.vip/tags/douyin/"/>
    
  </entry>
  
  <entry>
    <title>Python升级</title>
    <link href="http://www.yfshare.vip/2018/11/04/Python%E5%8D%87%E7%BA%A7/"/>
    <id>http://www.yfshare.vip/2018/11/04/Python升级/</id>
    <published>2018-11-03T16:20:00.000Z</published>
    <updated>2018-11-05T16:05:49.973Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Python是一种解释型、面向对象、动态数据类型的高级程序设计语言。<br><a id="more"></a><br>编译安装python3.7.1并支持ssl 模块  </p>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　python 3.7.1<br>　　centos 7.4  </p>
<h4 id="解决依赖关系"><a href="#解决依赖关系" class="headerlink" title="解决依赖关系"></a>解决依赖关系</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># yum install -y openssl openssl-devel libffi-devel zlib*</span></div></pre></td></tr></table></figure>
<h4 id="编译安装python"><a href="#编译安装python" class="headerlink" title="编译安装python"></a>编译安装python</h4><figure class="highlight autoit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root<span class="symbol">@localhost</span> ~]<span class="meta"># wget -c <span class="string">'https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz'</span></span></div><div class="line">[root<span class="symbol">@localhost</span> ~]<span class="meta"># tar -xf Python-3.7.1.tar.xz</span></div><div class="line">[root<span class="symbol">@localhost</span> ~]<span class="meta"># cd Python-3.7.1</span></div><div class="line">[root<span class="symbol">@localhost</span> Python<span class="number">-3.7</span><span class="number">.1</span>]<span class="meta"># ./configure --prefix=/usr/local/python3.7 --with-ssl</span></div><div class="line">[root<span class="symbol">@localhost</span> Python<span class="number">-3.7</span><span class="number">.1</span>]<span class="meta"># make &amp;&amp; make install</span></div></pre></td></tr></table></figure>
<h4 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat /etc/profile.d/python37.sh </span></div><div class="line"><span class="comment">#!/bin/bash</span></div><div class="line">python37=<span class="string">"/usr/local/python3.7"</span></div><div class="line">export PATH=$PATH:$&#123;python37&#125;/bin</div><div class="line">[root@localhost ~]<span class="comment"># source /etc/profile.d/python37.sh</span></div></pre></td></tr></table></figure>
<h4 id="测试python"><a href="#测试python" class="headerlink" title="测试python"></a>测试python</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># python3.7 --version</span></div><div class="line">Python <span class="number">3.7</span><span class="number">.1</span></div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p><img src="https://note.youdao.com/yws/api/personal/file/7CBDCBB3F8B64896AD16B3A66B44877F?method=download&amp;shareKey=b5647bf99905c28ebe479774600eabe7" alt="test_ssl_module">  </p>
<h4 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h4><p>如果不安装libffi-devel，在编译时会报下面的错误<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">File <span class="string">"/root/Python-3.7.0/Lib/ctypes/__init__.py"</span>, line <span class="number">7</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> _ctypes <span class="keyword">import</span> Union, Structure, Array</div><div class="line">ModuleNotFoundError: No module named <span class="string">'_ctypes'</span></div><div class="line">make: *** [install] Error <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>如果不安装openssl，在使用ssl模块时会报错<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">pip is configured with locations that require TLS/SSL, however the ssl module in Python is </div><div class="line">not available.Collecting douyin</div><div class="line">  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after conn</div><div class="line">ection broken by 'SSLError("Can't connect to HTTPS URL because the SSL module is not available.")': /simple/douyin/  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after conn</div><div class="line">ection broken by 'SSLError("Can't connect to HTTPS URL because the SSL module is not available.")': /simple/douyin/  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after conn</div><div class="line">ection broken by 'SSLError("Can't connect to HTTPS URL because the SSL module is not available.")': /simple/douyin/  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after conn</div><div class="line">ection broken by 'SSLError("Can't connect to HTTPS URL because the SSL module is not available.")': /simple/douyin/  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after conn</div><div class="line">ection broken by 'SSLError("Can't connect to HTTPS URL because the SSL module is not available.")': /simple/douyin/  Could not fetch URL https://pypi.org/simple/douyin/: There was a problem confirming the s</div><div class="line">sl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/douyin/ (Caused by SSLError("Can't connect to HTTPS URL because the SSL module is not available.")) - skipping  Could not find a version that satisfies the requirement douyin (from versions: )</div><div class="line">No matching distribution found for douyin</div><div class="line">pip is configured with locations that require TLS/SSL, however the ssl module in Python is </div><div class="line">not available.Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl ce</div><div class="line">rtificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError("Can't connect to HTTPS URL because the SSL module is not available.")) - skipping</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">You are using pip version <span class="number">10.0</span><span class="number">.1</span>, however version <span class="number">18.1</span> <span class="keyword">is</span> available.</div><div class="line">You should consider upgrading via the <span class="string">'pip install --upgrade pip'</span> command.</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># python3.7 -m pip install --upgrade pip</span></div></pre></td></tr></table></figure>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/11/04/Python升级/">http://www.yfshare.vip/2018/11/04/Python升级/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python是一种解释型、面向对象、动态数据类型的高级程序设计语言。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.yfshare.vip/categories/Python/"/>
    
    
      <category term="Python" scheme="http://www.yfshare.vip/tags/Python/"/>
    
      <category term="SSL" scheme="http://www.yfshare.vip/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>node_exporter自定义key</title>
    <link href="http://www.yfshare.vip/2018/09/29/node-exporter%E8%87%AA%E5%AE%9A%E4%B9%89key/"/>
    <id>http://www.yfshare.vip/2018/09/29/node-exporter自定义key/</id>
    <published>2018-09-29T14:04:34.000Z</published>
    <updated>2018-09-29T14:09:54.753Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>在部署完k8s及使用prometheus监控后，发现一个问题啊，那就是能不能像zabbix一样，自己定义key啊，监控自己的服务，毕竟官方提供的key也不能完全覆盖。。<br><a id="more"></a><br>在查阅官网资料发现，prometheus是可以使用自定义key的。<br>官方git地址：<a href="https://github.com/prometheus/node_exporter" target="_blank" rel="external">https://github.com/prometheus/node_exporter</a>  </p>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>官方描述：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">There is varying support <span class="keyword">for</span> collectors on each operating system. The tables below list all existing collectors and the supported systems.</div><div class="line">Collectors are enabled by providing a --collector.&lt;name&gt; flag. Collectors that are enabled by default can be disabled by providing a --no-collector.&lt;name&gt; flag.</div></pre></td></tr></table></figure></p>
<p>即通过官方提供的collector来实现自定义key功能。<br>具体的collector这里就不一一列出了，可<a href="https://github.com/prometheus/node_exporter#enabled-by-default" target="_blank" rel="external">参考这里</a>  </p>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　　Prometheus version 2.2.0<br>　　　node_exporter version 0.15.2<br>　　　Centos 7.4  </p>
<p>这里就以监控nginx页面的状态码为例了，使用的是<code>textfile</code>这个collector  </p>
<h4 id="监控脚本"><a href="#监控脚本" class="headerlink" title="监控脚本"></a>监控脚本</h4><p>编写和zabbix类似的监控脚本并赋予其返回值<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat node_nginx_testpage_status</span></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment">#Monitor nginx test page.</span></div><div class="line">NGINX_PATH=<span class="string">'/etc/nginx'</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="_">-d</span> <span class="string">"<span class="variable">$&#123;NGINX_PATH&#125;</span>"</span> ];<span class="keyword">then</span> </div><div class="line">NGINX_PORT=<span class="string">'80'</span></div><div class="line"><span class="comment">#NGINX_PORT=`grep -v "#"  /etc/nginx/nginx.conf | grep listen |awk -F ";" '&#123;print $1&#125;' |awk '&#123;print $2&#125;'`</span></div><div class="line">/usr/bin/curl -o /dev/null --retry 1 --max-time 3 -w %&#123;http_code&#125; <span class="_">-s</span> <span class="string">"http://127.0.0.1:<span class="variable">$&#123;NGINX_PORT&#125;</span>"</span>| grep -c <span class="string">'200'</span></div><div class="line"><span class="keyword">fi</span></div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># sh node_nginx_testpage_status</span></div><div class="line">1</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat /etc/profile.d/node_exporter.sh</span></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">node_exporter_HOME=<span class="string">'/usr/local/node_exporter'</span></div><div class="line">PATH=<span class="variable">$node_exporter_HOME</span>:<span class="variable">$PATH</span></div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>编写脚本生成key<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat zabbix_runner </span></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment"># Runs a textfile collector.</span></div><div class="line"></div><div class="line">textfile_dir=$(dirname <span class="variable">$0</span>)</div><div class="line"><span class="built_in">source</span> /etc/profile.d/node_exporter.sh</div><div class="line">metric=<span class="string">"<span class="variable">$1</span>"</span></div><div class="line"><span class="built_in">shift</span></div><div class="line">script=<span class="string">"<span class="variable">$textfile_dir</span>/<span class="variable">$metric</span>"</span></div><div class="line">prom_file=<span class="string">"<span class="variable">$textfile_dir</span>/<span class="variable">$metric</span>"</span>.prom</div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ ! -x <span class="string">"<span class="variable">$script</span>"</span> || <span class="_">-d</span> <span class="string">"<span class="variable">$script</span>"</span> ]]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"ERROR: Can't find script for '<span class="variable">$metric</span>'. Aborting."</span></div><div class="line">  <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line">VALUE=`<span class="string">"<span class="variable">$script</span>"</span> <span class="string">"<span class="variable">$@</span>"</span>`</div><div class="line"><span class="keyword">if</span> [[ ! -n <span class="variable">$VALUE</span> ]]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">exit</span> 0</div><div class="line">    <span class="comment"># echo "ERROR: Can't get value for '$metric'. Aborting."</span></div><div class="line">    <span class="comment"># exit 1</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"# TYPE <span class="variable">$&#123;metric&#125;</span> gauge"</span>&gt; <span class="string">"<span class="variable">$prom_file</span>"</span>.$$</div><div class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;metric&#125;</span> <span class="variable">$&#123;VALUE&#125;</span>"</span> &gt;&gt; <span class="string">"<span class="variable">$prom_file</span>"</span>.$$ &amp;&amp; mv <span class="string">"<span class="variable">$prom_file</span>"</span>.$$ <span class="string">"<span class="variable">$prom_file</span>"</span></div><div class="line"><span class="keyword">fi</span> </div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># chmod +x node_nginx_testpage_status</span></div><div class="line">[root@localhost ~]<span class="comment"># sh zabbix_runner node_nginx_testpage_status</span></div></pre></td></tr></table></figure>
<p>zabbix_runner脚本会生成<code>node_nginx_testpage_status.prom</code>文件，该文件记录当前监控指标的状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat node_nginx_testpage_status.prom</span></div><div class="line"><span class="comment"># TYPE node_nginx_testpage_status gauge</span></div><div class="line">node_nginx_testpage_status 1</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>启动node_exporter<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># /usr/local/node_exporter/node_exporter --web.listen-address=:9100 --collector.textfile.directory=/root/</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>--collector.textfile.directory</code>指定<code>textfile</code>收集器读取文件的目录。根据官网说明，<code>textfile</code>收集器会读取以<code>.prom</code>结尾的文件  </li>
</ul>
<p>访问prometheus Dashboard<br><a href="http://ip:9090" target="_blank" rel="external">http://ip:9090</a>  </p>
<p>首先先确认prometheus与node_exporter建立连接<br><img src="https://note.youdao.com/yws/api/personal/file/87B6E9F59C6541F5A6D6A3A2B4C5B700?method=download&amp;shareKey=885e2641c12a47114e2bf203db8943e4" alt="node_exporter_key">  </p>
<p>输入在<code>node_nginx_testpage_status.prom</code>文件中生成的key<br><img src="https://note.youdao.com/yws/api/personal/file/CE364CF089124053A580A8CC5F0314B5?method=download&amp;shareKey=373b947e4f6e88f5be8c61ed89a8bbad" alt="node_exporter_key"><br>这样就完成了node_exporter自定义key  </p>
<p>接下来把其放到定时任务中即可<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat node_exporter_key.sh </span></div><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment">#这里存储所有node_exporter自定义key</span></div><div class="line"></div><div class="line"><span class="comment">#监控脚本存储目录</span></div><div class="line">scripts=<span class="string">'/root'</span></div><div class="line"></div><div class="line"><span class="comment">#zabbix_runner存储目录</span></div><div class="line">zabbix_runner_dir=<span class="string">'/root'</span></div><div class="line"></div><div class="line"><span class="comment">#注：需要进入监控脚本目录后执行命令，否则会报"ERROR: Can't get value for '$metric'. Aborting."。即脚本文件找不到，是脚本目录不对</span></div><div class="line"><span class="comment">#监控nginx testpage</span></div><div class="line"><span class="built_in">cd</span> <span class="variable">$scripts</span></div><div class="line">/bin/bash <span class="variable">$zabbix_runner_dir</span>/zabbix_runner node_nginx_testpage_status</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># crontab -l</span></div><div class="line"><span class="comment">#获取node_exporter自定义key</span></div><div class="line">*/1 * * * * /bin/bash /root/node_exporter_key.sh</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p><img src="https://note.youdao.com/yws/api/personal/file/FF00A411CA8D426994E3A3E2EC7882D4?method=download&amp;shareKey=5814789420b234dec300c1765d1f2e2f" alt="node_exporter_key">  </p>
<p>附件：<br><a href="https://note.youdao.com/yws/api/personal/file/D56F016DBFCB42F39F2263BDCE3306EE?method=download&amp;shareKey=6cf1afcfc8f377aedf9fc34ae6d137b5" target="_blank" rel="external">zabbix_runner</a><br><a href="https://note.youdao.com/yws/api/personal/file/64E262258A4B4FD68F6D888B9AC03FFD?method=download&amp;shareKey=ecd61fdba126cdaca22b57500b7527be" target="_blank" rel="external">node_nginx_testpage_status</a><br><a href="https://note.youdao.com/yws/api/personal/file/67FCF3AD71024400BA69FAEACF5B7D19?method=download&amp;shareKey=84d3478ec225f0a10630075f2e4f4233" target="_blank" rel="external">node_exporter_key.sh</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/09/29/node-exporter自定义key/">http://www.yfshare.vip/2018/09/29/node-exporter自定义key/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在部署完k8s及使用prometheus监控后，发现一个问题啊，那就是能不能像zabbix一样，自己定义key啊，监控自己的服务，毕竟官方提供的key也不能完全覆盖。。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="prometheus" scheme="http://www.yfshare.vip/tags/prometheus/"/>
    
      <category term="node_exporter" scheme="http://www.yfshare.vip/tags/node-exporter/"/>
    
      <category term="key" scheme="http://www.yfshare.vip/tags/key/"/>
    
      <category term="monitor" scheme="http://www.yfshare.vip/tags/monitor/"/>
    
  </entry>
  
  <entry>
    <title>Python运算符</title>
    <link href="http://www.yfshare.vip/2018/08/04/Python%E8%BF%90%E7%AE%97%E7%AC%A6/"/>
    <id>http://www.yfshare.vip/2018/08/04/Python运算符/</id>
    <published>2018-08-04T11:44:54.000Z</published>
    <updated>2018-08-04T12:12:20.475Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>运算符的优先级：<code>+</code>和<code>-</code>优先级最低，<code>*</code>,<code>/</code>,<code>//</code>,<code>%</code>优先级较高，单目运算符<code>+</code>和<code>-</code>优先级更高，乘方的优先级最高<br><a id="more"></a></p>
<h3 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h3><p>环境：<br>　　　python 2.7<br>　　　centos 7.5  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> <span class="number">-2</span> * <span class="number">4</span> + <span class="number">3</span> ** <span class="number">2</span></div><div class="line"><span class="number">1</span></div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<h3 id="算数运算"><a href="#算数运算" class="headerlink" title="算数运算"></a>算数运算</h3><ul>
<li>字符串之间使用加号(<code>+</code>)可以做字符串拼接  </li>
<li>只要除数或被除数中有一个数是浮点数，那么得到的结果也是浮点数  </li>
</ul>
<table><tr><td>运算符</td><td>描述</td><td>示例</td></tr><tr><td>+</td><td>两个对象相加</td><td>&gt;&gt;&gt;print ‘abc’+’def’<br>abcdef<br>&gt;&gt;&gt;print 123+234<br>357<br>&gt;&gt;&gt;</td></tr><tr><td>-</td><td>两个对象相减</td><td>&gt;&gt;&gt;print 123-456<br>-333<br>&gt;&gt;&gt;</td></tr><tr><td>*</td><td>两个数相乘或返回一个重复若干次的序列</td><td>&gt;&gt;&gt;print 123*456<br>56088<br>&gt;&gt;&gt;print ‘abc’*2<br>abcabc<br>&gt;&gt;&gt;</td></tr><tr><td>/</td><td>两个数相除</td><td>&gt;&gt;&gt;print 2/3<br>0<br>&gt;&gt;&gt;print 2/3.0<br>0.666666666667<br>&gt;&gt;&gt;print 2.0/3<br>0.666666666667<br>&gt;&gt;&gt;</td></tr><tr><td>//</td><td>整除，返回商的整数部分</td><td>&gt;&gt;&gt;print 3//2<br>1<br>&gt;&gt;&gt;print 3//2.0<br>1.0<br>&gt;&gt;&gt;</td></tr><tr><td>%</td><td>求余/取模，返回除法的余数</td><td>&gt;&gt;&gt;print 4%3<br>1<br>&gt;&gt;&gt;print 4%3.0<br>1.0<br>&gt;&gt;&gt;print 4%2.0<br>0.0<br>&gt;&gt;&gt;print 4%2<br>0<br>&gt;&gt;&gt;</td></tr><tr><td>**</td><td>求幂/次方</td><td>&gt;&gt;&gt;print 2**3<br>8<br>&gt;&gt;&gt;print 2**3.0<br>8.0<br>&gt;&gt;&gt;</td></tr></table>

<h3 id="赋值运算"><a href="#赋值运算" class="headerlink" title="赋值运算"></a>赋值运算</h3><table><tr><td>运算符</td><td>描述</td><td>实例</td><td>示例</td></tr><tr><td>=</td><td>简单赋值运算符</td><td>a = 5,b = 3,c = a - b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;c = a - b<br>&gt;&gt;&gt;print ‘c的值为:’,c<br>c的值为:2<br>&gt;&gt;&gt;</td></tr><tr><td>+=</td><td>加法赋值运算符</td><td>a+= b 相当于 a = a+b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a+= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:8<br>&gt;&gt;&gt;</td></tr><tr><td>-=</td><td>减法赋值运算符</td><td>a -= b 相当于 a = a -b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a -= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:2<br>&gt;&gt;&gt;</td></tr><tr><td>*=</td><td>乘法赋值运算符</td><td>a *= b 相当于 a = ab</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a *= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:15<br>&gt;&gt;&gt;</td></tr><tr><td>/=</td><td>除法赋值运算符</td><td>a /= b 相当于 a = a / b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a /= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:1<br>&gt;&gt;&gt;</td></tr><tr><td>//=</td><td>取整除赋值运算符</td><td>a //= b 相当于 a = a // b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a //= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:1<br>&gt;&gt;&gt;</td></tr><tr><td>%=</td><td>取模赋值运算符</td><td>a %= b 相当于 a = a % b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a %= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:2<br>&gt;&gt;&gt;</td></tr><tr><td>**=</td><td>幂赋值运算符符</td><td>a **= b 相当于 a = a^b</td><td>&gt;&gt;&gt;a = 5<br>&gt;&gt;&gt;b = 3<br>&gt;&gt;&gt;a **= b<br>&gt;&gt;&gt;print ‘a的值为:’,a<br>a的值为:125<br>&gt;&gt;&gt;</td></tr></table>

<p>python 不支持C语言中的自增1和自减1运算符，因为<code>+</code>和<code>-</code>也是单目运算，python会将<code>--n</code>解释为<code>-(-n)</code>从而得到n，同样，<code>++n</code>的结果也是n。  </p>
<h3 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h3><table><tr><td>运算符</td><td>描述</td><td>示例</td></tr><tr><td>&lt;</td><td>小于，比较运算符返回1表示真，返回0表示假。<br>这分别与特殊的变量True和False等价</td><td>&gt;&gt;&gt;a=21<br>&gt;&gt;&gt;b=10<br>&gt;&gt;&gt;a&gt;b<br>True<br>&gt;&gt;&gt;</td></tr><tr><td>&lt;=</td><td>小于或等于</td><td>&gt;&gt;&gt;a=21<br>&gt;&gt;&gt;b=31<br>&gt;&gt;&gt;a&lt;=b<br>True<br>&gt;&gt;&gt;</td></tr><tr><td>&gt;</td><td>大于</td><td>&gt;&gt;&gt;a=21<br>&gt;&gt;&gt;b=31<br>&gt;&gt;&gt;a&gt;b<br>False<br>&gt;&gt;&gt;</td></tr><tr><td>&gt;=</td><td>大于或等于</td><td>&gt;&gt;&gt;a=31<br>&gt;&gt;&gt;b=21<br>&gt;&gt;&gt;a&gt;=b<br>True<br>&gt;&gt;&gt;</td></tr><tr><td>==</td><td>等于</td><td>&gt;&gt;&gt;a=31<br>&gt;&gt;&gt;b=21<br>&gt;&gt;&gt;a==b<br>False<br>&gt;&gt;&gt;</td></tr><tr><td>!=</td><td>不等于</td><td>&gt;&gt;&gt;a=31<br>&gt;&gt;&gt;b=21<br>&gt;&gt;&gt;a!=b<br>True<br>&gt;&gt;&gt;</td></tr><tr><td>is</td><td>判断两个标识符是否引用自一个对象</td><td>&gt;&gt;&gt;x = [4,5,6]<br>&gt;&gt;&gt;y = [7,8,9]<br>&gt;&gt;&gt;x is y<br>False<br>&gt;&gt;&gt;</td></tr><tr><td>is not</td><td>判断两个标识符是否引用自不同对象</td><td>&gt;&gt;&gt;x = [4,5,6]<br>&gt;&gt;&gt;y = [7,8,9]<br>&gt;&gt;&gt;x is not y<br>True<br>&gt;&gt;&gt;</td></tr></table>

<h3 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h3><h4 id="“真”值测试-Truth-Value-Testing"><a href="#“真”值测试-Truth-Value-Testing" class="headerlink" title="“真”值测试(Truth Value Testing)"></a>“真”值测试(Truth Value Testing)</h4><p>Python中的任何一个对象都可以被测试“真”值。可以这样理解：Python中的任何一个对象都可以转换为一个布尔值，而这个“真”值测试就是获取一个对象对应的布尔值的过程。<br>Python中只有下面这些值对应的布尔值为False：  </p>
<ul>
<li><code>None</code>  </li>
<li><code>False</code>  </li>
<li>数字类型中的<code>0</code>，如：<code>0</code>,<code>0.0</code>,<code>0j</code>  </li>
<li>任意空序列，如：<code>&#39;&#39;</code>,<code>()</code>,<code>[]</code>  </li>
<li>任意一个空映射，如：<code>{}</code>  </li>
<li>一个用户自定义类的实例。该用户自定义类中定义了一个<code>__bool__()</code>或<code>__len__()</code>方法，且实例调用该方法时返回整数0或布尔值False  </li>
</ul>
<p>除此之外，所有的其它值对应的布尔值都是True, 因此许多类型的对象永远是True。”真”值测试可以被用在 if或while条件中，也可以作为布尔操作的操作数。  </p>
<h4 id="布尔运算-Boolean-Operations"><a href="#布尔运算-Boolean-Operations" class="headerlink" title="布尔运算(Boolean Operations)"></a>布尔运算(Boolean Operations)</h4><p>Python中的逻辑运算称为“布尔运算(Boolean Operations)”，操作符包括：and(与)、or(或)、not(非)。  </p>
<table><tr><td>运算符</td><td>逻辑表达式</td><td>描述</td><td>示例</td></tr><tr><td>and</td><td>x and y</td><td>与运算，只有两个布尔值都为 True 时，计算结果才为 True</td><td>&gt;&gt;&gt;x = True<br>&gt;&gt;&gt;y = False<br>&gt;&gt;&gt;x and y<br>False<br>&gt;&gt;&gt;</td></tr><tr><td>or</td><td>x or y</td><td>或运算，只要有一个布尔值为 True，计算结果就是 True</td><td>&gt;&gt;&gt;x = True<br>&gt;&gt;&gt;y = False<br>&gt;&gt;&gt;x or y<br>True<br>&gt;&gt;&gt;</td></tr><tr><td>not</td><td>not x</td><td>非运算，把True变为False，或者把False变为True</td><td>&gt;&gt;&gt;x = True<br>&gt;&gt;&gt;not x<br>False<br>&gt;&gt;&gt;</td></tr></table>

<h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><p>按位运算是指把数字转换为二进制来进行计算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#左边为十进制，右边为二进制</span></div><div class="line"><span class="number">60</span> ，<span class="number">0011</span> <span class="number">1100</span></div><div class="line"><span class="number">13</span> ，<span class="number">0000</span> <span class="number">1101</span></div></pre></td></tr></table></figure></p>
<table><tr><td>运算符</td><td>描述</td><td>实例</td><td>示例</td></tr><tr><td>&amp;</td><td>按位与，参与运算的两个值。<br>如果相应的二进制位都为1，则该位结果为1，否则为0</td><td>a&amp;b对应的二进制结果为 0000 1100，十进制为12</td><td>&gt;&gt;&gt;a=60<br>&gt;&gt;&gt;b=13<br>&gt;&gt;&gt;c=a &amp; b<br>&gt;&gt;&gt;print c<br>12<br>&gt;&gt;&gt;</td></tr><tr><td>|</td><td>按位或运算符，<br>只要对应的二个二进位有一个为1时，结果位就为1</td><td>(a|b)输出结果61，<br>二进制解释： 0011 1101</td><td>&gt;&gt;&gt;a=60<br>&gt;&gt;&gt;b=13<br>&gt;&gt;&gt;c=a|b<br>&gt;&gt;&gt;print c<br>61<br>&gt;&gt;&gt;</td></tr><tr><td>^</td><td>按位异或运算符，<br>当两对应的二进位相异时，结果为1</td><td>(a^b)输出结果49，<br>二进制解释： 0011 0001</td><td>&gt;&gt;&gt;a=60<br>&gt;&gt;&gt;b=13<br>&gt;&gt;&gt;c=a^b<br>&gt;&gt;&gt;print c<br>49<br>&gt;&gt;&gt;</td></tr><tr><td>~</td><td>按位取反运算符，对数据的每个二进制位取反，<br>即把1变为0,把0变为1。~x 类似于-x-1</td><td>(~a)输出结果-61，<br>二进制解释：1100 0011，<br>在一个有符号二进制数的补码形式</td><td>&gt;&gt;&gt;a=60<br>&gt;&gt;&gt;b=~a<br>&gt;&gt;&gt;print b<br>-61<br>&gt;&gt;&gt;</td></tr><tr><td>&lt;&lt;</td><td>左移动运算符，运算数的各二进位全部左移若干位，<br>由 &lt;&lt; 右边的数字指定了移动的位数，高位丢弃，低位补0</td><td>a &lt;&lt; 2输出结果240，<br>二进制解释： 1111 0000</td><td>&gt;&gt;&gt;a=60<br>&gt;&gt;&gt;b=a&lt;<2< br="">&gt;&gt;&gt;print b<br>240<br>&gt;&gt;&gt;</2<></td></tr><tr><td>&gt;&gt;</td><td>右移动运算符，<br>把”&gt;&gt;”左边的运算数的各二进位全部右移若干位，<br>&gt;&gt;右边的数字指定了移动的位数</td><td>a&gt;&gt;2输出结果15，<br>二进制解释：0000 1111</td><td>&gt;&gt;&gt;a=60<br>&gt;&gt;&gt;b=a&gt;&gt;2<br>&gt;&gt;&gt;print b<br>15<br>&gt;&gt;&gt;</td></tr></table>

<h3 id="Python成员运算符"><a href="#Python成员运算符" class="headerlink" title="Python成员运算符"></a>Python成员运算符</h3><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
<th>实例</th>
</tr>
</thead>
<tbody>
<tr>
<td>in</td>
<td>如果在指定的序列中找到值返回 True，否则返回 False</td>
<td>x 在 y 序列中 , 如果 x 在 y 序列中返回 True</td>
</tr>
<tr>
<td>not in</td>
<td>如果在指定的序列中没有找到值返回 True，否则返回 False</td>
<td>x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="comment">#-*- coding: UTF-8 -*-</span></div><div class="line">a=<span class="number">10</span></div><div class="line">b=<span class="number">20</span></div><div class="line"></div><div class="line">list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</div><div class="line"><span class="keyword">if</span> a <span class="keyword">in</span> list:</div><div class="line">   <span class="keyword">print</span> <span class="string">'变量 a 在给定的列表中 list 中'</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">   <span class="keyword">print</span> <span class="string">'变量 a 不在给定的列表中 list 中'</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> b <span class="keyword">not</span> <span class="keyword">in</span> list:</div><div class="line">   <span class="keyword">print</span> <span class="string">'变量 b 不在给定的列表中 list 中'</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">   <span class="keyword">print</span> <span class="string">'变量 b 在给定的列表中 list 中'</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># python in.py</span></div><div class="line">变量 a 不在给定的列表中 list 中</div><div class="line">变量 b 不在给定的列表中 list 中</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<h3 id="Python运算符优先级"><a href="#Python运算符优先级" class="headerlink" title="Python运算符优先级"></a>Python运算符优先级</h3><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>**</code></td>
<td>指数(最高优先级)</td>
</tr>
<tr>
<td><code>~</code>,<code>+</code>,<code>-</code></td>
<td>按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@)</td>
</tr>
<tr>
<td><code>*</code>,<code>/</code>,<code>%</code>,<code>//</code></td>
<td>乘,除,取模和取整除</td>
</tr>
<tr>
<td><code>+</code>,<code>-</code></td>
<td>加法减法</td>
</tr>
<tr>
<td><code>&gt;&gt;</code>,<code>&lt;&lt;</code></td>
<td>右移,左移运算符</td>
</tr>
<tr>
<td><code>&amp;</code></td>
<td>位’AND’</td>
</tr>
<tr>
<td><code>^</code></td>
<td>位运算符</td>
</tr>
<tr>
<td><code>&lt;=</code>,<code>&lt;</code>,<code>&gt;</code>,<code>&gt;=</code></td>
<td>比较运算符</td>
</tr>
<tr>
<td><code>&lt;&gt;</code>,<code>==</code>,<code>!=</code></td>
<td>等于运算符</td>
</tr>
<tr>
<td><code>=</code>,<code>%=</code>,<code>/=</code>,<code>//=</code>,<code>-=</code>,<code>+=</code>,<code>*=</code>,<code>**=</code></td>
<td>赋值运算符</td>
</tr>
<tr>
<td><code>is is not</code></td>
<td>身份运算符</td>
</tr>
<tr>
<td><code>in not in</code></td>
<td>成员运算符</td>
</tr>
<tr>
<td><code>not and or</code></td>
<td>逻辑运算符</td>
</tr>
</tbody>
</table>
<h3 id="单目运算符"><a href="#单目运算符" class="headerlink" title="单目运算符"></a>单目运算符</h3><p>指运算所需变量为一个的运算符，又叫一元运算符，其中有：  </p>
<ul>
<li>逻辑非运算符：<code>!</code>  </li>
<li>按位取反运算符：<code>~</code>  </li>
<li>自增自减运算符：<code>++</code>,<code>–</code>  </li>
<li>负号运算符：<code>-</code>  </li>
<li>类型转换运算符：<code>(类型)</code>  </li>
<li>指针运算符和取地址运算符：<code>*</code>,<code>&amp;</code>  </li>
<li>长度运算符：<code>sizeof</code>等  </li>
</ul>
<h3 id="双目运算符"><a href="#双目运算符" class="headerlink" title="双目运算符"></a>双目运算符</h3><p>指对两个变量进行操作  </p>
<ul>
<li>初等运算符  <ul>
<li>下标运算符：<code>[]</code>  </li>
<li>分量运算符：<code>-&gt;</code>(向结构体成员运算符)  </li>
<li>结构体成员运算符：<code>_</code>  </li>
</ul>
</li>
<li>算术运算符  <ul>
<li>乘法运算符：<code>*</code>  </li>
<li>除法运算符：<code>/</code>  </li>
<li>取余运算符：<code>%</code>  </li>
<li>加法运算符：<code>+</code>  </li>
<li>减法运算符：<code>-</code>  </li>
</ul>
</li>
<li>关系运算符  <ul>
<li>等于运算符：<code>==</code>  </li>
<li>不等于运算符：<code>!=</code>  </li>
<li>关系运算符：<code>&lt;</code>,<code>&gt;</code>,<code>&lt;=</code>,<code>&gt;=</code>  </li>
</ul>
</li>
<li>逻辑运算符  <ul>
<li>逻辑与运算符：<code>&amp;&amp;</code>  </li>
<li>逻辑或运算符：<code>||</code>  </li>
<li>逻辑非运算符：<code>!</code>  </li>
</ul>
</li>
<li>位运算符  <ul>
<li>按位与运算符：<code>&amp;</code>  </li>
<li>按位异或运算符：<code>^</code>  </li>
<li>按位或运算符：<code>|</code>  </li>
<li>左移动运算符：<code>&lt;&lt;</code>  </li>
<li>右移动运算符：<code>&gt;&gt;</code>  </li>
</ul>
</li>
<li>赋值运算符  <ul>
<li>简单赋值运算符：<code>=</code></li>
<li>加法赋值运算符：<code>+=</code></li>
<li>减法赋值运算符：<code>-=</code></li>
<li>乘法赋值运算符：<code>*=</code></li>
<li>除法赋值运算符：<code>/=</code></li>
<li>取模赋值运算符：<code>%=</code></li>
<li>左移动运算符：<code>&gt;&gt;=</code>(对变量进行位运算移位之后的结果再赋值给原来的变量)  </li>
<li>右移动运算符：<code>&lt;&lt;=</code>(同上)</li>
<li>实现按位与运算符：<code>&amp;=</code>，<code>a = iand(a, b) is equivalent to a &amp;= b</code> <a href="https://docs.python.org/2/library/operator.html?highlight=operator" target="_blank" rel="external">参考</a>，<a href="https://blog.csdn.net/u012332571/article/details/70141438" target="_blank" rel="external">参考</a>  </li>
<li>实现按位或赋值运算符：<code>|=</code>，<code>a|=2等价于a=a|2()</code>；<code>a = ior(a, b) is equivalent to a |= b</code>。参考同上  </li>
<li>按位异或赋值运算符：<code>^=</code>；<code>a = ixor(a, b) is equivalent to a ^= b</code>。参考同上  </li>
</ul>
</li>
<li>逗号运算符：<code>,</code>  </li>
</ul>
<h3 id="三目运算符"><a href="#三目运算符" class="headerlink" title="三目运算符"></a>三目运算符</h3><p>三目运算符对三个变量进行操作，指的是计算机c语言的重要组成部分。条件运算符是唯一有3个操作数的运算符，所以有时又称为三元运算符。一般来说，三目运算符的结合性是右结合的。  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/08/04/Python运算符/">http://www.yfshare.vip/2018/08/04/Python运算符/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;运算符的优先级：&lt;code&gt;+&lt;/code&gt;和&lt;code&gt;-&lt;/code&gt;优先级最低，&lt;code&gt;*&lt;/code&gt;,&lt;code&gt;/&lt;/code&gt;,&lt;code&gt;//&lt;/code&gt;,&lt;code&gt;%&lt;/code&gt;优先级较高，单目运算符&lt;code&gt;+&lt;/code&gt;和&lt;code&gt;-&lt;/code&gt;优先级更高，乘方的优先级最高&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://www.yfshare.vip/categories/Python/"/>
    
    
      <category term="python" scheme="http://www.yfshare.vip/tags/python/"/>
    
      <category term="运算符" scheme="http://www.yfshare.vip/tags/%E8%BF%90%E7%AE%97%E7%AC%A6/"/>
    
  </entry>
  
  <entry>
    <title>运维知识体系之操作系统层</title>
    <link href="http://www.yfshare.vip/2018/07/27/%E8%BF%90%E7%BB%B4%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%B1%82/"/>
    <id>http://www.yfshare.vip/2018/07/27/运维知识体系之操作系统层/</id>
    <published>2018-07-27T14:03:19.000Z</published>
    <updated>2018-07-28T10:26:16.370Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>到目前为止，也工作了好几年了吧。曾经某次听赵班长的公开课，听着听着发现，越听越懵…有种啥也不会的感觉..-_-!!!<br>但是呢，这些东西也确实是在工作中用到过，只是比较零碎。某些名词也确实很熟悉，但是呢，比如你现在问我里面具体的细节，我会用一脸懵逼来回应你，哈哈。<br>这些天抽空对着赵班长总结的表格，尝试在网上收集了些资料，整理出下面的文章。如果有不对的地方，欢迎大家指点。在整理这些东西的过程中，也是对我自己所掌握知识的一种回顾吧。让我们为了自己心中所想，一起努力吧！<br><a id="more"></a></p>
<p>整理运维知识体系之操作系统层<br>参考：<a href="https://www.unixhot.com/page/ops" target="_blank" rel="external">https://www.unixhot.com/page/ops</a>  </p>
<table><tr><td>运维架构层级</td><td>运维角度</td><td>内容描述/主要技术关键词</td><td>监控体系</td><td>自动化/DevOps</td><td>云计算</td></tr><tr><td rowspan="5">操作系统层</td><td>CPU</td><td>CPU运行级别、CPU管理(进程管理、taskset、intel VT-X)、使用率、上下文切换、运行队列、进程调度、系统调用</td><td>mpstat、strace</td><td rowspan="5">虚拟化</td><td rowspan="5">公有云弹性计算产品</td></tr><tr><td>内存</td><td>虚拟内存、SWAP换入换出、内存寻址、内存管理（Buffer Cache、HugePages、ksmd、EPT）</td><td>vmstat、free</td></tr><tr><td>I/O(磁盘)</td><td>缺页中断、IOPS(顺序IO、随机IO)、IO管理(IO调度算法、virtio)、VFS</td><td>iostat、iotop</td></tr><tr><td>I/O(网络)</td><td>TCP/IP(三次握手、四次挥手、状态转换、TCP队列)、IO模型、Bonding、Bridge、网络管理(iftop、tcpdump)</td><td>iftop</td></tr><tr><td>内核/Shell</td><td>内核定制、内存参数优化、脚本编程(AWK、Sed、Shell、Python、PHP、Perl、Ruby、Lua)</td><td>系统监控</td></tr></table>

<h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p><a href="https://blog.csdn.net/albenxie/article/details/72885951" target="_blank" rel="external">文档参考</a>  </p>
<ul>
<li>CPU运行级别  <ol>
<li>Intel的 x86处理器是通过Ring级别来进行访问控制的，级别共分4层，RING0,RING1,RING2,RING3。Windows只使用其中的两个级别RING0和RING3。RING0层拥有最高的权限，RING3层拥有最低的权限。  </li>
<li>按照Intel原有的构想，应用程序工作在RING3层，只能访问RING3层的数据，操作系统工作在RING0层，可以访问所有层的数据，而其他驱动程序位于RING1、RING2层，每一层只能访问本层以及权限更低层的数据。RING设计的初衷是将系统权限与程序分离出来，使之能够让OS更好的管理当前系统资源，也使得系统更加稳定。  </li>
<li>应用程序的代码运行在最低运行级别上ring3上，不能做受控操作。如果要做，比如要访问磁盘，写文件，那就要通过执行系统调用（函数），执行系统调用的时候，CPU的运行级别会发生从ring3到ring0的切换，并跳转到系统调用对应的内核代码位置执行，这样内核就为你完成了设备访问，完成之后再从ring0返回ring3。这个过程也称作用户态和内核态的切换。  </li>
<li>驱动程序都是工作在ring0上，否则驱动不了设备。  </li>
</ol>
</li>
<li>intel VT-X  <ol>
<li>虚拟化在这里就遇到了一个难题，因为宿主操作系统是工作在ring0的，客户操作系统就不能也在ring0了。但执行的指令还是一样的，这样肯定不行，因为没权限，跑不起来。一般客户操作系统执行特权指令时，会触发异常（CPU机制，没权限的指令，触发异常），然后VMM捕获这个异常，在异常里面做翻译，模拟，最后返回到客户操作系统内，客户操作系统认为自己的特权指令工作正常，继续运行。但是这个性能损耗非常的大。以前只是简单的执行一条指令，现在却需要复杂的异常处理。  </li>
<li>后来，CPU厂商开始支持虚拟化，支持Intel-VT 的CPU，有VMX root operation 和 VMX non-root operation两种模式，两种模式都支持Ring 0 ~ Ring 3 这 4 个运行级别。这时VMM可以运行在VMX root operation模式下，客户OS运行在VMX non-root operation模式下。即硬件这层做了些区分，在全虚拟化下，有些靠“捕获异常-翻译-模拟”的实现就不需要了。  </li>
</ol>
</li>
<li>进程管理  <ol>
<li>进程是正在运行的程序实体，并且包括这个运行的程序中占据的所有系统资源，如CPU，IO，内存，网络资源等。进程和程序的区别在于：程序是指令的集合，是程序的静态描述，而进程是动态的一次活动的执行。<a href="http://www.cnblogs.com/sun1993/p/7777589.html" target="_blank" rel="external">参考</a>  </li>
</ol>
</li>
<li><p>taskset  </p>
<ol>
<li>LINUX提供的一个命令，可以让某个程序运行在某个（或）某些CPU上。<a href="https://coolshell.cn/articles/7490.html" target="_blank" rel="external">参考性能调优攻略</a>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># taskset -p 31100</span></div><div class="line">pid 31100<span class="string">'s current affinity mask: f</span></div><div class="line"># 显示结果的 f 实际上是二进制4个低位均为1的bitmask，每一个1对应于1个CPU，表示该进程在4个CPU上运行</div></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># taskset -pc 2 31100</span></div><div class="line">pid 31100<span class="string">'s current affinity list: 0-3</span></div><div class="line">pid 31100's new affinity list: 2</div><div class="line"><span class="comment"># 2表示CPU将只会运行在第3个CPU上(从0开始计数)</span></div></pre></td></tr></table></figure>
</li>
<li><p>CPU使用率  </p>
<ol>
<li>反映的是当前cpu的繁忙程度，忽高忽低的原因在于占用cpu处理时间的进程可能处于io等待状态但却还未释放进入wait。  </li>
<li>平均负载(load average)是指某段时间内占用cpu时间的进程和等待cpu时间的进程数，这里等待cpu时间的进程是指等待被唤醒的进程，不包括处于wait状态进程。  </li>
<li>对于每一个CPU来说运行队列最好不要超过3。如果是双核CPU就不要超过6。如果队列长期保持在3以上，说明任何一个进程运行时都不能马上得到cpu的响应，这时可能需要考虑升级cpu。另外满负荷运行cpu的使用率最好是user空间保持在<code>65%～70%</code>，system空间保持在<code>30%</code>，空闲保持在<code>0%~5%</code>。  </li>
<li>top  </li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>%us</td>
<td>用户空间程序的cpu使用率(没有通过nice调度)</td>
</tr>
<tr>
<td>%sy</td>
<td>系统空间的cpu使用率，主要是内核程序</td>
</tr>
<tr>
<td>%ni</td>
<td>用户空间且通过nice调度过的程序的cpu使用率</td>
</tr>
<tr>
<td>%id</td>
<td>空闲cpu</td>
</tr>
<tr>
<td>%wa</td>
<td>cpu运行时在等待io的时间</td>
</tr>
<tr>
<td>%hi</td>
<td>cpu处理硬中断的数量</td>
</tr>
<tr>
<td>%si</td>
<td>cpu处理软中断的数量</td>
</tr>
<tr>
<td>%st</td>
<td>被虚拟机偷走的cpu</td>
</tr>
</tbody>
</table>
<ol>
<li>vmstat  </li>
</ol>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>r</td>
<td>运行队列(多少个进程真的分配到CPU)。当值超过CPU数目，就会出现CPU瓶颈。和CPU的负载有关系，一般负载超过3就比较高，超过5就很高，超过10就不正常，服务器状态很危险。top的负载类似每秒的运行队列，如果队列过大，表示CPU很繁忙，一般会造成CPU使用率很高。</td>
</tr>
<tr>
<td>b</td>
<td>阻塞的进程</td>
</tr>
<tr>
<td>swpd</td>
<td>虚拟内存使用的大小。如果大于0表示机器的物理内存不足</td>
</tr>
<tr>
<td>free</td>
<td>空闲的物理内存大小</td>
</tr>
<tr>
<td>buff</td>
<td>缓存(主要用于块设备)</td>
</tr>
<tr>
<td>cache</td>
<td>缓存(缓存文件)</td>
</tr>
<tr>
<td>si</td>
<td>每秒从磁盘读入虚拟内存的大小。如果大于0表示物理内存不够或内存泄漏</td>
</tr>
<tr>
<td>so</td>
<td>每秒虚拟内存写入磁盘的大小。如果大于0表示物理内存不够或内存泄漏</td>
</tr>
<tr>
<td>bi</td>
<td>块设备每秒写入的块数量，块设备指系统上所有的磁盘和其他设备，默认为1024byte</td>
</tr>
<tr>
<td>bo</td>
<td>块设备每秒读取的块数量，如果读取文件，bo会大于0。bi和bo一般都接近0，不然就是IO过于频繁</td>
</tr>
<tr>
<td>in</td>
<td>每秒CPU的中断次数，包括时间中断</td>
</tr>
<tr>
<td>cs</td>
<td>每秒上下文切换次数，在调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目。每次调用系统函数，代码就会进入内核空间，导致上下文切换，这个很耗资源。</td>
</tr>
<tr>
<td>us</td>
<td>用户CPU使用率</td>
</tr>
<tr>
<td>sy</td>
<td>系统CPU使用率，如果太高，表示系统调用时间长</td>
</tr>
<tr>
<td>id</td>
<td>空闲CPU时间，一般来说，id + us + sy = 100</td>
</tr>
<tr>
<td>wt</td>
<td>等待IO CPU时间</td>
</tr>
</tbody>
</table>
<ol>
<li><p>sar<br>sar命令语法和vmstat一样  </p>
<table><tr><td>命令</td><td>参数</td><td>描述</td></tr><tr><td rowspan="5">sar -q 1 5</td><td>runq-sz</td><td>运行队列的长度(等待运行的进程数)</td></tr><tr><td>plist-sz</td><td>进程列表中进程(processes)和线程(threads)的数量</td></tr><tr><td>ldavg-1</td><td>最后1分钟的系统平均负载(System load average)</td></tr><tr><td>ldavg-5</td><td>过去5分钟的系统平均负载</td></tr><tr><td>ldavg-15</td><td>过去15分钟的系统平均负载</td></tr></table>

<ol>
<li><p>mpstat  </p>
<table><tr><td>命令</td><td>参数</td><td>描述</td></tr><tr><td rowspan="8">mpstat 1 5</td><td>%user</td><td>处理用户进程所使用 CPU 的百分比</td></tr><tr><td>%nice</td><td>使用 nice 命令对进程进行降级时 CPU 的百分比</td></tr><tr><td>%system</td><td>内核进程使用的 CPU 百分比</td></tr><tr><td>%iowait</td><td>等待进行 I/O 所使用的 CPU 时间百分比</td></tr><tr><td>%irq</td><td>用于处理系统中断的 CPU 百分比</td></tr><tr><td>%soft</td><td>软件中断的 CPU 百分比</td></tr><tr><td>%idle</td><td>显示 CPU 的空闲百分比</td></tr><tr><td>%intr/s</td><td>显示每秒 CPU 接收的中断总数</td></tr></table>
</li>
<li><p>pidstat  </p>
<table><tr><td>命令</td><td>参数</td><td>描述</td></tr><tr><td rowspan="6">pidstat</td><td>%user</td><td>处理用户进程所使用 CPU 的百分比</td></tr><tr><td>%system</td><td>内核进程使用的 CPU 百分比</td></tr><tr><td>%guest</td><td>进程在虚拟机占用cpu的百分比</td></tr><tr><td>%CPU</td><td>进程占用cpu的百分比</td></tr><tr><td>CPU</td><td>处理进程的cpu编号</td></tr><tr><td>Command</td><td>当前进程对应的命令</td></tr><tr><td rowspan="6">pidstat -r</td><td>PID</td><td>进程标识符</td></tr><tr><td>Minflt/s</td><td>任务每秒发生的次要错误，不需要从磁盘中加载页</td></tr><tr><td>Majflt/s</td><td>任务每秒发生的主要错误，需要从磁盘中加载页</td></tr><tr><td>VSZ</td><td>虚拟地址大小，虚拟内存的使用KB</td></tr><tr><td>RSS</td><td>常驻集合大小，非交换区五里内存使用KB</td></tr><tr><td>Command</td><td>当前进程对应的命令</td></tr><tr><td rowspan="5">pidstat -d</td><td>PID</td><td>进程id</td></tr><tr><td>kB_rd/s</td><td>每秒从磁盘读取的KB</td></tr><tr><td>kB_wr/s</td><td>每秒写入磁盘KB</td></tr><tr><td>kB_ccwr/s</td><td>任务取消的写入磁盘的KB。当任务截断脏的pagecache的时候会发生</td></tr><tr><td>Command</td><td>当前进程对应的命令</td></tr><tr><td rowspan="4">pidstat -w -p PID</td><td>PID</td><td>进程id</td></tr><tr><td>Cswch/s</td><td>每秒主动任务上下文切换数量</td></tr><tr><td>Nvcswch/s</td><td>每秒被动任务上下文切换数量</td></tr><tr><td>Command</td><td>当前进程对应的命令</td></tr><tr><td rowspan="8">pidstat -t -p PID</td><td>TGID</td><td>主线程号</td></tr><tr><td>TID</td><td>线程id</td></tr><td>%user</td><td>处理用户进程所使用 CPU 的百分比</td><tr><td>%system</td><td>内核进程使用的 CPU 百分比</td></tr><tr><td>%guest</td><td>进程在虚拟机占用cpu的百分比</td></tr><tr><td>%CPU</td><td>进程占用cpu的百分比</td></tr><tr><td>CPU</td><td>处理进程的cpu编号</td></tr><tr><td>Command</td><td>当前进程对应的命令</td></tr><tr><td rowspan="5">pidstat -T ALL -p PID</td><td>PID</td><td>进程标识符</td></tr><tr><td>Usr-ms</td><td>任务和子线程在用户级别使用的毫秒数</td></tr><tr><td>System-ms</td><td>任务和子线程在系统级别使用的毫秒数</td></tr><tr><td>Guest-ms</td><td>任务和子线程在虚拟机(running a virtual processor)使用的毫秒数</td></tr><tr><td>Command</td><td>当前进程对应的命令</td></tr></table>

</li>
</ol>
</li>
</ol>
<ul>
<li><p>上下文切换<br><a href="https://www.jianshu.com/p/8c026542d121" target="_blank" rel="external">文档参考</a><br>现在linux是大多基于抢占式，CPU给每个任务一定的服务时间，当时间片轮转的时候，需要把当前状态保存下来，同时加载下一个任务，这个过程叫做上下文切换。时间片轮转的方式，使得多个任务利用一个CPU执行成为可能，但是保存现场和加载现场，也带来了性能消耗。<br>对于抢占式操作系统，引起上下文切换的原因大致有几下几种：  </p>
<ul>
<li>当前任务的时间片用完之后，系统CPU正常调度下一个任务  </li>
<li>当前任务碰到IO阻塞，调度线程将挂起此任务，继续下一个任务  </li>
<li>多个任务抢占锁资源，当前任务没有抢到，被调度器挂起，继续下一个任务  </li>
<li>用户代码挂起当前任务，让出CPU时间  </li>
<li>硬件中断  </li>
</ul>
<p>监测Linux的应用的时候，当CPU的利用率非常高，但是系统的性能却上不去的时候，不妨监控一下线程/进程的切换，看看是不是context switching导致的overhead过高。常用命令：<code>pidstat</code>，<code>vmstat</code>  </p>
</li>
<li><p>运行队列<br><a href="https://blog.csdn.net/qq_21127313/article/details/54706233" target="_blank" rel="external">文档参考</a>  </p>
<ol>
<li>run-queue：活动（正在运行）和排队的进程数。每个CPU都会维持一个运行队列，理想情况下，调度器会不断让队列中的进程运行。进程不是处在sleep状态就是runable状态。如果CPU过载，就会出现调度器跟不上系统的要求，导致可运行的进程会填满队列。队列愈大，程序执行时间就愈长。</li>
<li>对于每一个CPU来说运行队列最好不要超过3。如果是双核CPU就不要超过6。如果队列长期保持在3以上，说明任何一个进程运行时都不能马上得到cpu的响应，这时可能需要考虑升级cpu。另外满负荷运行cpu的使用率最好是user空间保持在<code>65%～70%</code>，system空间保持在<code>30%</code>，空闲保持在<code>0%~5%</code>。  </li>
</ol>
</li>
<li><p>进程调度<br><a href="https://blog.csdn.net/lishichengyan/article/details/78327220" target="_blank" rel="external">文档参考1</a>，<a href="https://blog.csdn.net/codetz/article/details/51254914" target="_blank" rel="external">文档参考2</a>  </p>
<ol>
<li>CPU调度也叫进程调度。分为：短程调度、中程调度、长程调度。  </li>
<li><p>CPU调度发生的情况：  </p>
<ul>
<li>从运行状态切换到等待状态  </li>
<li>从运行状态切换到就绪状态  </li>
<li>从等待切换到准备就绪  </li>
<li>终止  </li>
</ul>
</li>
<li><p>非抢占式(nonpreemptive)和抢占式(preemptive)调度。前者是指让程序一直运行着，直到它自己出异常；后者允许其他程序抢占现在正在运行的程序。  </p>
</li>
<li>上下文切换。  </li>
</ol>
</li>
<li><p>系统调用<br><a href="https://blog.csdn.net/wrx1721267632/article/details/50547565" target="_blank" rel="external">文档参考1</a>，<a href="https://www.jianshu.com/p/4c8a1242082a" target="_blank" rel="external">文档参考2</a>，<a href="https://blog.csdn.net/jeanter/article/details/51776320" target="_blank" rel="external">文档参考3</a><br> 当用户态的进程调用一个系统调用时，CPU从用户态切换到内核态并开始执行一个内核函数。Linux通过由向量为128（0x80）的编程异常实现CPU由用户态到内核态的转换。  </p>
</li>
</ul>
<h4 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h4><ul>
<li><p>虚拟内存<br><a href="https://blog.csdn.net/youbingchen/article/details/52166318" target="_blank" rel="external">文档参考1</a>，<a href="https://www.cnblogs.com/shijingjing07/p/5611579.html" target="_blank" rel="external">文档参考2</a>  </p>
<ol>
<li>每个进程都有自己独立的内存空间，各个进程的内存空间具有类似的结构。  </li>
<li>Linux内存管理采用的是页式管理，使用的是多级页表，动态地址转换机构与主存、辅存共同实现虚拟内存。  </li>
<li>一个新进程建立的时候，将会建立起自己的内存空间，此进程的数据，代码等从磁盘拷贝到自己的进程空间，哪些数据在哪里，都由进程控制表中的task_struct记录，task_struct中记录中一条链表，记录中内存空间的分配情况，哪些地址有数据，哪些地址无数据，哪些可读，哪些可写，都可以通过这个链表记录。  </li>
<li>每个进程已经分配的内存空间，都与对应的磁盘空间映射。  </li>
<li>对于32位系统，寻址指针为4字节，对应的虚拟地址空间为0-2^32，即0-4G；对于64位系统，寻址指针为8字节，对应的虚拟地址空间为0-2^64，即0-16G。这个地址空间是虚拟的，并非实际存在的。  </li>
</ol>
</li>
<li><p>SWAP换入换出<br><a href="https://blog.csdn.net/jltxgcy/article/details/74783698" target="_blank" rel="external">文档参考</a><br>内存页面分为用户页面和内核页面。<br>关于SWAP的设置，Oracle官方推荐：<br>RAM|Swap Space<br>—|—<br>Up to 512 MB|2 times the size of RAM<br>Between 1024 MB and 2048 MB|1.5 times the size of RAM<br>Between 2049 MB and 8192 MB|Equal to the size of RAM<br>More than 8192 MB|0.75 times the size of RAM</p>
</li>
<li><p>内存寻址<br><a href="http://blog.nsfocus.net/memory-addressing-mode/" target="_blank" rel="external">文档参考1</a>，<a href="https://baike.baidu.com/item/%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80/1012006" target="_blank" rel="external">文档参考2</a><br>内存寻址是指CPU允许支持的内存大小。双通道内存技术其实是一种内存控制和管理技术，它依赖于芯片组的内存控制器发生作用，在理论上能够使两条同等规格内存所提供的带宽增长一倍。计算机管理内存的基本方式有两种：段式管理和页式管理。  </p>
</li>
<li><p>内存管理<br><a href="https://baike.baidu.com/item/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/5633616?fr=aladdin" target="_blank" rel="external">文档参考</a><br>内存管理是指软件运行时对计算机内存资源的分配和使用的技术。其最主要的目的是如何高效，快速的分配，并且在适当的时候释放和回收内存资源。  </p>
</li>
<li><p>Buffer Cache<br><a href="https://blog.csdn.net/qq_34649947/article/details/78774691" target="_blank" rel="external">文档参考</a><br>缓存区cache(磁盘缓存)和缓冲区buffer(内存缓存)都是临时存储区。不同点：  </p>
<ul>
<li>缓冲区buffer主要存在于RAM中，作为CPU暂时存储数据的区域  </li>
<li>高速缓存cache是一种高速存储区域，可以是主存或硬盘等其他独立存储区域的一部分  </li>
</ul>
</li>
<li><p>HugePages<br><a href="https://blog.csdn.net/hijk139/article/details/7656491" target="_blank" rel="external">文档参考</a><br>HugePages是linux内核的一个特性，使用hugepage可以用更大的内存页来取代传统的4K页面。优点有：  </p>
<ul>
<li>没有swap  </li>
<li>减轻快表压力  </li>
<li>减轻换页表的负载  </li>
<li>提高内存的性能，降低CPU负载  </li>
</ul>
</li>
<li><p>Kmsd<br><a href="https://www.cnblogs.com/ck1020/p/6770272.html" target="_blank" rel="external">文档参考</a><br>KSM是内核中的一种内存共享机制。在2.6.36版本的内核中引入，它会合并某些相同的页面以减少页面冗余，在内核中有一个KSM守护进程 ksmd,它定期扫描用户向它注册的内存区，寻找相同的页面，从而用一个添加写保护的页面来代替， 当有进程尝试写入的时候，会自动分配一个新页面，这点就是典型的COW机制。  </p>
</li>
<li><p>EPT<br><a href="https://www.cnblogs.com/ck1020/p/6043054.html" target="_blank" rel="external">文档参考</a><br>在虚拟化环境下，intel CPU在处理器级别加入了对内存虚拟化的支持。即扩展页表EPT，而AMD也有类似的成为NPT。  </p>
</li>
</ul>
<h4 id="I-0-磁盘"><a href="#I-0-磁盘" class="headerlink" title="I/0(磁盘)"></a>I/0(磁盘)</h4><ul>
<li><p>缺页中断<br><a href="https://baike.baidu.com/item/%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/5029040?fr=aladdin" target="_blank" rel="external">文档参考</a><br>缺页中断是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。在这个时候，被内存映射的文件实际上成了一个分页交换文件。  </p>
<ul>
<li>软性页缺失指页缺失发生时，相关的页已经被加载进内存，但是没有向MMU注册的情况。操作系统只需要在MMU中注册相关页对应的物理地址即可。  </li>
<li>硬性页缺失是指相关的页在页缺失发生时未被加载进内存的情况，这时操作系统需要：  <ol>
<li>寻找到一个空闲的页。或者把另外一个使用中的页写到磁盘上（如果其在最后一次写入后发生了变化的话），并注销在MMU内的记录；</li>
<li>将数据读入被选定的页  </li>
<li>向MMU注册该页  </li>
</ol>
</li>
</ul>
</li>
<li><p>IOPS<br><a href="http://blog.51cto.com/wushank/1708168" target="_blank" rel="external">文档参考1</a>，<a href="https://zhuanlan.zhihu.com/p/34895884" target="_blank" rel="external">文档参考2</a><br>IOPS(Input/Output Per Second)即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。IOPS是指单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求数量为单位。  </p>
<ul>
<li>顺序IO指读取和写入操作基于逻辑块逐个连续访问来自相邻地址的数据。在顺序IO访问中，HDD所需的磁道搜索时间显着减少，因为读/写磁头可以以最小的移动访问下一个块。  <ul>
<li>数据备份和日志记录等业务是顺序IO业务。  </li>
</ul>
</li>
<li>随机IO指读写操作时间连续，但访问地址不连续，随机分布在磁盘LUN的地址空间中。  <ul>
<li>产生随机IO的业务有OLTP服务，SQL，即时消息服务等  </li>
</ul>
</li>
</ul>
</li>
<li><p>IO调度<br><a href="https://www.cnblogs.com/albertrui/p/8867559.html" target="_blank" rel="external">文档参考1</a>，<a href="https://www.cnblogs.com/cutepig/p/3403711.html" target="_blank" rel="external">文档参考2</a>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#查看当前系统支持的IO调度算法</span></div><div class="line">dmesg | grep -i scheduler</div><div class="line">[    0.852570] io scheduler noop registered</div><div class="line">[    0.852575] io scheduler deadline registered (default)</div><div class="line">[    0.852614] io scheduler cfq registered</div><div class="line">[    0.852619] io scheduler mq-deadline registered</div><div class="line">[    0.852624] io scheduler kyber registered</div></pre></td></tr></table></figure>
<ul>
<li><p>IO调度算法  </p>
<ul>
<li><p>CFQ(完全公平排队I/O调度程序)<br>特点：  </p>
<ul>
<li>在最新的内核版本和发行版中,都选择CFQ做为默认的I/O调度器。  </li>
<li>CFQ试图均匀地分布对I/O带宽的访问,避免进程被饿死并实现较低的延迟,是deadline和as调度器的折中。  </li>
<li>CFQ对于多媒体应用(video,audio)和桌面系统是最好的选择。  </li>
<li>CFQ赋予I/O请求一个优先级，而I/O优先级请求独立于进程优先级，高优先级的进程的读写不能自动地继承高的I/O优先级。  </li>
</ul>
<p>工作原理：  </p>
<ul>
<li>CFQ为每个进程/线程，单独创建一个队列来管理该进程所产生的请求，也就是说每个进程一个队列,各队列之间的调度使用时间片来调度，以此来保证每个进程都能被很好的分配到I/O带宽。I/O调度器每次执行一个进程的4次请求。  </li>
</ul>
</li>
<li><p>NOOP(电梯式调度程序)<br>特点：  </p>
<ul>
<li>在Linux2.4或更早的版本的调度程序，那时只有这一种I/O调度算法。  </li>
<li>NOOP实现了一个简单的FIFO队列，它像电梯的工作方法一样对I/O请求进行组织，当有一个新的请求到来时，它将请求合并到最近的请求之后，以此来保证请求同一介质。  </li>
<li>NOOP倾向饿死读而利于写。  </li>
<li>NOOP对于闪存设备,RAM,嵌入式系统是最好的选择。  </li>
</ul>
<p>电梯算法饿死读请求的解释：</p>
<ul>
<li>因为写请求比读请求更容易。  </li>
<li>写请求通过文件系统cache，不需要等一次写完成，就可以开始下一次写操作，写请求通过合并，堆积到I/O队列中。  </li>
<li>读请求需要等到它前面所有的读操作完成，才能进行下一次读操作。在读操作之间有几毫秒时间，而写请求在这之间就到来，饿死了后面的读请求。  </li>
</ul>
</li>
<li>Deadline(截止时间调度程序)<br>特点：  <ul>
<li>通过时间以及硬盘区域进行分类，这个分类和合并要求类似于noop的调度程序。  </li>
<li>Deadline确保了在一个截止时间内服务请求，这个截止时间是可调整的，而默认读期限短于写期限。这样就防止了写操作因为不能被读取而饿死的现象。  </li>
<li>Deadline对数据库环境(ORACLE RAC,MYSQL等)是最好的选择。  </li>
</ul>
</li>
<li>AS(预料I/O调度程序)<br>特点：  <ul>
<li>本质上与Deadline一样，但在最后一次读操作后，要等待6ms，才能继续进行对其它I/O请求进行调度。  </li>
<li>可以从应用程序中预订一个新的读请求，改进读操作的执行，但以一些写操作为代价。  </li>
<li>它会在每个6ms中插入新的I/O操作，而会将一些小写入流合并成一个大写入流，用写入延时换取最大的写入吞吐量。  </li>
<li>AS适合于写入较多的环境，比如文件服务器。  </li>
<li>AS对数据库环境表现很差。  </li>
</ul>
</li>
</ul>
</li>
<li><p>virtio<br><a href="https://www.cnblogs.com/bakari/p/8309638.html" target="_blank" rel="external">文档参考</a><br>一种 I/O 半虚拟化解决方案，是一套通用 I/O 设备虚拟化的程序，是对半虚拟化 Hypervisor 中的一组通用 I/O 设备的抽象。提供了一套上层应用与各 Hypervisor虚拟化设备（KVM，Xen，VMware等）之间的通信框架和编程接口，减少跨平台所带来的兼容性问题，大大提高驱动程序开发效率。  </p>
</li>
<li><p>VFS<br><a href="https://baike.baidu.com/item/VFS/7519887?fr=aladdin" target="_blank" rel="external">文档参考1</a>，<a href="http://blog.jobbole.com/105537/" target="_blank" rel="external">文档参考2</a><br>VFS(virtual File System)，也称为虚拟文件系统交换层(Virtual Filesystem Switch)的作用就是采用标准的Unix系统调用读写位于不同物理介质上的不同文件系统，即为各类文件系统提供了一个统一的操作界面和应用编程接口。VFS是一个可以让<code>open()</code>、<code>read()</code>、<code>write()</code>等系统调用不用关心底层的存储介质和文件系统类型就可以工作的粘合层。  </p>
</li>
</ul>
</li>
<li><p>iostat  </p>
<table><tr><td>命令</td><td>参数</td><td>描述</td></tr><tr><td rowspan="13">iostat -x</td><td>rrqm/s</td><td>每秒这个设备相关的读取请求有多少被Merge了(当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge)</td></tr><tr><td>wrqm/s</td><td>每秒这个设备相关的写入请求有多少被Merge了</td></tr><tr><td>r/s</td><td>每秒读取的扇区数</td></tr><tr><td>w/s</td><td>每秒写入的扇区数</td></tr><tr><td>rkB/s</td><td>每秒读K字节数，是 rsect/s 的一半，因为每扇区大小为512字节。(需要计算)</td></tr><tr><td>wkB/s</td><td>每秒写K字节数。是 wsect/s 的一半。(需要计算)</td></tr><tr><td>avgrq-sz</td><td>平均每次设备I/O操作的数据大小(扇区)。delta(rsect+wsect)/delta(rio+wio)</td></tr><tr><td>avgqu-sz</td><td>平均I/O队列长度。即 delta(aveq)/s/1000(因为aveq的单位为毫秒)</td></tr><tr><td>await</td><td>平均每次设备I/O操作的等待时间(毫秒)。即 delta(ruse+wuse)/delta(rio+wio)</td></tr><tr><td>r_await</td><td>发送给要服务的设备的读取请求的平均时间(毫秒)。这包括队列中请求所花费的时间和服务它们所花费的时间。</td></tr><tr><td>w_await</td><td>发送给要服务的设备的写入请求的平均时间(毫秒)。这包括队列中请求所花费的时间和服务它们所花费的时间。</td></tr><tr><td>svctm</td><td>平均每次设备I/O操作的服务时间(毫秒)</td></tr><tr><td>%util</td><td>一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比</td></tr></table>
</li>
<li><p>iotop(yum安装)  </p>
<ul>
<li>左右箭头：改变排序方式，默认是按IO排序  </li>
<li>r：改变排序顺序  </li>
<li>o：只显示有IO输出的进程  </li>
<li>p：进程/线程的显示方式的切换  </li>
<li>a：显示累积使用量  </li>
<li>q：退出  </li>
</ul>
</li>
</ul>
<h4 id="I-O-网络"><a href="#I-O-网络" class="headerlink" title="I/O(网络)"></a>I/O(网络)</h4><ul>
<li><p>TCP三次握手四次挥手<br><a href="https://blog.csdn.net/qzcsu/article/details/72861891" target="_blank" rel="external">文档参考</a>  </p>
<p>最开始的时候客户端和服务器都是处于CLOSED状态。主动打开连接的为客户端，被动打开连接的是服务器。  </p>
<ol>
<li>TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN(监听)状态；  </li>
<li>TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这时报文首部中的同部位SYN=1，同时选择一个初始序列号 seq=x ，此时TCP客户端进程进入了 SYN-SENT(同步已发送状态)状态。TCP规定，SYN报文段(SYN=1的报文段)不能携带数据，但需要消耗掉一个序号。  </li>
<li>TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时TCP服务器进程进入了SYN-RCVD(同步收到)状态。这个报文也不能携带数据，但是同样要消耗一个序号。  </li>
<li>TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时TCP连接建立，客户端进入ESTABLISHED(已建立连接)状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。  </li>
<li>当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。<br><img src="https://note.youdao.com/yws/api/personal/file/35AAA3BE7E634F1CAFA9AA2D33544E69?method=download&amp;shareKey=1e2d9363595b8789c48b8fc0c00b7569" alt="TCP三次握手">  </li>
</ol>
<p>数据传输完毕后，双方都可释放连接。最开始的时候，客户端和服务器都是处于ESTABLISHED状态，然后客户端主动关闭，服务器被动关闭。   </p>
<ol>
<li>客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u(等于前面已经传送过来的数据的最后一个字节的序号加1)，此时客户端进入FIN-WAIT-1(终止等待1)状态。TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。  </li>
<li>服务器收到连接释放报文，发出确认报文ACK=1、ack=u+1，并且带上自己的序列号seq=v，此时服务端就进入了CLOSE-WAIT(关闭等待)状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。  </li>
<li>客户端收到服务器的确认请求后，此时客户端就进入FIN-WAIT-2(终止等待2)状态，等待服务器发送连接释放报文(在这之前还需要接受服务器发送的最后的数据)。  </li>
<li>服务器将最后的数据发送完毕后，就向客户端发送连接释放报文FIN=1、ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时服务器就进入了LAST-ACK(最后确认)状态，等待客户端的确认。  </li>
<li>客户端收到服务器的连接释放报文后，必须发出确认ACK=1、ack=w+1，而自己的序列号是seq=u+1，此时客户端就进入了TIME-WAIT(时间等待)状态。注意此时TCP连接还没有释放，必须经过2*MSL(最长报文段寿命)的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。  </li>
<li>服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。<br><img src="https://note.youdao.com/yws/api/personal/file/9E8B256246D74619AA37B5FFA4AC3AF6?method=download&amp;shareKey=fd03ef9a64d60ff4d98fe6191e857987" alt="TCP四次挥手">  </li>
</ol>
</li>
<li><p>TCP状态变迁<br><a href="https://blog.csdn.net/zjsyhjh/article/details/37056829" target="_blank" rel="external">文档参考</a><br><img src="https://note.youdao.com/yws/api/personal/file/922023EB6468470289645AB3FECF9964?method=download&amp;shareKey=a06e231294004af73c176e9e6a4caa3f" alt="TCP状态变迁图">  </p>
<ul>
<li><p>三次握手:  </p>
<ul>
<li>LISTEN：表示服务器的某个SOCKET处于监听状态，可以进行连接了。  </li>
<li>SYN_SENT：表示客户端的某个SOCKET与服务器进行connect时，首先发送SYN报文，然后进入SYN_SENT状态，等待服务器发送ACK+SYN报文。  </li>
<li>SYN_RECV：表示服务器收到客户端发送的SYN报文，然后向客户端发送SYN+ACK报文，随后服务器进入SYN_RECV状态。  </li>
<li>ESTABLISHED：表示连接已经建立，当客户端在SYN_SENT状态时，收到服务器发送的ACK+SYN报文之后，然后进行第三次握手，客户端发送ACK报文，然后进入ESTABLISHED状态，当处于SYN_RECV状态的服务器收到客户端发送的ACK报文之后，也进入ESTABLISHED状态，然后连接建立。  </li>
</ul>
</li>
<li><p>四次挥手：</p>
<ul>
<li>FIN_WAIT_1：表示客户端SOCKET想主动关闭连接，于是向服务器发送FIN报文，然后进入FIN_WAIT_1状态。  </li>
<li>FIN_WAIT_2：表示客户端收到服务器发来的ACK报文，此时客户端进入FIN_WAIT_2状态，此时客户端这边的连接已经关闭，但服务器端的连接还没关闭，也就是服务器还可以继续向客户端发送数据。  </li>
<li>CLOSING：这种状态表示此时双方刚好可能都在关闭连接，即客户端向服务器发送FIN报文，进入FIN_WAIT_1状态后，没有收到服务器发来的ACK报文，反而受到服务器发来的FIN报文，说明此时客户端和服务器同时发起关闭连接，随后，客户端进入CLOSING状态。  </li>
<li>TIME_WAIT：表示收到了服务器发来的FIN报文，然后客户端发送ACK报文，随后进入TIME_WAIT状态，等待2MSL之后进入CLOSED状态。  </li>
<li>CLOSE_WAIT：表示当服务器收到客户端发来的FIN报文之后，发送ACK报文，随后服务器进入CLOSE_WAIT状态。  </li>
<li>LAST_ACK：表示服务器主动关闭连接，向客户端发送FIN报文后，随即进入LAST_ACK状态，如果收到了客户端发来的ACK报文之后，就进入CLOSED状态。  </li>
</ul>
</li>
</ul>
<p>为何TIME_WAIT需要等2MSL时间才能回到CLOSED状态：<br>如果网络不可靠，那么就无法保证最后客户端发送的ACK报文服务器端一定能够收到，因此处于LAST_ACK状态的服务器可能会因为超时而未收到ACK报文，而重新向客户端发送FIN报文，TIME_WAIT的作用就是用来客户端重新发送可能丢失的ACK报文。  </p>
</li>
<li><p>TCP队列<br><a href="https://yq.aliyun.com/articles/4252" target="_blank" rel="external">文档参考</a>  </p>
<ul>
<li>半连接队列：保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置  </li>
<li>accept队列：保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn,backlog)。其中backlog是我们创建ServerSocket(intport,int backlog)时指定的参数，最终会传递给listen方法。  </li>
</ul>
</li>
<li><p>IO模型<br><a href="https://blog.csdn.net/xiexievv/article/details/44976215" target="_blank" rel="external">文档参考</a>  </p>
<ul>
<li>blocking IO  </li>
<li>nonblocking IO  </li>
<li>IO multiplexing  </li>
<li>signal driven IO(不常用)  </li>
<li>asynchronous IO  </li>
</ul>
</li>
<li><p>网卡Bonding模式<br><a href="https://blog.csdn.net/wuweilong/article/details/39720571" target="_blank" rel="external">文档参考</a>  </p>
<ul>
<li>Mode 0(balance-rr) Round-robin策略，这个模式具备负载均衡和容错能力  </li>
<li>Mode 1(active-backup) 主备策略，在绑定中只有一个网卡被激活，其他处于备份状态  </li>
<li>Mode 2(balance-xor) XOR策略，通过源MAC地址与目的MAC地址做异或操作选择slave网卡  </li>
<li>Mode 3 (broadcast) 广播，在所有的网卡上传送所有的报文  </li>
<li>Mode 4 (802.3ad) IEEE 802.3ad动态链路聚合。创建共享相同的速率和双工模式的聚合组  </li>
<li>Mode 5 (balance-tlb) 适配器传输负载均衡  </li>
<li>Mode 6 (balance-alb) 适配器适应性负载均衡  </li>
</ul>
</li>
<li><p>Bridge<br><a href="https://blog.csdn.net/acs713/article/details/42967191" target="_blank" rel="external">文档参考</a><br>计算机内部一般有系统总线来连接内部所有的硬件设备。一个典型的系统总线是PCI(Peripheral Component Interconnect)总线。其他类型的用得较多的总线还有ISA,EISA,MCA,SCSI,和USB。<br>一个计算机有多个不同类型的总线，这些总线由桥(bridge)链接起来。有以下两种高速总线处理到达或出自内存芯片的数据传输：  </p>
<ol>
<li>前端总线FSB：连接CPU和RAM控制器  </li>
<li>后端总线：连接CPU和外部硬件设备CACHE  </li>
</ol>
</li>
<li><p>网络管理  </p>
<ul>
<li><p>iftop(yum安装)  </p>
<ul>
<li>TX：发送流量  </li>
<li>RX：接收流量  </li>
<li>TOTAL：总流量  </li>
<li>Cumm：运行iftop到目前时间的总流量  </li>
<li>peak：流量峰值  </li>
<li>rates：分别表示过去 2s 10s 40s 的平均流量  </li>
</ul>
</li>
<li><p>tcpdump<br><a href="http://www.itshouce.com.cn/linux/linux-tcpdump.html" target="_blank" rel="external">文档参考</a>  </p>
<ul>
<li>抓取回环网口的包：<code>tcpdump -i lo</code>  </li>
<li>防止包截断：<code>tcpdump -s0</code>  </li>
<li>以数字显示主机及端口：<code>tcpdump -n</code>  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="内核-Shell"><a href="#内核-Shell" class="headerlink" title="内核/Shell"></a>内核/Shell</h4><ul>
<li><p>内核定制<br><a href="https://www.linuxidc.com/Linux/2014-03/97851.htm" target="_blank" rel="external">文档参考1</a>，<a href="https://blog.csdn.net/lixiangminghate/article/details/55224412" target="_blank" rel="external">文档参考2</a>，<a href="http://www.myir-tech.com/customize_linux.asp" target="_blank" rel="external">文档参考3</a><br>linux系统的启动流程：POST自检过程(BIOS) –&gt;如果有多块磁盘，需要在BIOS上选择启动磁盘 –&gt;引导MBR(bootloader引导程序) –&gt; 加载initrd文件 –&gt;执行进程init –&gt;显示欢迎界面<br>Linux系统定制的目的和意义：  </p>
<ul>
<li>系统小型化  </li>
<li>提高实时性  </li>
<li>对特殊硬件的支持  </li>
<li>提高系统的可靠性  </li>
</ul>
</li>
<li><p>Linux内核参数优化<br><a href="https://blog.csdn.net/menxu_work/article/details/51140572" target="_blank" rel="external">文档参考</a>  </p>
<ul>
<li><p><code>sysctl -a</code> 查看所有系统变量<br><code>/proc/sys</code>下内核文件与配置文件sysctl.conf中变量存在着对应关系  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#最大的待发送TCP数据缓冲区空间</span></div><div class="line">net.inet.tcp.sendspace=65536</div><div class="line"></div><div class="line"><span class="comment">#最大的接受TCP缓冲区空间</span></div><div class="line">net.inet.tcp.recvspace=65536</div><div class="line"></div><div class="line"><span class="comment">#最大的接受UDP缓冲区大小</span></div><div class="line">net.inet.udp.sendspace=65535</div><div class="line"></div><div class="line"><span class="comment">#最大的发送UDP数据缓冲区大小</span></div><div class="line">net.inet.udp.maxdgram=65535</div><div class="line"></div><div class="line"><span class="comment">#本地套接字连接的数据发送空间</span></div><div class="line">net.local.stream.sendspace=65535</div><div class="line"></div><div class="line"><span class="comment">#加快网络性能的协议</span></div><div class="line">net.inet.tcp.rfc1323=1</div><div class="line">net.inet.tcp.rfc1644=1</div><div class="line">net.inet.tcp.rfc3042=1</div><div class="line">net.inet.tcp.rfc3390=1</div><div class="line"></div><div class="line"><span class="comment">#最大的套接字缓冲区</span></div><div class="line">kern.ipc.maxsockbuf=2097152</div><div class="line"></div><div class="line"><span class="comment">#系统中允许的最多文件数量</span></div><div class="line">kern.maxfiles=65536</div><div class="line"></div><div class="line"><span class="comment">#每个进程能够同时打开的最大文件数量</span></div><div class="line">kern.maxfilesperproc=32768</div><div class="line"></div><div class="line"><span class="comment">#当一台计算机发起TCP连接请求时，系统会回应ACK应答数据包。该选项设置是否延迟ACK应答数据包，把它和包含数据的数据包一起发送，在高速网络和低负载的情况下会略微提高性能，但在网络连接较差的时候，对方计算机得不到应答会持续发起连接请求，反而会降低性能</span></div><div class="line">net.inet.tcp.delayed_ack=0</div><div class="line"></div><div class="line"><span class="comment">#屏蔽ICMP重定向功能</span></div><div class="line">net.inet.icmp.drop_redirect=1</div><div class="line">net.inet.icmp.log_redirect=1</div><div class="line">net.inet.ip.redirect=0</div><div class="line">net.inet6.ip6.redirect=0</div><div class="line"></div><div class="line"><span class="comment">#防止ICMP广播风暴</span></div><div class="line">net.inet.icmp.bmcastecho=0</div><div class="line">net.inet.icmp.maskrepl=0</div><div class="line"></div><div class="line"><span class="comment">#限制系统发送ICMP速率</span></div><div class="line">net.inet.icmp.icmplim=100</div><div class="line"></div><div class="line"><span class="comment">#安全参数，编译内核的时候加了options TCP_DROP_SYNFIN才可以用</span></div><div class="line">net.inet.icmp.icmplim_output=0</div><div class="line">net.inet.tcp.drop_synfin=1</div><div class="line"></div><div class="line"><span class="comment">#设置为1会帮助系统清除没有正常断开的TCP连接，这增加了一些网络带宽的使用，但是一些死掉的连接最终能被识别并清除。死的TCP连接是被拨号用户存取的系统的一个特别的问题，因为用户经常断开modem而不正确的关闭活动的连接</span></div><div class="line">net.inet.tcp.always_keepalive=1</div><div class="line"></div><div class="line"><span class="comment">#若看到net.inet.ip.intr_queue_drops这个在增加，就要调大net.inet.ip.intr_queue_maxlen，为0最好</span></div><div class="line">net.inet.ip.intr_queue_maxlen=1000</div><div class="line"></div><div class="line"><span class="comment">#防止DOS攻击，默认为30000</span></div><div class="line">net.inet.tcp.msl=7500</div><div class="line"></div><div class="line"><span class="comment">#接收到一个已经关闭的端口发来的所有包，直接drop，如果设置为1则是只针对TCP包</span></div><div class="line">net.inet.tcp.blackhole=2</div><div class="line"></div><div class="line"><span class="comment">#接收到一个已经关闭的端口发来的所有UDP包直接drop</span></div><div class="line">net.inet.udp.blackhole=1</div><div class="line"></div><div class="line"><span class="comment">#为网络数据连接时提供缓冲</span></div><div class="line">net.inet.tcp.inflight.enable=1</div><div class="line"></div><div class="line"><span class="comment">#如果打开的话每个目标地址一次转发成功以后它的数据都将被记录进路由表和arp数据表，节约路由的计算时间,但会需要大量的内核内存空间来保存路由表</span></div><div class="line">net.inet.ip.fastforwarding=0</div><div class="line"></div><div class="line"><span class="comment">#kernel编译打开options POLLING功能，高负载情况下使用低负载不推荐SMP不能和polling一起用</span></div><div class="line"><span class="comment">#kern.polling.enable=1</span></div><div class="line"></div><div class="line"><span class="comment">#并发连接数，默认为128，推荐在1024-4096之间，数字越大占用内存也越大</span></div><div class="line">kern.ipc.somaxconn=32768</div><div class="line"></div><div class="line"><span class="comment">#禁止用户查看其他用户的进程</span></div><div class="line">security.bsd.see_other_uids=0</div><div class="line"></div><div class="line"><span class="comment">#设置kernel安全级别</span></div><div class="line">kern.securelevel=0</div><div class="line"></div><div class="line"><span class="comment">#记录下任何TCP连接</span></div><div class="line">net.inet.tcp.log_in_vain=1</div><div class="line"></div><div class="line"><span class="comment">#记录下任何UDP连接</span></div><div class="line">net.inet.udp.log_in_vain=1</div><div class="line"></div><div class="line"><span class="comment">#防止不正确的udp包的攻击</span></div><div class="line">net.inet.udp.checksum=1</div><div class="line"></div><div class="line"><span class="comment">#防止DOS攻击</span></div><div class="line">net.inet.tcp.syncookies=1</div><div class="line"></div><div class="line"><span class="comment">#仅为线程提供物理内存支持，需要256兆以上内存</span></div><div class="line">kern.ipc.shm_use_phys=1</div><div class="line"></div><div class="line"><span class="comment"># 线程可使用的最大共享内存</span></div><div class="line">kern.ipc.shmmax=67108864</div><div class="line"></div><div class="line"><span class="comment"># 最大线程数量</span></div><div class="line">kern.ipc.shmall=32768</div><div class="line"></div><div class="line"><span class="comment"># 程序崩溃时不记录</span></div><div class="line">kern.coredump=0</div><div class="line"></div><div class="line"><span class="comment"># lo本地数据流接收和发送空间  </span></div><div class="line">net.local.stream.recvspace=65536</div><div class="line">net.local.dgram.maxdgram=16384</div><div class="line">net.local.dgram.recvspace=65536</div><div class="line"></div><div class="line"><span class="comment"># 数据包数据段大小，ADSL为1452</span></div><div class="line">net.inet.tcp.mssdflt=1460</div><div class="line"></div><div class="line"><span class="comment"># 为网络数据连接时提供缓冲  </span></div><div class="line">net.inet.tcp.inflight_enable=1</div><div class="line"></div><div class="line"><span class="comment"># 数据包数据段最小值，ADSL为1452</span></div><div class="line">net.inet.tcp.minmss=1460</div><div class="line"></div><div class="line"><span class="comment"># 本地数据最大数量</span></div><div class="line">net.inet.raw.maxdgram=65536</div><div class="line"></div><div class="line"><span class="comment"># 本地数据流接收空间</span></div><div class="line">net.inet.raw.recvspace=65536</div><div class="line"></div><div class="line"><span class="comment">#ipfw防火墙动态规则数量，默认为4096，增大该值可以防止某些病毒发送大量TCP连接，导致不能建立正常连接</span></div><div class="line">net.inet.ip.fw.dyn_max=65535</div><div class="line"></div><div class="line"><span class="comment">#设置ipf防火墙TCP连接空闲保留时间，默认8640000(120小时)</span></div><div class="line">net.inet.ipf.fr_tcpidletimeout=864000</div></pre></td></tr></table></figure>
</li>
<li><p>参考值(具体根据系统硬件配置对应值)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line">/proc/sys/net/core/wmem_max</div><div class="line"><span class="comment"># 最大socket写buffer,可参考的优化值:873200</span></div><div class="line"></div><div class="line">/proc/sys/net/core/rmem_max</div><div class="line"><span class="comment"># 最大socket读buffer,可参考的优化值:873200</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/tcp_wmem</div><div class="line"><span class="comment"># TCP写buffer,可参考的优化值: 8192 436600 873200</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/tcp_rmem</div><div class="line"><span class="comment"># TCP读buffer,可参考的优化值: 32768 436600 873200</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/tcp_mem</div><div class="line"><span class="comment"># 同样有3个值,意思是:</span></div><div class="line"><span class="comment"># net.ipv4.tcp_mem[0]:低于此值,TCP没有内存压力.</span></div><div class="line"><span class="comment"># net.ipv4.tcp_mem[1]:在此值下,进入内存压力阶段.</span></div><div class="line"><span class="comment"># net.ipv4.tcp_mem[2]:高于此值,TCP拒绝分配socket.</span></div><div class="line"><span class="comment"># 上述内存单位是页,而不是字节.可参考的优化值是:786432 1048576 1572864</span></div><div class="line"></div><div class="line">/proc/sys/net/core/netdev_max_backlog</div><div class="line"><span class="comment"># 进入包的最大设备队列.默认是300,对重负载服务器而言,该值太低,可调整到1000.</span></div><div class="line"></div><div class="line">/proc/sys/net/core/somaxconn</div><div class="line"><span class="comment"># listen()的默认参数,挂起请求的最大数量.默认是128.对繁忙的服务器,增加该值有助于网络性能.可调整到256.</span></div><div class="line"></div><div class="line">/proc/sys/net/core/optmem_max</div><div class="line"><span class="comment"># socket buffer的最大初始化值,默认10K.</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/tcp_max_syn_backlog</div><div class="line"><span class="comment"># 进入SYN包的最大请求队列.默认1024.对重负载服务器,增加该值显然有好处.可调整到2048.</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/tcp_retries2</div><div class="line"><span class="comment"># TCP失败重传次数,默认值15,意味着重传15次才彻底放弃.可减少到5,以尽早释放内核资源.</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/tcp_keepalive_time</div><div class="line">/proc/sys/net/ipv4/tcp_keepalive_intvl</div><div class="line">/proc/sys/net/ipv4/tcp_keepalive_probes</div><div class="line"><span class="comment"># 这3个参数与TCP KeepAlive有关.默认值是:</span></div><div class="line"><span class="comment"># tcp_keepalive_time = 7200 seconds (2 hours)</span></div><div class="line"><span class="comment"># tcp_keepalive_probes = 9</span></div><div class="line"><span class="comment"># tcp_keepalive_intvl = 75 seconds</span></div><div class="line"><span class="comment"># 意思是如果某个TCP连接在idle 2个小时后,内核才发起probe.如果probe 9次(每次75秒)不成功,内核才彻底放弃,认为该连接已失效.对服务器而言,显然上述值太大. 可调整到:</span></div><div class="line"><span class="comment"># /proc/sys/net/ipv4/tcp_keepalive_time 1800</span></div><div class="line"><span class="comment"># /proc/sys/net/ipv4/tcp_keepalive_intvl 30</span></div><div class="line"><span class="comment"># /proc/sys/net/ipv4/tcp_keepalive_probes 3</span></div><div class="line"></div><div class="line">/proc/sys/net/ipv4/ip_local_port_range</div><div class="line"><span class="comment"># 指定端口范围的一个配置,默认是32768 61000,已够大.</span></div><div class="line"></div><div class="line">net.ipv4.tcp_syncookies = 1</div><div class="line"><span class="comment"># 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭</span></div><div class="line"></div><div class="line">net.ipv4.tcp_tw_reuse = 1</div><div class="line"><span class="comment"># 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭</span></div><div class="line"></div><div class="line">net.ipv4.tcp_tw_recycle = 1</div><div class="line"><span class="comment"># 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭</span></div><div class="line"></div><div class="line">net.ipv4.tcp_fin_timeout = 30</div><div class="line"><span class="comment"># 表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间</span></div><div class="line"></div><div class="line">net.ipv4.tcp_keepalive_time = 1200</div><div class="line"><span class="comment"># 表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟</span></div><div class="line"></div><div class="line">net.ipv4.ip_local_port_range = 1024 65000</div><div class="line"><span class="comment"># 表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000</span></div><div class="line"></div><div class="line">net.ipv4.tcp_max_syn_backlog = 8192</div><div class="line"><span class="comment"># 表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数</span></div><div class="line"></div><div class="line">net.ipv4.tcp_max_tw_buckets = 5000</div><div class="line"><span class="comment"># 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为 5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死</span></div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>脚本编程  </p>
<ul>
<li><a href="http://www.runoob.com/linux/linux-comm-awk.html" target="_blank" rel="external">awk参考</a>  </li>
<li><a href="http://man.linuxde.net/sed" target="_blank" rel="external">sed参考</a>  </li>
<li><a href="http://www.runoob.com/linux/linux-shell.html" target="_blank" rel="external">shell参考</a>  </li>
<li><a href="http://www.runoob.com/python/python-tutorial.html" target="_blank" rel="external">python参考</a>  </li>
<li><a href="http://www.runoob.com/php/php-tutorial.html" target="_blank" rel="external">php参考</a>  </li>
<li><a href="http://www.runoob.com/perl/perl-tutorial.html" target="_blank" rel="external">perl参考</a>  </li>
<li><a href="http://www.runoob.com/ruby/ruby-tutorial.html" target="_blank" rel="external">ruby参考</a>  </li>
<li><a href="http://www.runoob.com/lua/lua-tutorial.html" target="_blank" rel="external">lua参考</a>  </li>
</ul>
</li>
<li><p>系统监控<br>在工作中我使用的是Zabbix做系统监控，<a href="https://www.zabbix.com/" target="_blank" rel="external">参考zabbix官网</a><br><img src="https://note.youdao.com/yws/api/personal/file/01ED5C8C063A4E06A9D67414E2B23D2D?method=download&amp;shareKey=fd7008621aae7a9506586165af188fd2" alt="zabbix dashboard">  </p>
</li>
</ul>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/07/27/运维知识体系之操作系统层/">http://www.yfshare.vip/2018/07/27/运维知识体系之操作系统层/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;到目前为止，也工作了好几年了吧。曾经某次听赵班长的公开课，听着听着发现，越听越懵…有种啥也不会的感觉..-_-!!!&lt;br&gt;但是呢，这些东西也确实是在工作中用到过，只是比较零碎。某些名词也确实很熟悉，但是呢，比如你现在问我里面具体的细节，我会用一脸懵逼来回应你，哈哈。&lt;br&gt;这些天抽空对着赵班长总结的表格，尝试在网上收集了些资料，整理出下面的文章。如果有不对的地方，欢迎大家指点。在整理这些东西的过程中，也是对我自己所掌握知识的一种回顾吧。让我们为了自己心中所想，一起努力吧！&lt;br&gt;
    
    </summary>
    
      <category term="运维知识体系" scheme="http://www.yfshare.vip/categories/%E8%BF%90%E7%BB%B4%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"/>
    
    
      <category term="Kernel" scheme="http://www.yfshare.vip/tags/Kernel/"/>
    
      <category term="CPU" scheme="http://www.yfshare.vip/tags/CPU/"/>
    
      <category term="Memory" scheme="http://www.yfshare.vip/tags/Memory/"/>
    
      <category term="I/O" scheme="http://www.yfshare.vip/tags/I-O/"/>
    
  </entry>
  
  <entry>
    <title>部署jenkins项目</title>
    <link href="http://www.yfshare.vip/2018/07/04/%E9%83%A8%E7%BD%B2jenkins%E9%A1%B9%E7%9B%AE/"/>
    <id>http://www.yfshare.vip/2018/07/04/部署jenkins项目/</id>
    <published>2018-07-04T14:56:08.000Z</published>
    <updated>2018-07-10T15:29:10.107Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Jenkins持续集成工具，也玩了不少遍，但很少分享这类文档，今天抽空整理了下文档，和大家分享，哈哈<br><a id="more"></a></p>
<p>回想一下我们之前的发布流程：</p>
<ol>
<li>本地把项目打包  </li>
<li>通过FTP等工具，传输到服务器  </li>
<li>关闭tomcat，把打包的项目放到webapp目录下  </li>
<li>启动tomcat<br>如果每次都这么搞的话，不仅慢，而且容易出错  </li>
</ol>
<p>有了jenkins以后，发布流程：  </p>
<ol>
<li>用户在jenkins上点击某个按钮进行发布  </li>
<li>jenkins收到发布命令  </li>
<li>jenkins从GIT上把源代码download下来  </li>
<li>jenkins根据你设置的mvn命令进行打包  </li>
<li>jenkins把你打包的好的war/jar工程传输到tomcat的webapps目录下  </li>
<li>tomcat启动  </li>
</ol>
<p>对于程序员而言，只要做两件事：  </p>
<ol>
<li>在jenkins上配置某个项目的部署流程  </li>
<li>在jenkins上点击某个项目的部署按钮，进行一键部署  </li>
</ol>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　　Centos 7.5<br>　　　nexus 3.12.0<br>　　　jenkins 2.128<br>　　　maven 3.5.3  </p>
<table>
<thead>
<tr>
<th>项目</th>
<th>IP</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>jenkins</td>
<td>192.168.1.60</td>
<td>基于docker部署</td>
</tr>
<tr>
<td>maven</td>
<td>192.168.1.60</td>
<td>和jenkins集成在一起</td>
</tr>
<tr>
<td>nexus3_oss</td>
<td>192.168.1.61</td>
<td>基于docker部署</td>
</tr>
</tbody>
</table>
<h4 id="环境部署"><a href="#环境部署" class="headerlink" title="环境部署"></a>环境部署</h4><p>安装docker环境<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># yum install -y https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></div><div class="line">[root@localhost ~]<span class="comment"># yum install -y docker-ce</span></div></pre></td></tr></table></figure></p>
<p>docker-compose  </p>
<ul>
<li><p>jenkins  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat jenkins-docker-compose.yml </span></div><div class="line">version: <span class="string">'3'</span></div><div class="line">services:</div><div class="line">  jenkins:</div><div class="line">    image: yfshare/jenkins:2.128</div><div class="line">    container_name: jenkins</div><div class="line">    ports:</div><div class="line">      - 8080:8080</div><div class="line">      - 50000:50000</div><div class="line">    hostname: jenkins.example.com</div><div class="line">    environment:</div><div class="line">      JAVA_OPTS: <span class="string">'-Xms1500m -Xmx1500m -XX:MaxPermSize=512m-Djava.awt.headless=true'</span></div><div class="line">    extra_hosts:</div><div class="line">      jenkins.example.com: 127.0.0.1</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/jenkins_home:/var/jenkins_home</div><div class="line">      - /data/docker_mount/maven_repository:/data/maven/repository</div><div class="line">      - /etc/localtime:/etc/localtime:ro</div><div class="line">    restart: always</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
</li>
<li><p>nexus3_oss  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat nexus3-docker-compose.yml</span></div><div class="line">version: <span class="string">'3'</span></div><div class="line">services:</div><div class="line">  jenkins:</div><div class="line">    image: sonatype/nexus3:3.12.0</div><div class="line">    container_name: nexus3</div><div class="line">    ports:</div><div class="line">      - 8081:8081</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/nexus-data:/nexus-data</div><div class="line">      - /etc/localtime:/etc/localtime:ro</div><div class="line">    restart: always</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>应用环境部署<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># docker-compose -f jenkins-docker-compose.yml up -d</span></div><div class="line">[root@localhost ~]<span class="comment"># docker-compose -f nexus3-docker-compose.yml up -d</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># docker ps -a</span></div><div class="line">CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS                  PORTS                                              NAMES</div><div class="line">56e49974e39d        yfshare/jenkins:2.128     <span class="string">"/sbin/tini -- /usr/…"</span>   2 days ago          Up 42 hours             0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:50000-&gt;50000/tcp   jenkins</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># docker exec -it jenkins /bin/bash</span></div><div class="line">root@jenkins:/<span class="comment"># ps -ef |grep -i jenkins | grep -iv grep</span></div><div class="line">root         1     0  0 Jul05 ?        00:00:01 /sbin/tini -- /usr/<span class="built_in">local</span>/bin/jenkins.sh</div><div class="line">root         5     1  0 Jul05 ?        00:02:04 java -Duser.home=/var/jenkins_home -Xms1500m -Xmx1500m -XX:MaxPermSize=512m -Djava.awt.headless=<span class="literal">true</span> -jar /usr/share/jenkins/jenkins.war</div><div class="line">root@jenkins:/<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># docker ps -a</span></div><div class="line">CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                    NAMES</div><div class="line">28751b5a7cdc        sonatype/nexus3:3.12.0   <span class="string">"sh -c $&#123;SONATYPE_DI…"</span>   20 hours ago        Up 19 hours         0.0.0.0:8081-&gt;8081/tcp   nexus3</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="访问Dashboard"><a href="#访问Dashboard" class="headerlink" title="访问Dashboard"></a>访问Dashboard</h5><p>打开nexus3_oss，访问<a href="http://192.168.1.61:8081" target="_blank" rel="external">http://192.168.1.61:8081</a><br>默认的账户密码为：<code>admin</code>/<code>admin123</code><br><img src="https://note.youdao.com/yws/api/personal/file/97C59350D1AD4FBCA3F211FAF8CB2CC1?method=download&amp;shareKey=0cbc6712ff451180a39d2087bd8df453" alt="nexus3_oss_dashboard">  </p>
<p><code>Online - Remote Available</code>表示nexus仓库正在向nexus远端中央库下载公共依赖包<br><code>Online - Ready to Connect</code>表示nexus仓库当前没有下载任务(向远端nexus中央库)<br><img src="https://note.youdao.com/yws/api/personal/file/854205C4CFCF4A40834C3DE98D687CF3?method=download&amp;shareKey=bed0eb3704e242c38c48a214147f5434" alt="nexus3_oss_dashboard">  </p>
<p>打开jenkins，访问<a href="http://192.168.1.60:8080" target="_blank" rel="external">http://192.168.1.60:8080</a><br>登录密码在日志中查找，在初始化jenkins时也会用到<br><img src="https://note.youdao.com/yws/api/personal/file/7C8C219890AF41679B4859B2FE80BA1E?method=download&amp;shareKey=74e9e2a4ccf14ea86cc02b0bf4ffa882" alt="jenkins_login">  </p>
<p>jenkins安装<code>maven Integration</code>插件，可以配置全局Maven Options<br>“系统管理” –“系统设置” –“Maven项目配置”<br><code>-Xms1024m -Xmx1024m -XX:MaxPermSize=512m</code><br><img src="https://note.youdao.com/yws/api/personal/file/FA529CED364544FEAE47848F09B33157?method=download&amp;shareKey=5342c404e556a95a6b54cfc8e0be0998" alt="maven_option">  </p>
<h4 id="编译项目"><a href="#编译项目" class="headerlink" title="编译项目"></a>编译项目</h4><p>登录jenkins后，点击“新建任务”，然后配置jenkins<br><img src="https://note.youdao.com/yws/api/personal/file/C80C1B7DD57342CBB448CA68EDDA4D37?method=download&amp;shareKey=f83b7fe00795afc7f5fd0080ffedd85b" alt="jenkins_CreateJob"><br><img src="https://note.youdao.com/yws/api/personal/file/9640A76362DF4195ABC95E5E40B47F38?method=download&amp;shareKey=1255e6b6f247182a8b49e2016ccbd518" alt="jenkins_CreateJob">  </p>
<p>在Git上添加相应的账户，Jenkins需要通过此账户从git上clone代码进行编译<br><img src="https://note.youdao.com/yws/api/personal/file/82FAFD7C7096416A9C5AAE9B3873E6A2?method=download&amp;shareKey=012e0b7dc33c9c566d98cae5a9483436" alt="jenkins_CreateJob"><br><img src="https://note.youdao.com/yws/api/personal/file/5FA3ED9850EA4E8DA563E6E21DC157E8?method=download&amp;shareKey=334c78005fae7fad91e47852f3c6d03a" alt="jenkins_CreateJob"><br><img src="https://note.youdao.com/yws/api/personal/file/3EAD197C7BEB4805A03DA51DD30E880C?method=download&amp;shareKey=f964b78ca0039257b5bf711a16e0678f" alt="jenkins_CreateJob"><br><img src="https://note.youdao.com/yws/api/personal/file/A2B447C6ABD54CC2826692759C74AC39?method=download&amp;shareKey=7a3a7d795ac3fdf8832e1e8b295e34d5" alt="jenkins_CreateJob">  </p>
<p>Jenkins配置完成后，就可以编译项目了  </p>
<h5 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h5><p>在编译过程中，会遇到各种依赖问题，需要找开发童鞋沟通获取相应的依赖包<br>总结遇到的错误有（部分）：  </p>
<ol>
<li><p>从Maven私服(nexus)正常下载依赖包  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[INFO] Scanning <span class="keyword">for</span> projects...</div><div class="line">Downloading from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/org/springframework/boot/spring-boot-starter-parent/1.5.1.RELEASE/spring-boot-starter-parent-1.5.1.RELEASE.pom</div><div class="line">Downloaded from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/org/springframework/boot/spring-boot-starter-parent/1.5.1.RELEASE/spring-boot-starter-parent-1.5.1.RELEASE.pom (7.4 kB at 986 B/s)</div><div class="line">Downloading from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/org/springframework/boot/spring-boot-dependencies/1.5.1.RELEASE/spring-boot-dependencies-1.5.1.RELEASE.pom</div><div class="line">Downloaded from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/org/springframework/boot/spring-boot-dependencies/1.5.1.RELEASE/spring-boot-dependencies-1.5.1.RELEASE.pom (89 kB at 12 kB/s)</div><div class="line">Downloading from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/com/fasterxml/jackson/jackson-bom/2.8.6/jackson-bom-2.8.6.pom</div><div class="line">Downloaded from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/com/fasterxml/jackson/jackson-bom/2.8.6/jackson-bom-2.8.6.pom (10 kB at 7.8 kB/s)</div></pre></td></tr></table></figure>
</li>
<li><p>在编译时遇到依赖关系  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># mvn -X package -P pre clean install -Dmaven.test.skip=true</span></div><div class="line">[INFO] Scanning <span class="keyword">for</span> projects...</div><div class="line">[INFO] </div><div class="line">[INFO] ---------------------&lt; com.recycle:recycle-common &gt;---------------------</div><div class="line">[INFO] Building recycle-common 1.0-SNAPSHOT</div><div class="line">[INFO] --------------------------------[ jar ]---------------------------------</div><div class="line">[WARNING] The POM <span class="keyword">for</span> fakepath:json-lib:jar:jdk15:2.4 is missing, no dependency information available</div><div class="line">Downloading from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/ctc-smscloud/jsonhttp/1.0/jsonhttp-1.0.pom</div><div class="line">Downloaded from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/ctc-smscloud/jsonhttp/1.0/jsonhttp-1.0.pom (391 B at 1.8 kB/s)</div><div class="line">Downloading from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/ctc-smscloud/jsonhttp/1.0/jsonhttp-1.0.jar</div><div class="line">Downloaded from deploymentRepo: http://192.168.1.61:8081/repository/maven-public/ctc-smscloud/jsonhttp/1.0/jsonhttp-1.0.jar (5.1 kB at 212 kB/s)</div><div class="line">[INFO] ------------------------------------------------------------------------</div><div class="line">[INFO] BUILD FAILURE</div><div class="line">[INFO] ------------------------------------------------------------------------</div><div class="line">[INFO] Total time: 3.857 s</div><div class="line">[INFO] Finished at: 2018-06-28T09:58:07Z</div><div class="line">[INFO] ------------------------------------------------------------------------</div><div class="line">[WARNING] The requested profile <span class="string">"pre"</span> could not be activated because it does not exist.</div><div class="line">[ERROR] Failed to execute goal on project recycle-common: Could not resolve dependencies <span class="keyword">for</span> project com.recycle:recycle-common:jar:1.0-SNAPSHOT: Failure to find fakepath:json-lib:jar:jdk15:2.4 <span class="keyword">in</span> http://192.168.1.61:8081/repository/maven-public/ was cached <span class="keyword">in</span> the <span class="built_in">local</span> repository, resolution will not be reattempted until the update interval of deploymentRepo has elapsed or updates are forced -&gt; [Help 1]</div><div class="line">[ERROR] </div><div class="line">[ERROR] To see the full stack trace of the errors, re-run Maven with the <span class="_">-e</span> switch.</div><div class="line">[ERROR] Re-run Maven using the -X switch to <span class="built_in">enable</span> full debug logging.</div><div class="line">[ERROR] </div><div class="line">[ERROR] For more information about the errors and possible solutions, please <span class="built_in">read</span> the following articles:</div><div class="line">[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException</div></pre></td></tr></table></figure>
</li>
</ol>
<p>与开发童鞋沟通获取相应的依赖包上传到nexus仓库  </p>
<p>上传到nexus命令<br>需要先安装Maven和配置settings.xml(<code>maven/conf/settings.xml</code>)<br>settings.xml文件配置 <a href="https://note.youdao.com/share/?id=de325335990c1dda2b48dff6cba36e0c&amp;type=note#/" target="_blank" rel="external">参考这里</a>  </p>
<p>通过报错可以看到，缺少json-lib-2.4-jdk15.jar这个依赖jar包，且json-lib-2.4-jdk15.jar这个依赖包存放的路径为：<code>fakepath/json-lib/2.4/json-lib-2.4-jdk15.jar</code><br>手动上传到nexus仓库需要各个参数指定jar的文件名，如下：<br><img src="https://note.youdao.com/yws/api/personal/file/C0222A4B0CC44D6681E444A4A2A65991?method=download&amp;shareKey=d191776a99e98854fa268cf927e4090e" alt="nexus_jar包命名规范"><br><code>Dclassifier</code>字段有的jar依赖包没有就不需要指定该参数<br><code>DgroupId</code>字段表示jar包的目录路径，如果有多级目录写法如上<br><code>Durl</code>字段为上传到nexus仓库的路径<br><code>deploymentRepo</code>字段为maven setting.xml里定义的nexus仓库的密码，具体见settings.xml文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mvn deploy:deploy-file -DgroupId=fakepath -DartifactId=json-lib -Dversion=2.4 -Dclassifier=jdk15 -Dpackaging=jar -Dfile=/data/code/json-lib-2.4-jdk15.jar -Durl=http://192.168.1.61:8081/repository/maven-releases/ -DrepositoryId=<span class="string">"deploymentRepo"</span></div></pre></td></tr></table></figure></p>
<p>上传到nexus私服后，再去maven仓库目录删除对应出错的依赖jar包目录后，再次编译即可<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cd /data/docker_mount/maven_repository</span></div><div class="line"><span class="comment"># rm -rf fakepath/json-lib/</span></div></pre></td></tr></table></figure></p>
<p>编译时再遇到jar包依赖解决方法如上，找开发童鞋获取相应的jar依赖包并上传到nexus仓库。这个上传到nexus仓库开发写好pom.xml文件后应该可以自动上传的，而不需要我们手动上传到nexus仓库  </p>
<p>如果不想通过jenkins来测试jar包是否存在依赖问题，可以配置好maven后并在该服务器上手动执行如下命令测试<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/recycle-back/</div><div class="line">mvn clean -P pre</div><div class="line">mvn package -P pre clean install -Dmaven.test.skip=<span class="literal">true</span></div></pre></td></tr></table></figure></p>
<p>附上一张jenkins编译失败的图片：<br><img src="https://note.youdao.com/yws/api/personal/file/C5BE4325907640F2A04900991B67BEB3?method=download&amp;shareKey=ecc35dd5eb2d4a7dc5bff8ff76f9f814" alt="jenkins_build_fail">  </p>
<h5 id="编译成功"><a href="#编译成功" class="headerlink" title="编译成功"></a>编译成功</h5><p>披荆斩棘，经过九九八十一难后，jenkins编译成功啦，结果如下<br><img src="https://note.youdao.com/yws/api/personal/file/8079B3B04F1748859525928C21A16BD8?method=download&amp;shareKey=bef4cd9d6de736e8de03baeecbe090de" alt="jenkins_built_result">  </p>
<p>找到jenkins的$WORKSPACE 目录，进入到项目里面的可以看到编译成功后的jar包文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ls recycle-*/target/*.jar | grep -iv 'dubbo-privider'</span></div><div class="line">recycle-activity/target/recycle-activity.jar</div><div class="line">recycle-common/target/recycle-common-1.0-SNAPSHOT.jar</div><div class="line">recycle-front/target/recycle-front.jar</div><div class="line">recycle-message/target/recycle-message.jar</div><div class="line">recycle-product/target/recycle-product.jar</div><div class="line">recycle-risk/target/recycle-risk.jar</div><div class="line">recycle-task/target/recycle-task.jar</div><div class="line">recycle-trade/target/recycle-trade.jar</div><div class="line">recycle-user/target/recycle-user.jar</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h4 id="项目发布"><a href="#项目发布" class="headerlink" title="项目发布"></a>项目发布</h4><p><img src="https://note.youdao.com/yws/api/personal/file/CCFC6463CAB24CB48333BA23A7DEA36B?method=download&amp;shareKey=679ae369e6fd8cee53e1438e3a6bec39" alt="Jenkins_ExecuteShell"><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">BUILD_ID=DONTKILLME</div><div class="line"></div><div class="line"><span class="built_in">cd</span> <span class="variable">$WORKSPACE</span>/king-back/</div><div class="line">mvn clean -P pre</div><div class="line">mvn package -P pre clean install -Dmaven.test.skip=<span class="literal">true</span></div><div class="line">result=$?</div><div class="line"><span class="keyword">if</span> [ <span class="variable">$result</span> == <span class="string">'0'</span> ];<span class="keyword">then</span></div><div class="line">find . -type f -iname <span class="string">"king*.jar"</span> | egrep -iv <span class="string">'common|dubbo-privider|api'</span> | xargs -i scp &#123;&#125; user@ip:/data/king/project/</div><div class="line">find . -type f -iname <span class="string">"king*.war"</span> | egrep -iv <span class="string">'common|dubbo-privider|api'</span> | xargs -i scp &#123;&#125; user@ip:/data/king/project/</div><div class="line"></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-front.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-mis.war' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-task.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-trade.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-risk.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-user.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-activity.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-product.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep 'king-message.jar' |grep -v grep |awk '&#123;print \$2&#125;' |xargs -i kill &#123;&#125;"</span></div><div class="line"></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-message.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-user.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-trade.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-risk.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-task.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-product.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-activity.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms600m -Xmx600m /data/king/project/king-mis.war &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line">ssh -fn user@ip <span class="string">"nohup java -jar -Xms350m -Xmx350m /data/king/project/king-front.jar &gt;/dev/null 2&gt;&amp;1 &amp;"</span></div><div class="line"></div><div class="line">ssh -fn user@ip <span class="string">"ps -ef | grep -i king | egrep -iv 'grep|color'"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">	<span class="built_in">echo</span> <span class="string">'编译失败'</span></div><div class="line">    <span class="built_in">exit</span> 1</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/0D70F4B55CAF4B78A88930AD14298D45?method=download&amp;shareKey=dbcaa6ff3eae1b55c9fbd16e77f55f8e" alt="jenkins_build_fail"><br><img src="https://note.youdao.com/yws/api/personal/file/1A91736DAF704FFE80F2576ACF0C1CED?method=download&amp;shareKey=cdff65123ea737392cc440fe4fc912d7" alt="jenkins_build_success">  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/07/04/部署jenkins项目/">http://www.yfshare.vip/2018/07/04/部署jenkins项目/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Jenkins持续集成工具，也玩了不少遍，但很少分享这类文档，今天抽空整理了下文档，和大家分享，哈哈&lt;br&gt;
    
    </summary>
    
      <category term="Jenkins" scheme="http://www.yfshare.vip/categories/Jenkins/"/>
    
    
      <category term="Jenkins" scheme="http://www.yfshare.vip/tags/Jenkins/"/>
    
      <category term="docker" scheme="http://www.yfshare.vip/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>部署Zabbix 3.4</title>
    <link href="http://www.yfshare.vip/2018/06/30/%E9%83%A8%E7%BD%B2Zabbix-3-4/"/>
    <id>http://www.yfshare.vip/2018/06/30/部署Zabbix-3-4/</id>
    <published>2018-06-30T12:30:00.000Z</published>
    <updated>2019-03-14T14:02:49.878Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>虽然zabbix已经玩了好多遍了，但每次重新部署时，还得到处找，今天抽空整理下文档，以后就可以直接复制粘贴了，哈哈<br><a id="more"></a></p>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　centos 7.4<br>　　zabbix 3.4.11<br>　　mysql 5.6<br>　　php 5.4<br>　　nginx 12.2  </p>
<p>zabbix 3.4环境要求：<br>参考：<a href="https://www.zabbix.com/documentation/3.4/manual/installation/requirements" target="_blank" rel="external">zabbix requirements</a>  </p>
<ul>
<li>MySQL 5.0.3 - 5.7.x</li>
<li>PHP 5.4.0 or later</li>
</ul>
<h4 id="安装yum源"><a href="#安装yum源" class="headerlink" title="安装yum源"></a>安装yum源</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># wget https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</span></div><div class="line">[root@localhost ~]<span class="comment"># wget https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm</span></div><div class="line"></div><div class="line"><span class="comment">#如果网络不好，可以用epel源自带的php5.4</span></div><div class="line">[root@localhost ~]<span class="comment"># wget https://mirror.webtatic.com/yum/el7/webtatic-release.rpm</span></div><div class="line">[root@localhost ~]<span class="comment"># wget https://jaist.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/3.4.11/zabbix-3.4.11.tar.gz</span></div><div class="line">[root@localhost ~]<span class="comment"># yum -y install epel-release-latest-7.noarch.rpm mysql80-community-release-el7-1.noarch.rpm webtatic-release.rpm</span></div></pre></td></tr></table></figure>
<h4 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># yum -y install yum-utils gcc gcc-c++ make libxml2 libxml2-devel libjpeg-devel libpng-devel bzip2-devel libcurl-devel gd-devel net-snmp-devel libevent-devel</span></div></pre></td></tr></table></figure>
<h4 id="部署PHP"><a href="#部署PHP" class="headerlink" title="部署PHP"></a>部署PHP</h4><p>如果能使用webtatic源的话，可以安装高版本的PHP。这里用不了webtatic的yum源。-_-!!!<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#安装php5.4</span></div><div class="line">[root@localhost ~]<span class="comment"># yum -y install php php-mysql php-gd libjpeg* php-imap php-odbc libmcrypt libmcrypt-devel php-fpm php-cli php-bcmath php-mbstring php-xml</span></div><div class="line"><span class="comment">#安装php70w  </span></div><div class="line">[root@localhost ~]<span class="comment"># yum install php70w php70w-cli php70w-common php70w-fpm php70w-gd php70w-devel php70w-imap php70w-mysql php70w-odbc php70w-bcmath php70w-mbstring php70w-xml</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#配置PHP  </span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/max_execution_time/s/30/300/' /etc/php.ini</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/post_max_size/s/8/16/' /etc/php.ini</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/max_input_time/s/60/300/' /etc/php.ini</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^;date.timezone/adate.timezone \= \"Asia\/Shanghai\"' /etc/php.ini</span></div><div class="line"><span class="comment">#yum安装的php下面配置不需要修改</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/run\/php-fpm.pid/s/^;//g' /etc/php-fpm.d/www.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^;pm.min_spare_servers/s/^;//g' /etc/php-fpm.d/www.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^;pm.max_spare_servers/s/^;//g' /etc/php-fpm.d/www.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^;pm.start_servers/s/^;//g' /etc/php-fpm.d/www.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># systemctl enable php-fpm</span></div><div class="line">[root@localhost ~]<span class="comment"># systemctl start php-fpm</span></div></pre></td></tr></table></figure>
<h4 id="部署Mysql-5-6"><a href="#部署Mysql-5-6" class="headerlink" title="部署Mysql 5.6"></a>部署Mysql 5.6</h4><p>这里也操作了mysql迁移DATA目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># yum-config-manager --disable mysql80-community</span></div><div class="line">[root@localhost ~]<span class="comment"># yum-config-manager --enable mysql56-community</span></div><div class="line">[root@localhost ~]<span class="comment"># yum install -y mysql-community-server.x86_64 mysql-community-devel.x86_64 mysql-community-common.x86_64 mysql-community-client.x86_64</span></div><div class="line">[root@localhost ~]<span class="comment"># systemctl enable mysqld</span></div><div class="line">[root@localhost ~]<span class="comment"># systemctl start mysqld</span></div></pre></td></tr></table></figure></p>
<h5 id="迁移mysql-data目录"><a href="#迁移mysql-data目录" class="headerlink" title="迁移mysql data目录"></a>迁移mysql data目录</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># mysql -u root</span></div><div class="line">mysql&gt; use mysql</div><div class="line">mysql&gt; update user <span class="built_in">set</span> password=password(<span class="string">'123456'</span>) <span class="built_in">where</span> user=<span class="string">'root'</span>;</div><div class="line">mysql&gt; SELECT Host,User,password_expired FROM user;</div><div class="line">mysql&gt; update user <span class="built_in">set</span> host=<span class="string">'192.168.%.%'</span>,password_expired=<span class="string">'N'</span> <span class="built_in">where</span> host=<span class="string">'127.0.0.1'</span>;</div><div class="line">mysql&gt; grant all privileges on *.* to <span class="string">'root'</span>@<span class="string">'localhost'</span> identified by <span class="string">'123456'</span> with grant option;</div><div class="line">mysql&gt; grant all privileges on *.* to <span class="string">'root'</span>@<span class="string">'192.168.%.%'</span> identified by <span class="string">'123456'</span> with grant option;</div><div class="line">mysql&gt; flush privileges;</div><div class="line">mysql&gt; commit;</div><div class="line">mysql&gt; show global variables like <span class="string">"%datadir%"</span>;</div><div class="line">+---------------+-----------------+</div><div class="line">| Variable_name | Value           |</div><div class="line">+---------------+-----------------+</div><div class="line">| datadir       | /var/lib/mysql/ |</div><div class="line">+---------------+-----------------+</div><div class="line">1 row <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</div><div class="line"></div><div class="line">mysql&gt; quit</div><div class="line">[root@localhost ~]<span class="comment"># systemctl stop mysqld</span></div><div class="line">[root@localhost ~]<span class="comment"># mkdir -p /data</span></div><div class="line">[root@localhost ~]<span class="comment"># mv /var/lib/mysql /data/mysql_data</span></div><div class="line">[root@localhost ~]<span class="comment"># grep -iv '#' /etc/my.cnf | grep -iv '^$'</span></div><div class="line">[mysqld]</div><div class="line"><span class="comment">#修改为新目录</span></div><div class="line">datadir=/data/mysql_data</div><div class="line">socket=/data/mysql_data/mysql.sock</div><div class="line">symbolic-links=0</div><div class="line">sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES </div><div class="line">[mysqld_safe]</div><div class="line"><span class="built_in">log</span>-error=/var/<span class="built_in">log</span>/mysqld.log</div><div class="line">pid-file=/var/run/mysqld/mysqld.pid</div><div class="line"><span class="comment">#添加</span></div><div class="line">[client]</div><div class="line">socket=/data/mysql_data/mysql.sock</div><div class="line">[root@localhost ~]<span class="comment"># systemctl start mysqld</span></div></pre></td></tr></table></figure>
<h4 id="部署zabbix"><a href="#部署zabbix" class="headerlink" title="部署zabbix"></a>部署zabbix</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># useradd -s /sbin/nologin zabbix</span></div><div class="line">[root@localhost ~]<span class="comment"># tar -zxf zabbix-3.4.11.tar.gz</span></div><div class="line">[root@localhost ~]<span class="comment"># cd zabbix-3.4.11</span></div><div class="line">[root@localhost zabbix-3.4.11]<span class="comment"># ./configure --prefix=/usr/local/zabbix --enable-server --enable-agent --with-mysql --with-net-snmp --with-libcurl --with-libxml2</span></div><div class="line">[root@localhost zabbix-3.4.11]<span class="comment"># make &amp;&amp; make install</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#在编译zabbix-server时，`--enable-java`需要安装下面这个依赖包  </span></div><div class="line">[root@localhost ~]<span class="comment"># yum install -y java*</span></div></pre></td></tr></table></figure>
<h5 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># mysql -uroot -p123456</span></div><div class="line">mysql&gt; create database zabbix default charset utf8;</div><div class="line">mysql&gt; grant all privileges on zabbix.* to zbxuser@localhost identified by <span class="string">'zbxpass'</span>;</div><div class="line">mysql&gt; grant all privileges on zabbix.* to zbxuser@<span class="string">'192.168.%.%'</span> identified by <span class="string">'zbxpass'</span>;</div><div class="line">mysql&gt; flush privileges;</div><div class="line">mysql&gt; commit;</div><div class="line">mysql&gt; quit</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cd zabbix-3.4.11</span></div><div class="line">[root@localhost zabbix-3.4.11]<span class="comment"># mysql -uzbxuser -pzbxpass zabbix &lt; database/mysql/schema.sql</span></div><div class="line">[root@localhost zabbix-3.4.11]<span class="comment"># mysql -uzbxuser -pzbxpass zabbix &lt; database/mysql/images.sql</span></div><div class="line">[root@localhost zabbix-3.4.11]<span class="comment"># mysql -uzbxuser -pzbxpass zabbix &lt; database/mysql/data.sql</span></div></pre></td></tr></table></figure>
<h5 id="配置zabbix"><a href="#配置zabbix" class="headerlink" title="配置zabbix"></a>配置zabbix</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># sed -i '/LogFile/s/tmp/var\/log\/zabbix/' /usr/local/zabbix/etc/zabbix_server.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^DBUser\=/s/root/zbxuser/' /usr/local/zabbix/etc/zabbix_server.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/# DBPassword/aDBPassword=zbxpass' /usr/local/zabbix/etc/zabbix_server.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/# DBPort/aDBPort=3306' /usr/local/zabbix/etc/zabbix_server.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^Timeout/s/4/30/' /usr/local/zabbix/etc/zabbix_server.conf</span></div><div class="line"><span class="comment">#如果修改了数据库的data目录，也需要修改zabbix server配置文件</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^# DBSocket/aDBSocket=/data/mysql_data/mysql.sock' /usr/local/zabbix/etc/zabbix_server.conf</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^Server/s/127.0.0.1/192.168.1.202/g' /usr/local/zabbix/etc/zabbix_agentd.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i "/^ServerActive/s/127.0.0.1/192.168.1.202:10051/g" /usr/local/zabbix/etc/zabbix_agentd.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i "/^Hostname/s/Zabbix server/192.168.1.202/g" /usr/local/zabbix/etc/zabbix_agentd.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># sed -i '/^LogFile/s/tmp/var\/log\/zabbix/' /usr/local/zabbix/etc/zabbix_agentd.conf</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># ln -s /usr/local/zabbix/sbin/* /usr/local/sbin/</span></div><div class="line">[root@localhost ~]<span class="comment"># ln -s /usr/local/zabbix/bin/* /usr/local/sbin/</span></div><div class="line">[root@localhost ~]<span class="comment"># mkdir -p /var/log/zabbix</span></div><div class="line">[root@localhost ~]<span class="comment"># touch /var/log/zabbix/&#123;zabbix_agentd.log,zabbix_server.log&#125;</span></div><div class="line">[root@localhost ~]<span class="comment"># chown zabbix:zabbix /var/log/zabbix/ -R</span></div><div class="line">[root@localhost ~]<span class="comment"># /usr/local/zabbix/sbin/zabbix_server -c /usr/local/zabbix/etc/zabbix_server.conf</span></div><div class="line">[root@localhost ~]<span class="comment"># /usr/local/zabbix/sbin/zabbix_agentd -c /usr/local/zabbix/etc/zabbix_agentd.conf</span></div><div class="line"></div><div class="line"><span class="comment">#或者用启动脚本启动，zabbix源码自带启动脚本  </span></div><div class="line">[root@localhost ~]<span class="comment"># cp -a zabbix-3.4.11/misc/init.d/fedora/core/zabbix_* /etc/init.d/</span></div><div class="line">[root@localhost ~]<span class="comment"># chmod +x /etc/init.d/zabbix_*</span></div><div class="line">[root@localhost ~]<span class="comment"># /etc/init.d/zabbix_server restart</span></div><div class="line">[root@localhost ~]<span class="comment"># /etc/init.d/zabbix_agentd restart</span></div></pre></td></tr></table></figure>
<h4 id="部署Nginx"><a href="#部署Nginx" class="headerlink" title="部署Nginx"></a>部署Nginx</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># yum install -y nginx</span></div><div class="line">[root@localhost ~]<span class="comment"># mkdir /usr/share/nginx/html/zabbix</span></div><div class="line">[root@localhost ~]<span class="comment"># cd zabbix-3.4.11</span></div><div class="line">[root@localhost zabbix-3.4.11]<span class="comment"># cp -a frontends/php/* /usr/share/nginx/html/zabbix/</span></div><div class="line">[root@localhost ~]<span class="comment"># chown zabbix:zabbix /usr/share/nginx/html/zabbix/ -R</span></div><div class="line">[root@localhost ~]<span class="comment"># chmod o+w /usr/share/nginx/html/zabbix/conf -R</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat /etc/nginx/conf.d/zabbix.conf </span></div><div class="line">server &#123;</div><div class="line">	listen	80;</div><div class="line">	server_name 192.168.1.202;</div><div class="line">	root /usr/share/nginx/html;</div><div class="line"></div><div class="line">	access_log /var/<span class="built_in">log</span>/nginx/zabbix_access.log main;</div><div class="line">	error_log /var/<span class="built_in">log</span>/nginx/zabbix_error.log warn;</div><div class="line"></div><div class="line">	location /zabbix &#123;</div><div class="line">                index index.php;</div><div class="line">                try_files <span class="variable">$uri</span> <span class="variable">$uri</span>/ /index.php?<span class="variable">$args</span>;</div><div class="line">        &#125;</div><div class="line">        location ~ ^(.+.php)(.*)$ &#123;</div><div class="line">                fastcgi_buffer_size 128k;</div><div class="line">                fastcgi_buffers 32 32k;</div><div class="line">                fastcgi_split_path_info ^(.+.php)(.*)$;</div><div class="line">                include fastcgi.conf;</div><div class="line">                fastcgi_pass  127.0.0.1:9000;</div><div class="line">                fastcgi_index index.php;</div><div class="line">                fastcgi_param  PATH_INFO          <span class="variable">$fastcgi_path_info</span>;</div><div class="line">        &#125;</div><div class="line">&#125;</div><div class="line">[root@localhost ~]<span class="comment">#</span></div><div class="line">[root@localhost ~]<span class="comment"># systemctl enable nginx</span></div><div class="line">[root@localhost ~]<span class="comment"># systemctl start nginx</span></div></pre></td></tr></table></figure>
<h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p><img src="https://note.youdao.com/yws/api/personal/file/A597A39410474658A3313ADC3EDF68E4?method=download&amp;shareKey=480d9268f2992b63b82774adf23e4502" alt="zabbix_install">  </p>
<p>如果点”next step”没有反应，需要检查下<code>/var/lib/php/session</code>权限，如果没有这个目录就新建一个并授权777<br><img src="https://note.youdao.com/yws/api/personal/file/9C64C8CEBAA6455BBC13A5447D304C5A?method=download&amp;shareKey=6cbd04ad3bdd9ff34c6e333ea90e388c" alt="zabbix_install"><br><img src="https://note.youdao.com/yws/api/personal/file/BAA9FD90A2AA411D97BBC7C1C134308B?method=download&amp;shareKey=79ef66cf8aa8f17476559c4603b94a2f" alt="zabbix_install"><br><img src="https://note.youdao.com/yws/api/personal/file/81B49818E1554BE4B84AC75F80018219?method=download&amp;shareKey=50231b404b3f603fc8abc470d354d113" alt="zabbix_install"><br><img src="https://note.youdao.com/yws/api/personal/file/0D5392E253F8418A84A09558CB0C0164?method=download&amp;shareKey=a2a2cc826de97085201ccf7b8e29044a" alt="zabbix_install"><br><img src="https://note.youdao.com/yws/api/personal/file/7986498043F648D4AD3B5DA79791B31A?method=download&amp;shareKey=41c5881257604b0f1c51285b661f4898" alt="zabbix_install"><br><img src="https://note.youdao.com/yws/api/personal/file/FE90EDE6BBD64BE48D7A7538E74CCE84?method=download&amp;shareKey=a40d636aa98ae67f6028c03553410e21" alt="zabbix_login"><br><img src="https://note.youdao.com/yws/api/personal/file/12E0CD54864D4D95B4436999407B41E8?method=download&amp;shareKey=d8af7d7866ebb11b58ade11c56a4c397" alt="zabbix_dashboard">  </p>
<p>附件：<br><a href="https://note.youdao.com/yws/api/personal/file/CA54F4C49CC446C08B157ABC07E178DB?method=download&amp;shareKey=e118ac9cb68feebe28e0d35d11ae7a13" target="_blank" rel="external">epel-release-latest-7.noarch</a><br><a href="https://note.youdao.com/yws/api/personal/file/AEDAB38F808A4441BAA8B02D75C8E004?method=download&amp;shareKey=96b2d60bdb51bcef8a20ad6f53a3065a" target="_blank" rel="external">mysql80-community-release-el7-1.noarch</a><br><a href="http://note.youdao.com/noteshare?id=46f5f435bf9273ef9c6ac0a4f4d0ad4f&amp;sub=BE91DBE3A81A4021BFA3359028178F77" target="_blank" rel="external">webtatic-release</a><br><a href="https://note.youdao.com/yws/api/personal/file/9DEDA9CC216A459FB00A3138E7D31738?method=download&amp;shareKey=c1a862025490a298da625b9203bb8892" target="_blank" rel="external">zabbix-3.4.11.tar.gz</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/06/30/部署Zabbix-3-4/">http://www.yfshare.vip/2018/06/30/部署Zabbix-3-4/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然zabbix已经玩了好多遍了，但每次重新部署时，还得到处找，今天抽空整理下文档，以后就可以直接复制粘贴了，哈哈&lt;br&gt;
    
    </summary>
    
      <category term="Monitor" scheme="http://www.yfshare.vip/categories/Monitor/"/>
    
    
      <category term="Zabbix" scheme="http://www.yfshare.vip/tags/Zabbix/"/>
    
  </entry>
  
  <entry>
    <title>Ansible部署Jenkins环境</title>
    <link href="http://www.yfshare.vip/2018/06/21/Ansible%E9%83%A8%E7%BD%B2Jenkins%E7%8E%AF%E5%A2%83/"/>
    <id>http://www.yfshare.vip/2018/06/21/Ansible部署Jenkins环境/</id>
    <published>2018-06-21T15:00:00.000Z</published>
    <updated>2018-06-21T15:41:34.465Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>本文是基于Ansible批量部署Jenkins环境<br><a id="more"></a><br>支持部署的环境：  </p>
<ul>
<li>基于docker的Jenkins基础环境</li>
<li>Maven</li>
<li>Nodejs</li>
<li>基于docker的Nexus3基础环境</li>
<li>docker基础环境</li>
</ul>
<h4 id="工具包测试环境信息"><a href="#工具包测试环境信息" class="headerlink" title="工具包测试环境信息"></a>工具包测试环境信息</h4><table>
<thead>
<tr>
<th>环境</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr>
<td>Jenkins</td>
<td>192.168.1.201</td>
</tr>
<tr>
<td>Maven</td>
<td>192.168.1.202</td>
</tr>
<tr>
<td>Nodejs</td>
<td>192.168.1.203</td>
</tr>
<tr>
<td>Nexus3</td>
<td>192.168.1.206</td>
</tr>
</tbody>
</table>
<h4 id="工具版本信息"><a href="#工具版本信息" class="headerlink" title="工具版本信息"></a>工具版本信息</h4><table>
<thead>
<tr>
<th>工具</th>
<th>版本</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>centos</td>
<td>7.4</td>
<td>7版本的系统对docker性能支持更好</td>
</tr>
<tr>
<td>Ansible</td>
<td>2.5.2</td>
<td>因有些语法较新，需要ansible大于2.4</td>
</tr>
<tr>
<td>docker-ce</td>
<td>latest</td>
<td>-</td>
</tr>
<tr>
<td>Jenkins</td>
<td>2.128</td>
<td>-</td>
</tr>
<tr>
<td>Maven</td>
<td>3.5.3</td>
<td>-</td>
</tr>
<tr>
<td>Nodejs</td>
<td>v8.11.2</td>
<td>-</td>
</tr>
<tr>
<td>Nexus3_oss</td>
<td>3.12.0</td>
<td>-</td>
</tr>
</tbody>
</table>
<h4 id="Ansible工具包内容"><a href="#Ansible工具包内容" class="headerlink" title="Ansible工具包内容"></a>Ansible工具包内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># tree jenkins</span></div><div class="line">jenkins</div><div class="line">├── 01-install-jenkins.yml</div><div class="line">├── 02-clean-jenkins.yml</div><div class="line">├── 03-install-maven.yml</div><div class="line">├── 04-install-nodejs.yml</div><div class="line">├── 05-install-nexus3.yml</div><div class="line">├── 06-clean-nexus3.yml</div><div class="line">├── 07-install-docker.yml</div><div class="line">├── hosts</div><div class="line">├── pics</div><div class="line">│   ├── jenkins_dashboard.png</div><div class="line">│   └── nexus3_dashboard.png</div><div class="line">├── README.md</div><div class="line">└── roles</div><div class="line">    ├── clean-jenkins</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── jenkins-docker-compose.yml.j2</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── clean-nexus3</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── nexus3-docker-compose.yml.j2</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── common</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── epel-release-latest-7.noarch.rpm</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── 20-nproc.conf</div><div class="line">    │       └── limits.conf</div><div class="line">    ├── docker</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── daemon.json</div><div class="line">    │   │   ├── docker-ce.repo</div><div class="line">    │   │   ├── docker-compose</div><div class="line">    │   │   └── docker.service</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── jdk</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── java.sh</div><div class="line">    │   │   └── jdk-8u171-linux-x64.tar.gz</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── jenkins</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── jenkins_2.128.tar.gz</div><div class="line">    │   │   └── jenkins-docker-compose.yml.j2</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── maven</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── apache-maven-3.5.3-bin.tar.gz</div><div class="line">    │   │   └── maven.sh</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── nexus3</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── nexus3-docker-compose.yml.j2</div><div class="line">    │   │   └── nexus_oss_3.12.0.tar.gz</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    └── nodejs</div><div class="line">        ├── files</div><div class="line">        │   ├── nodejs.sh</div><div class="line">        │   └── node-v8.11.2-linux-x64.tar.xz</div><div class="line">        └── tasks</div><div class="line">            └── main.yml</div><div class="line"></div><div class="line">30 directories, 39 files</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>工具包大小：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># du -sh jenkins</span></div><div class="line">1.5G	jenkins</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h4 id="工具包README"><a href="#工具包README" class="headerlink" title="工具包README"></a>工具包README</h4><p>本工具测试环境为：centos 7.4<br>Author：Jack_wang<br>Blog：<a href="http://www.yfshare.vip">http://www.yfshare.vip</a>  </p>
<h5 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h5><ol>
<li>ansible当前服务器自己root互信  </li>
<li>ansible服务器和其他所有各个节点root互信<br>命令: <code>ssh-copy-id -i ~/.ssh/id_rsa.pub root@ip</code>  </li>
</ol>
<p>注：ansible需要使用2.4以上的版本，因有些语法2.4以下不支持.<br>当前ansible版本为2.5.2。  </p>
<h5 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum -y install https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</div><div class="line">yum -y install ansible</div></pre></td></tr></table></figure>
<p>注：这里安装ansible-2.5.2时，yum源里会缺少python-babel-0.9.6-8.el7.noarch.rpm包，可以在<a href="https://pkgs.org/download/python-b" target="_blank" rel="external">https://pkgs.org/download/python-b</a><br>abel这里下载.在当前目录也提供了该依赖包.<br>按要求修改hosts文件相关参数<br>操作步骤：  </p>
<ol>
<li>ansible-playbook -i hosts 01-install-jenkins.yml   #部署jenkins环境，docker镜像里集成maven  </li>
<li>ansible-playbook -i hosts 03-install-maven.yml     #部署maven环境  </li>
<li>ansible-playbook -i hosts 04-install-nodejs.yml    #部署nodejs环境  </li>
<li>ansible-playbook -i hosts 05-install-nexus3.yml    #部署nexus3-OSS环境  </li>
<li>ansible-playbook -i hosts 07-install-docker.yml    #部署docker-ce基础环境  </li>
</ol>
<p>部署成功后，可执行下面命令查看结果：<br>Jenkins：访问<a href="http://ip:8080" target="_blank" rel="external">http://ip:8080</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker ps <span class="_">-a</span> |grep -i <span class="string">'jenkins'</span></div></pre></td></tr></table></figure></p>
<p>Maven：<br>执行<code>mvn -v</code>  </p>
<p>Nodejs：<br>执行<code>node -v</code>  </p>
<p>nexus3：访问<a href="http://ip:8081" target="_blank" rel="external">http://ip:8081</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker ps <span class="_">-a</span> |grep -i <span class="string">'nexus'</span></div></pre></td></tr></table></figure></p>
<h5 id="卸载jenkins"><a href="#卸载jenkins" class="headerlink" title="卸载jenkins"></a>卸载jenkins</h5><p>按要求修改hosts文件相关参数<br>操作步骤：  </p>
<ol>
<li>ansible-playbook -i hosts 02-clean-jenkins.yml    #卸载jenkins  </li>
</ol>
<h5 id="卸载nexus3-oss"><a href="#卸载nexus3-oss" class="headerlink" title="卸载nexus3_oss"></a>卸载nexus3_oss</h5><p>按要求修改hosts文件相关参数<br>操作步骤：  </p>
<ol>
<li>ansible-playbook -i hosts 06-clean-nexus3.yml     #卸载nexus3</li>
</ol>
<p><img src="https://note.youdao.com/yws/api/personal/file/94440E3553464D6BAB6149486E74FE3B?method=download&amp;shareKey=39982b3d8a9b1c638cb6b37a16e2e72d" alt="jenkins_dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/352F095BF60249FDA6C6875E06688DD4?method=download&amp;shareKey=272cdc4d1c77ae74361b506ce6f502e2" alt="nexus3_dashboard">  </p>
<p>附件：<br><a href="https://pan.baidu.com/s/11SS6moEencz_4rcwtGQdeA" target="_blank" rel="external">jenkins_ansible_2.128.tar.gz百度网盘，密码：dxng</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/06/21/Ansible部署Jenkins环境/">http://www.yfshare.vip/2018/06/21/Ansible部署Jenkins环境/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是基于Ansible批量部署Jenkins环境&lt;br&gt;
    
    </summary>
    
      <category term="Jenkins" scheme="http://www.yfshare.vip/categories/Jenkins/"/>
    
    
      <category term="Jenkins" scheme="http://www.yfshare.vip/tags/Jenkins/"/>
    
      <category term="Ansible" scheme="http://www.yfshare.vip/tags/Ansible/"/>
    
      <category term="Maven" scheme="http://www.yfshare.vip/tags/Maven/"/>
    
      <category term="Nodejs" scheme="http://www.yfshare.vip/tags/Nodejs/"/>
    
  </entry>
  
  <entry>
    <title>k8s滚动升级(RollingUpdate)</title>
    <link href="http://www.yfshare.vip/2018/05/28/k8s%E6%BB%9A%E5%8A%A8%E5%8D%87%E7%BA%A7-RollingUpdate/"/>
    <id>http://www.yfshare.vip/2018/05/28/k8s滚动升级-RollingUpdate/</id>
    <published>2018-05-28T07:03:50.000Z</published>
    <updated>2018-05-29T06:45:32.657Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>kubernets scheduler主要负责的工作是：接受API Server创建的新Pod，并为其安排一个主机，将信息写入etcd中。<br><a id="more"></a></p>
<h4 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h4><p>kubernetes调度器通过API Server查找还未分配主机的Pod，并尝试为这些Pod分配主机。  </p>
<ul>
<li>客户端提交创建请求，可以通过API Server的Restful API，也可以使用kubectl命令行工具。支持的数据类型包括JSON和YAML  </li>
<li>API Server处理用户请求，存储Pod数据到etcd  </li>
<li>调度器通过API Server查看未绑定的Pod。尝试为Pod分配主机  </li>
<li>过滤主机：调度器用一组规则过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉  </li>
<li>主机打分：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把容一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等  </li>
<li>选择主机：选择打分最高的主机，进行binding操作，结果存储到etcd中  </li>
<li>所选主机对于的kubelet根据调度结果执行Pod创建操作  </li>
</ul>
<p>kubernetes中实现的过滤规则主要包括以下几种(在 <code>kubernetes/plugin/pkg/scheduler/algorithm/predicates</code> 中实现)：<br>参考：<a href="https://blog.csdn.net/horsefoot/article/details/51263364" target="_blank" rel="external">https://blog.csdn.net/horsefoot/article/details/51263364</a>  </p>
<ul>
<li><code>NoDiskConflict</code>：检查在此主机上是否存在卷冲突。如果这个主机已经挂载了卷，其它同样使用这个卷的Pod不能调度到这个主机上。  </li>
<li><code>NoVolumeZoneConflict</code>：检查给定的zone限制前提下，检查如果在此主机上部署Pod是否存在卷冲突。  </li>
<li><code>PodFitsResources</code>：检查主机的资源是否满足Pod的需求。根据实际已经分配的资源量做调度，而不是使用已实际使用的资源量做调度。  </li>
<li><code>PodFitsHostPorts</code>：检查Pod内每一个容器所需的HostPort是否已被其它容器占用。如果有所需的HostPort不满足需求，那么Pod不能调度到这个主机上。  </li>
<li><code>HostName</code>：检查主机名称是不是Pod指定的HostName。  </li>
<li><code>MatchNodeSelector</code>：检查主机的标签是否满足Pod的<em>nodeSelector</em>属性需求。  </li>
<li><code>MaxEBSVolumeCount</code>：确保已挂载的EBS存储卷不超过设置的最大值，默认值是39。它会检查直接使用的存储卷，和间接使用这种类型存储的PVC。计算不同卷的总目，如果新的Pod部署上去后卷的数目会超过设置的最大值，那么Pod不能调度到这个主机上。  </li>
<li><code>MaxGCEPDVolumeCount</code>：确保已挂载的GCE存储卷不超过设置的最大值，默认值是16。规则同上。  </li>
</ul>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　　k8s v1.9.2<br>　　　centos v7.4<br>　　　tomcat v8.0/9.0<br>　　　docker v18.05.0-ce  </p>
<table>
<thead>
<tr>
<th>节点名称</th>
<th>IP</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>KUBE-MASTER+ETCD1</td>
<td>192.168.1.201</td>
<td>部署nginx反向代理</td>
</tr>
<tr>
<td>KUBE-ETCD2</td>
<td>192.168.1.202</td>
<td>-</td>
</tr>
<tr>
<td>KUBE-ETCD3</td>
<td>192.168.1.203</td>
<td>-</td>
</tr>
<tr>
<td>KUBE-NODE1</td>
<td>192.168.1.204</td>
<td>-</td>
</tr>
<tr>
<td>KUBE-NODE2</td>
<td>192.168.1.205</td>
<td>-</td>
</tr>
<tr>
<td>KUBE-NODE3</td>
<td>192.168.1.206</td>
<td>-</td>
</tr>
</tbody>
</table>
<h4 id="RollingUpdate"><a href="#RollingUpdate" class="headerlink" title="RollingUpdate"></a>RollingUpdate</h4><p><img src="https://note.youdao.com/yws/api/personal/file/E01D734A3356410D9E20FA9D247A8083?method=download&amp;shareKey=ea26f2c05d91a3bf6ddb99a9dc3cc5f0" alt="Kubernetes rolling-update升级流程图"><br>查看tomcat编排文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># cat tomcat.yml </span></div><div class="line">apiVersion: apps/v1beta2</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: tomcat-deployment</div><div class="line">  namespace: web</div><div class="line">spec:</div><div class="line">  replicas: 2</div><div class="line">  selector:</div><div class="line">    matchLabels: </div><div class="line">      app: tomcat</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels: </div><div class="line">        app: tomcat</div><div class="line">    spec:</div><div class="line">      nodeSelector:</div><div class="line">        nodelabel: tomcat</div><div class="line">      containers:</div><div class="line">      - name: tomcat</div><div class="line">        image: tomcat:8.0</div><div class="line">        imagePullPolicy: Always</div><div class="line">        ports:</div><div class="line">          - containerPort: 8080</div><div class="line">        volumeMounts:</div><div class="line">          - name: time</div><div class="line">            mountPath: <span class="string">"/etc/localtime"</span></div><div class="line">            <span class="built_in">read</span>Only: <span class="literal">true</span></div><div class="line">      volumes:</div><div class="line">        - name: time</div><div class="line">          hostPath:</div><div class="line">            path: <span class="string">"/etc/localtime"</span></div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: tomcat-service</div><div class="line">  namespace: web</div><div class="line">  labels:</div><div class="line">    app: tomcat-service</div><div class="line">spec:</div><div class="line">  <span class="built_in">type</span>: NodePort</div><div class="line">  selector:</div><div class="line">    app: tomcat</div><div class="line">  ports:</div><div class="line">  - port: 8080</div><div class="line">    nodePort: 38080</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看k8s节点信息：<br>这里应用<code>replicas: 2</code>且固定了端口，所以Label的Node数必须比应用个数大一个，否则会失败<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl get nodes</span></div><div class="line">NAME            STATUS    ROLES     AGE       VERSION</div><div class="line">192.168.1.204   Ready     &lt;none&gt;    1d        v1.9.2</div><div class="line">192.168.1.205   Ready     &lt;none&gt;    1d        v1.9.2</div><div class="line">192.168.1.206   Ready     &lt;none&gt;    5h        v1.9.2</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>应用tomcat编排文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl label node 192.168.1.204 192.168.1.205 192.168.1.206 "nodelabel=tomcat"</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl create namespace web</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl apply -f tomcat.yml</span></div><div class="line"></div><div class="line">[root@localhost ~]<span class="comment"># kubectl get pod  -n web -o wide</span></div><div class="line">NAME                                 READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">tomcat-deployment-5768d76c66-jwlhp   1/1       Running   0          46s       172.30.5.2    192.168.1.205</div><div class="line">tomcat-deployment-5768d76c66-sxcj6   1/1       Running   0          46s       172.30.79.2   192.168.1.204</div><div class="line">[root@localhost ~]<span class="comment">#</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl get svc  -n web -o wide</span></div><div class="line">NAME             TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE       SELECTOR</div><div class="line">tomcat-service   NodePort   172.16.30.72   &lt;none&gt;        8080:38080/TCP   21s       app=tomcat</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看应用部署状态：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl rollout status deployment tomcat-deployment --namespace=web</span></div><div class="line">deployment <span class="string">"tomcat-deployment"</span> successfully rolled out</div><div class="line">[root@localhost ~]<span class="comment">#</span></div><div class="line"></div><div class="line"><span class="comment">#升级前tomcat为8.0版本</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl describe pod tomcat-deployment-5768d76c66-jwlhp -n web | grep -i 'image:'</span></div><div class="line">    Image:          tomcat:8.0</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>升级应用：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl set image deployment tomcat-deployment tomcat=tomcat:9.0 --namespace=web --record</span></div><div class="line">deployment <span class="string">"tomcat-deployment"</span> image updated</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看升级结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl get pod -n web -o wide</span></div><div class="line">NAME                                 READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">tomcat-deployment-54b6<span class="built_in">fc</span>9b99-6jr2x   1/1       Running   0          1m        172.30.79.3   192.168.1.204</div><div class="line">tomcat-deployment-54b6<span class="built_in">fc</span>9b99-c6rs9   1/1       Running   0          1m        172.30.27.2   192.168.1.206</div><div class="line">[root@localhost ~]<span class="comment">#</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl rollout status deployment tomcat-deployment --namespace=web</span></div><div class="line">Waiting <span class="keyword">for</span> rollout to finish: 1 out of 2 new replicas have been updated...</div><div class="line">Waiting <span class="keyword">for</span> rollout to finish: 1 out of 2 new replicas have been updated...</div><div class="line">Waiting <span class="keyword">for</span> rollout to finish: 1 out of 2 new replicas have been updated...</div><div class="line">Waiting <span class="keyword">for</span> rollout to finish: 1 old replicas are pending termination...</div><div class="line">Waiting <span class="keyword">for</span> rollout to finish: 1 old replicas are pending termination...</div><div class="line">deployment <span class="string">"tomcat-deployment"</span> successfully rolled out</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl describe deployment tomcat-deployment --namespace=web</span></div><div class="line">Name:                   tomcat-deployment</div><div class="line">Namespace:              web</div><div class="line">CreationTimestamp:      Mon, 28 May 2018 17:28:27 +0800</div><div class="line">Labels:                 app=tomcat</div><div class="line">Annotations:            deployment.kubernetes.io/revision=2</div><div class="line">                        kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:<span class="string">"apps/v1beta2"</span>,<span class="string">"kind"</span>:<span class="string">"Deployment"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"tomcat-deployment"</span>,<span class="string">"namespace"</span>:<span class="string">"web"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"replicas"</span>:2,<span class="string">"selec...                        kubernetes.io/change-cause=kubectl set image deployment tomcat-deployment tomcat=tomcat:9.0 --namespace=web --record=trueSelector:               app=tomcat</span></div><div class="line">Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable</div><div class="line">StrategyType:           RollingUpdate</div><div class="line">MinReadySeconds:        0</div><div class="line">RollingUpdateStrategy:  25% max unavailable, 25% max surge</div><div class="line">Pod Template:</div><div class="line">  Labels:  app=tomcat</div><div class="line">  Containers:</div><div class="line">   tomcat:</div><div class="line">    Image:        tomcat:9.0</div><div class="line">    Port:         8080/TCP</div><div class="line">    Environment:  &lt;none&gt;</div><div class="line">    Mounts:</div><div class="line">      /etc/localtime from time (ro)</div><div class="line">  Volumes:</div><div class="line">   time:</div><div class="line">    Type:          HostPath (bare host directory volume)</div><div class="line">    Path:          /etc/localtime</div><div class="line">    HostPathType:  </div><div class="line">Conditions:</div><div class="line">  Type           Status  Reason</div><div class="line">  ----           ------  ------</div><div class="line">  Available      True    MinimumReplicasAvailable</div><div class="line">  Progressing    True    NewReplicaSetAvailable</div><div class="line">OldReplicaSets:  &lt;none&gt;</div><div class="line">NewReplicaSet:   tomcat-deployment-54b6fc9b99 (2/2 replicas created)</div><div class="line">Events:</div><div class="line">  Type    Reason             Age   From                   Message</div><div class="line">  ----    ------             ----  ----                   -------</div><div class="line">  Normal  ScalingReplicaSet  4m    deployment-controller  Scaled up replica set tomcat-deployment-5768d76c66 to 2</div><div class="line">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled up replica set tomcat-deployment-54b6fc9b99 to 1</div><div class="line">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled down replica set tomcat-deployment-5768d76c66 to 1</div><div class="line">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled up replica set tomcat-deployment-54b6fc9b99 to 2</div><div class="line">  Normal  ScalingReplicaSet  52s   deployment-controller  Scaled down replica set tomcat-deployment-5768d76c66 to 0</div><div class="line">[root@localhost ~]#</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#tomcat成功升级到9.0版本</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl describe pod tomcat-deployment-54b6fc9b99-6jr2x -n web | grep -i 'image:'</span></div><div class="line">    Image:          tomcat:9.0</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>查看deployments版本：<br>由于之前在升级(set image)时添加<code>--record</code>参数，故在查看历史版本时会记录操作的指令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl rollout history deployment tomcat-deployment --namespace=web</span></div><div class="line">deployments <span class="string">"tomcat-deployment"</span></div><div class="line">REVISION  CHANGE-CAUSE</div><div class="line">1         &lt;none&gt;</div><div class="line">2         kubectl <span class="built_in">set</span> image deployment tomcat-deployment tomcat=tomcat:9.0 --namespace=web --record=<span class="literal">true</span></div><div class="line"></div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h4 id="版本回滚"><a href="#版本回滚" class="headerlink" title="版本回滚"></a>版本回滚</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl rollout undo deployment tomcat-deployment --namespace=web --to-revision=1</span></div><div class="line">deployment <span class="string">"tomcat-deployment"</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl rollout history deployment tomcat-deployment --namespace=web</span></div><div class="line">deployments <span class="string">"tomcat-deployment"</span></div><div class="line">REVISION  CHANGE-CAUSE</div><div class="line">2         kubectl <span class="built_in">set</span> image deployment tomcat-deployment tomcat=tomcat:9.0 --namespace=web --record=<span class="literal">true</span></div><div class="line">3         &lt;none&gt;</div><div class="line"></div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>查看回滚版本结果：<br>tomcat成功回退到8.0版本了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl get pod -n web -o wide</span></div><div class="line">NAME                                 READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">tomcat-deployment-5768d76c66-bjwlc   1/1       Running   0          3m        172.30.5.2    192.168.1.205</div><div class="line">tomcat-deployment-5768d76c66-bz5rf   1/1       Running   0          2m        172.30.79.2   192.168.1.204</div><div class="line">[root@localhost ~]<span class="comment">#</span></div><div class="line">[root@localhost ~]<span class="comment"># kubectl describe pod tomcat-deployment-5768d76c66-bjwlc -n web | grep -i 'image:'</span></div><div class="line">    Image:          tomcat:8.0</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看版本回滚历史：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl describe deployment tomcat-deployment --namespace=web</span></div><div class="line">Name:                   tomcat-deployment</div><div class="line">Namespace:              web</div><div class="line">CreationTimestamp:      Mon, 28 May 2018 17:28:27 +0800</div><div class="line">Labels:                 app=tomcat</div><div class="line">Annotations:            deployment.kubernetes.io/revision=3</div><div class="line">                        kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">"apiVersion"</span>:<span class="string">"apps/v1beta2"</span>,<span class="string">"kind"</span>:<span class="string">"Deployment"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"tomcat-deployment"</span>,<span class="string">"namespace"</span>:<span class="string">"web"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"replicas"</span>:2,<span class="string">"selec...Selector:               app=tomcat</span></div><div class="line">Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable</div><div class="line">StrategyType:           RollingUpdate</div><div class="line">MinReadySeconds:        0</div><div class="line">RollingUpdateStrategy:  25% max unavailable, 25% max surge</div><div class="line">Pod Template:</div><div class="line">  Labels:  app=tomcat</div><div class="line">  Containers:</div><div class="line">   tomcat:</div><div class="line">    Image:        tomcat:8.0</div><div class="line">    Port:         8080/TCP</div><div class="line">    Environment:  &lt;none&gt;</div><div class="line">    Mounts:</div><div class="line">      /etc/localtime from time (ro)</div><div class="line">  Volumes:</div><div class="line">   time:</div><div class="line">    Type:          HostPath (bare host directory volume)</div><div class="line">    Path:          /etc/localtime</div><div class="line">    HostPathType:  </div><div class="line">Conditions:</div><div class="line">  Type           Status  Reason</div><div class="line">  ----           ------  ------</div><div class="line">  Available      True    MinimumReplicasAvailable</div><div class="line">  Progressing    True    NewReplicaSetAvailable</div><div class="line">OldReplicaSets:  &lt;none&gt;</div><div class="line">NewReplicaSet:   tomcat-deployment-5768d76c66 (2/2 replicas created)</div><div class="line">Events:</div><div class="line">  Type    Reason              Age               From                   Message</div><div class="line">  ----    ------              ----              ----                   -------</div><div class="line">  Normal  ScalingReplicaSet   55m               deployment-controller  Scaled up replica set tomcat-deployment-54b6fc9b99 to 1</div><div class="line">  Normal  ScalingReplicaSet   55m               deployment-controller  Scaled down replica set tomcat-deployment-5768d76c66 to 1</div><div class="line">  Normal  ScalingReplicaSet   55m               deployment-controller  Scaled up replica set tomcat-deployment-54b6fc9b99 to 2</div><div class="line">  Normal  ScalingReplicaSet   54m               deployment-controller  Scaled down replica set tomcat-deployment-5768d76c66 to 0</div><div class="line">  Normal  ScalingReplicaSet   4m                deployment-controller  Scaled up replica set tomcat-deployment-5768d76c66 to 1</div><div class="line">  Normal  DeploymentRollback  4m                deployment-controller  Rolled back deployment "tomcat-deployment<span class="string">" to revision 1</span></div><div class="line">  Normal  ScalingReplicaSet   4m (x2 over 58m)  deployment-controller  Scaled up replica set tomcat-deployment-5768d76c66 to 2</div><div class="line">  Normal  ScalingReplicaSet   4m                deployment-controller  Scaled down replica set tomcat-deployment-54b6fc9b99 to 1</div><div class="line">  Normal  ScalingReplicaSet   3m                deployment-controller  Scaled down replica set tomcat-deployment-54b6fc9b99 to 0</div><div class="line">[root@localhost ~]#</div></pre></td></tr></table></figure></p>
<p>注：在升级容器应用版本和回滚时，测试发现会约有2-3秒时间会出现应用访问故障。<br>测试情况为：Nginx反向代理所有Node IP地址+应用端口号。建议该应用端口号在Label Node上全局唯一。  </p>
<p>参数说明：<br>参考：<a href="https://www.jianshu.com/p/6bc8e0ae65d1" target="_blank" rel="external">https://www.jianshu.com/p/6bc8e0ae65d1</a><br><a href="https://www.ipcpu.com/2017/09/kubernetes-rolling-update/" target="_blank" rel="external">https://www.ipcpu.com/2017/09/kubernetes-rolling-update/</a>  </p>
<ul>
<li><code>maxSurge</code>与<code>maxUnavailable</code><br> maxSurge: 1 表示滚动升级时会先启动1个pod；maxUnavailable: 1 表示滚动升级时允许的最大Unavailable的pod个数</li>
<li><code>terminationGracePeriodSeconds</code><br> k8s将会给应用发送SIGTERM信号，可以用来正确、优雅地关闭应用,默认为30秒。如果需要更优雅地关闭，则可以使用k8s提供的pre-stop lifecycle hook 的配置声明，将会在发送SIGTERM之前执行  </li>
<li><code>livenessProbe</code>与<code>readinessProbe</code><br> livenessProbe是kubernetes认为该pod是存活的，不存在则需要kill掉，然后再新启动一个，以达到replicas指定的个数；readinessProbe是kubernetes认为该pod是启动成功的，这里根据每个应用的特性，自己去判断，可以执行command，也可以进行httpGet  </li>
</ul>
<h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h4><p>Question1：<br>如果报下面的错误，是k8s在调度时发现无可用的Node，因应用需要的主机端口在Node上被占用，无法完成RollingUpdate。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Warning  FailedScheduling  12s (x26 over 6m)  default-scheduler  0/2 nodes are available: 2 PodFitsHostPorts.</div></pre></td></tr></table></figure></p>
<p>思考：<br>应用升级成功，升级后NodeIP改变，怎么访问容器应用？<br>Answer：直接通过POD的IP和端口号可以访问容器应用，但POD的IP地址是不可靠的，如：当POD所在Node发生故障时，POD会被K8S重新调度到另一个POD，这是POD的IP地址就发生了变化，应用将无法访问。部署分布式容器应用，多个实例同时提供服务，容器应用前面配置一个负载均衡来实现转发。K8S的<code>kind:server</code>就是用来解决这些核心问题的。<br>测试经过：<code>kind:Deployment+service</code>部署的应用，访问任何一个NodeIP+端口号都可以，<code>kind:Deloyment</code>部署应用，访问应用只能用所在的Node IP+端口号才可以。  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/05/28/k8s滚动升级-RollingUpdate/">http://www.yfshare.vip/2018/05/28/k8s滚动升级-RollingUpdate/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;kubernets scheduler主要负责的工作是：接受API Server创建的新Pod，并为其安排一个主机，将信息写入etcd中。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="k8s" scheme="http://www.yfshare.vip/tags/k8s/"/>
    
      <category term="RollingUpdate" scheme="http://www.yfshare.vip/tags/RollingUpdate/"/>
    
  </entry>
  
  <entry>
    <title>Ansible部署TLS K8S</title>
    <link href="http://www.yfshare.vip/2018/05/21/Ansible%E9%83%A8%E7%BD%B2TLS-K8S/"/>
    <id>http://www.yfshare.vip/2018/05/21/Ansible部署TLS-K8S/</id>
    <published>2018-05-21T08:09:57.000Z</published>
    <updated>2018-05-21T08:20:12.080Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>本文是基于Ansible批量部署kubernetes集群环境<br><a id="more"></a><br>支持部署k8s环境：  </p>
<ul>
<li>ETCD集群</li>
<li>kube-master</li>
<li>kube-node</li>
<li>kube-dashboard</li>
<li>vmware-harbor仓库</li>
</ul>
<h4 id="工具包测试环境信息"><a href="#工具包测试环境信息" class="headerlink" title="工具包测试环境信息"></a>工具包测试环境信息</h4><table>
<thead>
<tr>
<th>环境</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr>
<td>ETCD1</td>
<td>192.168.1.201</td>
</tr>
<tr>
<td>ETCD2</td>
<td>192.168.1.202</td>
</tr>
<tr>
<td>ETCD3</td>
<td>192.168.1.203</td>
</tr>
<tr>
<td>KUBE-MASTER</td>
<td>192.168.1.201</td>
</tr>
<tr>
<td>KUBE-NODE1</td>
<td>192.168.1.204</td>
</tr>
<tr>
<td>KUBE-NODE2</td>
<td>192.168.1.205</td>
</tr>
<tr>
<td>KUBE-DASHBOARD</td>
<td>192.168.1.204</td>
</tr>
<tr>
<td>VMWARE-HARBOR</td>
<td>192.168.1.205</td>
</tr>
</tbody>
</table>
<h4 id="工具版本信息"><a href="#工具版本信息" class="headerlink" title="工具版本信息"></a>工具版本信息</h4><table>
<thead>
<tr>
<th>工具</th>
<th>版本</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>centos</td>
<td>7.4</td>
<td>7版本的系统对docker性能支持更好</td>
</tr>
<tr>
<td>Ansible</td>
<td>2.5.2</td>
<td>因有些语法较新，需要ansible大于2.4</td>
</tr>
<tr>
<td>kubernetes</td>
<td>1.9.2</td>
<td>当前官方最新版为1.10</td>
</tr>
<tr>
<td>docker</td>
<td>latest</td>
<td>-</td>
</tr>
<tr>
<td>etcd</td>
<td>v3.2.15</td>
<td>-</td>
</tr>
<tr>
<td>flannel</td>
<td>v0.10.0</td>
<td>-</td>
</tr>
<tr>
<td>heapster</td>
<td>1.5.3</td>
<td>-</td>
</tr>
<tr>
<td>grafana</td>
<td>v4.4.3</td>
<td>-</td>
</tr>
<tr>
<td>influxdb</td>
<td>v1.3.3</td>
<td>-</td>
</tr>
<tr>
<td>kube-dashboard</td>
<td>v1.8.3</td>
<td>-</td>
</tr>
<tr>
<td>vmware-harbor</td>
<td>v1.5.0</td>
<td>支持k8s集群部署和单机部署，支持ip和域名部署访问</td>
</tr>
</tbody>
</table>
<h4 id="Ansible工具包内容"><a href="#Ansible工具包内容" class="headerlink" title="Ansible工具包内容"></a>Ansible工具包内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># tree k8s</span></div><div class="line">k8s</div><div class="line">├── 01-kube-etcd-all.yml</div><div class="line">├── 02-kube-master.yml</div><div class="line">├── 03-kube-node.yml</div><div class="line">├── 04-clean-kube-node.yml</div><div class="line">├── 05-clean-kube-master-all.yml</div><div class="line">├── 06-kube-dashboard.yml</div><div class="line">├── 07-clean-kube-dashboard.yml</div><div class="line">├── 08-vmware-harbor.yml</div><div class="line">├── 09-clean-vmware-harbor.yml</div><div class="line">├── epel-release-latest-7.noarch.rpm</div><div class="line">├── hosts</div><div class="line">├── pics</div><div class="line">│   ├── harbor.png</div><div class="line">│   └── kube-dashboard.png</div><div class="line">├── python-babel-0.9.6-8.el7.noarch.rpm</div><div class="line">├── README.md</div><div class="line">└── roles</div><div class="line">    ├── ca</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── cfssl</div><div class="line">    │   │   ├── cfssl-certinfo</div><div class="line">    │   │   └── cfssljson</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── ca-config.json</div><div class="line">    │       └── ca-csr.json</div><div class="line">    ├── clean-dashboard</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── clean-etcd</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── clean-master</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── clean-node</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── clean-vmware-harbor</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── common</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── epel-release-latest-7.noarch.rpm</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── 20-nproc.conf</div><div class="line">    │       └── limits.conf</div><div class="line">    ├── docker</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── daemon.json</div><div class="line">    │   │   ├── docker-ce.repo</div><div class="line">    │   │   └── docker.service</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    ├── etcd</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── etcd-v3.2.15-linux-amd64.tar.gz</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── etcd-csr.json.j2</div><div class="line">    │       └── etcd.service.j2</div><div class="line">    ├── flannel</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── flannel-v0.10.0-linux-amd64.tar.gz</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── flanneld-csr.json</div><div class="line">    │       └── flanneld.service.j2</div><div class="line">    ├── kube-client</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── kubernetes-client-linux-amd64-v1.9.2.tar.gz</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       └── admin-csr.json</div><div class="line">    ├── kube-dashboard</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── heapster-1.5.3.tar.gz</div><div class="line">    │   │   ├── heapster-amd64_v1.5.3.tar.gz</div><div class="line">    │   │   ├── heapster-grafana-amd64_v4.4.3.tar.gz</div><div class="line">    │   │   ├── heapster-influxdb-amd64_v1.3.3.tar.gz</div><div class="line">    │   │   ├── kubernetes-dashboard-amd64_v1.8.3.tar.gz</div><div class="line">    │   │   └── pod-infrastructure_v3.6.173.0.49.tar.gz</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       └── kubernetes-dashboard.yaml</div><div class="line">    ├── kube-master</div><div class="line">    │   ├── files</div><div class="line">    │   │   └── kubernetes-server-linux-amd64-v1.9.2.tar.gz</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── basic-auth.csv.j2</div><div class="line">    │       ├── kube-apiserver.service.j2</div><div class="line">    │       ├── kube-controller-manager.service.j2</div><div class="line">    │       ├── kubernetes-csr.json.j2</div><div class="line">    │       ├── kube-scheduler.service.j2</div><div class="line">    │       └── token.csv.j2</div><div class="line">    ├── kube-node</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── etcd</div><div class="line">    │   │   ├── etcdctl</div><div class="line">    │   │   ├── kubelet</div><div class="line">    │   │   ├── kube-proxy</div><div class="line">    │   │   └── README.txt</div><div class="line">    │   ├── tasks</div><div class="line">    │   │   └── main.yml</div><div class="line">    │   └── templates</div><div class="line">    │       ├── kubelet.service.j2</div><div class="line">    │       ├── kube-proxy-csr.json</div><div class="line">    │       └── kube-proxy.service.j2</div><div class="line">    ├── prepare</div><div class="line">    │   ├── files</div><div class="line">    │   │   ├── cfssl</div><div class="line">    │   │   ├── cfssl-certinfo</div><div class="line">    │   │   ├── cfssljson</div><div class="line">    │   │   ├── etcd</div><div class="line">    │   │   └── etcdctl</div><div class="line">    │   └── tasks</div><div class="line">    │       └── main.yml</div><div class="line">    └── vmware-harbor</div><div class="line">        ├── files</div><div class="line">        │   ├── csr</div><div class="line">        │   ├── docker-ce.repo</div><div class="line">        │   ├── docker-compose</div><div class="line">        │   ├── docker.service</div><div class="line">        │   └── harbor-offline-installer-v1.5.0.tgz</div><div class="line">        ├── tasks</div><div class="line">        │   └── main.yml</div><div class="line">        └── templates</div><div class="line">            ├── daemon.json.j2</div><div class="line">            ├── docker-compose.yml</div><div class="line">            └── harbor.cfg.j2</div><div class="line"></div><div class="line">55 directories, 84 files</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>工具包大小：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># du -sh k8s/</span></div><div class="line">2.1G	k8s/</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h4 id="工具包README"><a href="#工具包README" class="headerlink" title="工具包README"></a>工具包README</h4><p>本ansible脚本测试执行环境为：centos 7.4，kubernetes v1.9.2<br>Author：Jack_wang<br>Blog：<a href="http://www.yfshare.vip">http://www.yfshare.vip</a>  </p>
<h5 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h5><ol>
<li>ansible当前服务器自己root互信  </li>
<li>ansible服务器和其他所有各个节点root互信<br>命令: <code>ssh-copy-id -i ~/.ssh/id_rsa.pub root@ip</code>  </li>
</ol>
<p>注：ansible需要使用2.4以上的版本，因有些语法2.4以下不支持.<br>当前ansible版本为2.5.2。  </p>
<h5 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum -y install https://mirrors.aliyun.com/epel/epel-release-latest-7.noarch.rpm</div><div class="line">yum -y install ansible</div></pre></td></tr></table></figure>
<p>注：这里安装ansible-2.5.2时，yum源里会缺少python-babel-0.9.6-8.el7.noarch.rpm包，可以在<a href="https://pkgs.org/download/python-babel" target="_blank" rel="external">https://pkgs.org/download/python-babel</a> 这里下载.在当前目录也提供了该依赖包.<br>按要求修改hosts文件相关参数<br>操作步骤：  </p>
<ol>
<li>ansible-playbook -i hosts 01-kube-etcd-all.yml  #部署etcd  </li>
<li>ansible-playbook -i hosts 02-kube-master.yml    #部署kube-master  </li>
<li>ansible-playbook -i hosts 03-kube-node.yml      #部署kube-node  </li>
</ol>
<p>部署成功后，可执行以下命令，查看是否安装成功.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">kubectl get componentstatuses</div><div class="line">kubectl get csr</div><div class="line">kubectl get nodes</div></pre></td></tr></table></figure></p>
<p>也可以安装POD测试<br>下面提供一个测试yml文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">apiVersion: apps/v1beta2</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: nginx-deployment </div><div class="line">spec:</div><div class="line">  replicas: 1 </div><div class="line">  selector:</div><div class="line">    matchLabels: </div><div class="line">      app: nginx</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels: </div><div class="line">        app: nginx</div><div class="line">    spec:</div><div class="line">     containers:</div><div class="line">     - name: nginx</div><div class="line">       image: nginx:1.14</div><div class="line">       ports:</div><div class="line">       - containerPort: 80</div></pre></td></tr></table></figure></p>
<p>保存文件为nginx-deployment.yaml  </p>
<p>执行安装POD命令，等待安装完成.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl apply <span class="_">-f</span> nginx-deployment.yaml</div></pre></td></tr></table></figure></p>
<h5 id="卸载Kubernetes集群步骤"><a href="#卸载Kubernetes集群步骤" class="headerlink" title="卸载Kubernetes集群步骤"></a>卸载Kubernetes集群步骤</h5><p>按要求修改hosts文件相关参数.  </p>
<ul>
<li><p>卸载 kube-node 操作步骤：<br>只删除指定的kube-node节点.  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ansible-playbook -i hosts 04-clean-kube-node.yml</div></pre></td></tr></table></figure>
</li>
<li><p>卸载整个Kubernetes集群集群步骤：<br>删除整个K8s集群，包括kube-node，kube-master，etcd。  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ansible-playbook -i hosts 05-clean-kube-master-all.yml</div></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="部署kube-dashboard"><a href="#部署kube-dashboard" class="headerlink" title="部署kube-dashboard"></a>部署kube-dashboard</h5><p>按要求修改hosts文件相关参数.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ansible-playbook -i hosts 06-kube-dashboard.yml</div></pre></td></tr></table></figure></p>
<p>安装完成后，访问<a href="https://ip:8443" target="_blank" rel="external">https://ip:8443</a> 打开kube-dashboard WEB页面。访问 <a href="http://ip:3000" target="_blank" rel="external">http://ip:3000</a> 打开Grafana WEB页面。<br>配置Grafana略.</p>
<h5 id="卸载kube-dashboard"><a href="#卸载kube-dashboard" class="headerlink" title="卸载kube-dashboard"></a>卸载kube-dashboard</h5><p>按要求修改hosts文件相关参数.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ansible-playbook -i hosts 07-clean-kube-dashboard.yml</div></pre></td></tr></table></figure></p>
<h5 id="部署vmware-harbor-docker仓库"><a href="#部署vmware-harbor-docker仓库" class="headerlink" title="部署vmware-harbor docker仓库"></a>部署vmware-harbor docker仓库</h5><p>按要求修改hosts文件相关参数.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ansible-playbook -i hosts 08-vmware-harbor.yml</div></pre></td></tr></table></figure></p>
<h5 id="卸载vmware-harbor-docker仓库"><a href="#卸载vmware-harbor-docker仓库" class="headerlink" title="卸载vmware-harbor docker仓库"></a>卸载vmware-harbor docker仓库</h5><p>按要求修改hosts文件相关参数.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ansible-playbook -i hosts 09-clean-vmware-harbor.yml</div></pre></td></tr></table></figure></p>
<h4 id="部署结果"><a href="#部署结果" class="headerlink" title="部署结果"></a>部署结果</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl get componentstatuses</span></div><div class="line">NAME                 STATUS    MESSAGE              ERROR</div><div class="line">controller-manager   Healthy   ok                   </div><div class="line">scheduler            Healthy   ok                   </div><div class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl get nodes</span></div><div class="line">NAME            STATUS    ROLES     AGE       VERSION</div><div class="line">192.168.1.204   Ready     &lt;none&gt;    3h        v1.9.2</div><div class="line">192.168.1.205   Ready     &lt;none&gt;    2m        v1.9.2</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p><img src="https://note.youdao.com/yws/api/personal/file/5A97833F75F7451DB589CFFD10C850E3?method=download&amp;shareKey=0e95e5366f21b2d6077998885cb543e4" alt="kube-dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/FB115B2A772B4173813F29B9647714D7?method=download&amp;shareKey=9e42f21481013b1bcd8e899d1931ac75" alt="vmware-harbor">  </p>
<p>如想获取该Ansible工具包，请扫描下方微信支付宝付款二维码支持下作者(100元以上请备注<code>获取Ansible部署TLS K8S工具包</code>并留下联系方式)，谢谢  </p>
<p>遇到此工具包相关问题，可联系作者QQ：838554604  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/05/21/Ansible部署TLS-K8S/">http://www.yfshare.vip/2018/05/21/Ansible部署TLS-K8S/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是基于Ansible批量部署kubernetes集群环境&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Ansible" scheme="http://www.yfshare.vip/tags/Ansible/"/>
    
      <category term="k8s" scheme="http://www.yfshare.vip/tags/k8s/"/>
    
      <category term="kubernetes" scheme="http://www.yfshare.vip/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>glusterfs做持久化存储</title>
    <link href="http://www.yfshare.vip/2018/03/17/glusterfs%E5%81%9A%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/"/>
    <id>http://www.yfshare.vip/2018/03/17/glusterfs做持久化存储/</id>
    <published>2018-03-16T16:42:57.000Z</published>
    <updated>2018-03-16T15:55:57.877Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><ul>
<li>Glusterfs简介<br>GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBandRDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据  <a id="more"></a></li>
<li>Glusterfs特点  <ul>
<li>扩展性和高性能<br>GlusterFS利用双重特性来提供几TB至数PB的高扩展存储解决方案。Scale-Out架构允许通过简单地增加资源来提高存储容量和性能，磁盘、计算和I/O资源都可以独立增加，支持10GbE和InfiniBand等高速网络互联。Gluster弹性哈希（ElasticHash）解除了GlusterFS对元数据服务器的需求，消除了单点故障和性能瓶颈，真正实现了并行化数据访问</li>
<li>高可用性<br>GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至是在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。GlusterFS没有设计自己的私有数据文件格式，而是采用操作系统中主流标准的磁盘文件系统（如EXT3、ZFS）来存储文件，因此数据可以使用各种标准工具进行复制和访问</li>
<li>弹性卷管理<br>数据储存在逻辑卷中，逻辑卷可以从虚拟化的物理存储池进行独立逻辑划分而得到。存储服务器可以在线进行增加和移除，不会导致应用中断。逻辑卷可以在所有配置服务器中增长和缩减，可以在不同服务器迁移进行容量均衡，或者增加和移除系统，这些操作都可在线进行。文件系统配置更改也可以实时在线进行并应用，从而可以适应工作负载条件变化或在线性能调优</li>
</ul>
</li>
</ul>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><table>
<thead>
<tr>
<th>主机名</th>
<th>IP</th>
<th>Size</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>master1.example.com</td>
<td>192.168.1.195</td>
<td>50G</td>
<td>glusterfs</td>
</tr>
<tr>
<td>master2.example.com</td>
<td>192.168.1.196</td>
<td>50G</td>
<td>glusterfs</td>
</tr>
<tr>
<td>master3.example.com</td>
<td>192.168.1.197</td>
<td>50G</td>
<td>glusterfs</td>
</tr>
<tr>
<td>node1.example.com</td>
<td>192.168.1.198</td>
<td>50G</td>
<td>glusterfs</td>
</tr>
<tr>
<td>node2.example.com</td>
<td>192.168.1.199</td>
<td>50G</td>
<td>glusterfs</td>
</tr>
</tbody>
</table>
<p>环境：<br>　　　centos 7.4<br>　　　glusterfs v3.12.6<br>　　　k8s v1.8.4  </p>
<h4 id="部署glusterfs"><a href="#部署glusterfs" class="headerlink" title="部署glusterfs"></a>部署glusterfs</h4><p>以下操作所有准备做glusterfs存储的节点均要操作<br>同步时间<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># ntpdate time.windows.com</span></div></pre></td></tr></table></figure></p>
<p>安装gluster 源<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># yum install -y centos-release-gluster</span></div></pre></td></tr></table></figure></p>
<p>安装glusterfs 组件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># yum install -y glusterfs glusterfs-server glusterfs-fuse glust</span></div></pre></td></tr></table></figure></p>
<p>创建glusterfs 工作目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># mkdir -p /opt/glusterd</span></div></pre></td></tr></table></figure></p>
<p>修改glusterd 工作目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># sed -i 's/var\/lib/opt/g' /etc/glusterfs/glusterd.vol</span></div></pre></td></tr></table></figure></p>
<p>启动glusterfs<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># systemctl enable glusterd</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl start glusterd</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># netstat -tunlp | grep -i glusterd</span></div><div class="line">tcp        0      0 0.0.0.0:24007           0.0.0.0:*               LISTEN      25387/glusterd      </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>配置glusterfs<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># tail -5 /etc/hosts</span></div><div class="line">192.168.1.195 master1.example.com master1</div><div class="line">192.168.1.196 master2.example.com master2</div><div class="line">192.168.1.197 master3.example.com master3</div><div class="line">192.168.1.198 node1.example.com node1</div><div class="line">192.168.1.199 node2.example.com node2</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>开放端口<br>如果有防火墙需要打开glusterd端口24007<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span></div></pre></td></tr></table></figure></p>
<p>创建存储目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># mkdir /data/gfs_data</span></div></pre></td></tr></table></figure></p>
<p>添加节点到集群<br>执行操作的本机不需要probe 本机<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># gluster peer probe master2.example.com</span></div><div class="line">[root@master1 ~]<span class="comment"># gluster peer probe master3.example.com</span></div><div class="line">[root@master1 ~]<span class="comment"># gluster peer probe node1.example.com</span></div><div class="line">[root@master1 ~]<span class="comment"># gluster peer probe node2.example.com</span></div></pre></td></tr></table></figure></p>
<h4 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># gluster peer status</span></div><div class="line">Number of Peers: 4</div><div class="line"></div><div class="line">Hostname: master2.example.com</div><div class="line">Uuid: b91ba577-cccd-43cc-918a-44e001f3d7bf</div><div class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</div><div class="line"></div><div class="line">Hostname: master3.example.com</div><div class="line">Uuid: e0652345-9817-4a08-aec0-3673723002ed</div><div class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</div><div class="line"></div><div class="line">Hostname: node1.example.com</div><div class="line">Uuid: 34b3faed-9335-41ca-9ed5-734ed140f6f3</div><div class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</div><div class="line"></div><div class="line">Hostname: node2.example.com</div><div class="line">Uuid: f1ad8a8e-1469-4558-9a35<span class="_">-f</span>07cbbc679f2</div><div class="line">State: Peer <span class="keyword">in</span> Cluster (Connected)</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="配置volume"><a href="#配置volume" class="headerlink" title="配置volume"></a>配置volume</h4><p>GlusterFS 中的volume 的模式分为：  </p>
<ul>
<li>分布卷(默认模式)：即DHT, 也叫分布卷: 将文件已hash 算法随机分布到每台服务器节点中存储  </li>
<li>复制模式：即AFR, 创建volume 时带replica x 数量: 将文件复制到replica x 个节点中  </li>
<li>条带模式：即Striped, 创建volume 时带stripe x 数量： 将文件切割成数据块，分别存储到stripe x 个节点中( 类似raid 0 )  </li>
<li>分布式条带模式：最少需要4 台服务器才能创建。创建volume 时stripe 2 server = 4 个节点： 是DHT 与Striped 的组合型  </li>
<li>分布式复制模式：最少需要4 台服务器才能创建。创建volume 时replica 2 server = 4 个节点：是DHT 与AFR 的组合型  </li>
<li>条带复制卷模式：最少需要4 台服务器才能创建。创建volume 时stripe 2 replica 2 server = 4 个节点： 是Striped 与AFR 的组合型  </li>
<li>三种模式混合： 至少需要8 台服务器才能创建。stripe 2 replica 2 ，每4 个节点组成一个组  </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># gluster volume create k8s-volume master1.example.com:/data/gfs_data/ master2.example.com:/data/gfs_data/ master3.example.com:/data/gfs_data/ node1.example.com:/data/gfs_data/ node2.example.com:/data/gfs_data/ force</span></div></pre></td></tr></table></figure>
<p>查看volume 状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># gluster volume info</span></div><div class="line"> </div><div class="line">Volume Name: k8s-volume</div><div class="line">Type: Distribute</div><div class="line">Volume ID: e3bdd47a<span class="_">-d</span>166-4aef-827a-5cde1d6c7d91</div><div class="line">Status: Created</div><div class="line">Snapshot Count: 0</div><div class="line">Number of Bricks: 5</div><div class="line">Transport-type: tcp</div><div class="line">Bricks:</div><div class="line">Brick1: master1.example.com:/data/gfs_data</div><div class="line">Brick2: master2.example.com:/data/gfs_data</div><div class="line">Brick3: master3.example.com:/data/gfs_data</div><div class="line">Brick4: node1.example.com:/data/gfs_data</div><div class="line">Brick5: node2.example.com:/data/gfs_data</div><div class="line">Options Reconfigured:</div><div class="line">transport.address-family: inet</div><div class="line">nfs.disable: on</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>启动分布卷<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># gluster volume start k8s-volume</span></div></pre></td></tr></table></figure></p>
<h4 id="挂载测试"><a href="#挂载测试" class="headerlink" title="挂载测试"></a>挂载测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># mount -t glusterfs master1.example.com:k8s-volume /media/</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># df -h</span></div><div class="line">Filesystem                      Size  Used Avail Use% Mounted on</div><div class="line">master1.example.com:k8s-volume  250G  163M  250G   1% /media</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="配置endpoints"><a href="#配置endpoints" class="headerlink" title="配置endpoints"></a>配置endpoints</h4><p>参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-endpoints.json" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-endpoints.json</a><br><a href="https://note.youdao.com/yws/api/personal/file/30B8D024D97E49FEA5499BDAE4371AB5?method=download&amp;shareKey=329ab32250e811641622497ddeba9b71" target="_blank" rel="external">glusterfs-endpoints.json</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-endpoints.json</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat glusterfs-endpoints.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"kind"</span>: <span class="string">"Endpoints"</span>,</div><div class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</div><div class="line">  <span class="string">"metadata"</span>: &#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"gfs-cluster"</span>,</div><div class="line">    <span class="comment">#"namespace": "name"    #如果指定namespace，需要提前创建namespace，kubectl create namespace name</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"subsets"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"addresses"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"ip"</span>: <span class="string">"192.168.1.195"</span></div><div class="line">        &#125;</div><div class="line">      ],</div><div class="line">      <span class="string">"ports"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"port"</span>: 1</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="string">"addresses"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"ip"</span>: <span class="string">"192.168.1.196"</span></div><div class="line">        &#125;</div><div class="line">      ],</div><div class="line">      <span class="string">"ports"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"port"</span>: 1</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="string">"addresses"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"ip"</span>: <span class="string">"192.168.1.197"</span></div><div class="line">        &#125;</div><div class="line">      ],</div><div class="line">      <span class="string">"ports"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"port"</span>: 1</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="string">"addresses"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"ip"</span>: <span class="string">"192.168.1.198"</span></div><div class="line">        &#125;</div><div class="line">      ],</div><div class="line">      <span class="string">"ports"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"port"</span>: 1</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="string">"addresses"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"ip"</span>: <span class="string">"192.168.1.199"</span></div><div class="line">        &#125;</div><div class="line">      ],</div><div class="line">      <span class="string">"ports"</span>: [</div><div class="line">        &#123;</div><div class="line">          <span class="string">"port"</span>: 1</div><div class="line">        &#125;</div><div class="line">      ]</div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f glusterfs-endpoints.json</span></div></pre></td></tr></table></figure>
<h4 id="查看endpoints-信息"><a href="#查看endpoints-信息" class="headerlink" title="查看endpoints 信息"></a>查看endpoints 信息</h4><p>默认所有应用全在default的namespace中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get Endpoints --all-namespaces</span></div><div class="line">NAMESPACE           NAME                          ENDPOINTS                                         AGE</div><div class="line">default             gfs-cluster                   192.168.2.161:1,192.168.2.162:1,192.168.2.163:1   6d</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h4 id="创建服务"><a href="#创建服务" class="headerlink" title="创建服务"></a>创建服务</h4><p>参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-service.json" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-service.json</a><br><a href="https://note.youdao.com/yws/api/personal/file/6D6821D34984428EAEAFBCA17128ECEF?method=download&amp;shareKey=0229a4e5bf4c14222daf69325380e3a1" target="_blank" rel="external">glusterfs-service.json</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-service.json</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat glusterfs-service.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"kind"</span>: <span class="string">"Service"</span>,</div><div class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</div><div class="line">  <span class="string">"metadata"</span>: &#123;</div><div class="line">    <span class="string">"name"</span>: <span class="string">"gfs-cluster"</span>,</div><div class="line">    <span class="comment">#"namespace": "name"</span></div><div class="line">  &#125;,</div><div class="line">  <span class="string">"spec"</span>: &#123;</div><div class="line">    <span class="string">"ports"</span>: [</div><div class="line">      &#123;<span class="string">"port"</span>: 19999&#125;</div><div class="line">    ]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f glusterfs-service.json</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc -o wide</span></div><div class="line">NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE       SELECTOR</div><div class="line">glusterfs-cluster   ClusterIP   172.16.170.167   &lt;none&gt;        19999/TCP     1m        &lt;none&gt;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="创建测试pod"><a href="#创建测试pod" class="headerlink" title="创建测试pod"></a>创建测试pod</h4><p>参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-pod.json" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-pod.json</a><br><a href="https://note.youdao.com/yws/api/personal/file/31CE9F0796004E7EA6F340F746772ECF?method=download&amp;shareKey=6cea7b602a4333727c6c7478066b674e" target="_blank" rel="external">glusterfs-pod.json</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-pod.json</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat glusterfs-pod.json </span></div><div class="line">&#123;</div><div class="line">    <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</div><div class="line">    <span class="string">"kind"</span>: <span class="string">"Pod"</span>,</div><div class="line">    <span class="string">"metadata"</span>: &#123;</div><div class="line">        <span class="string">"name"</span>: <span class="string">"glusterfs"</span></div><div class="line">        <span class="comment">#"namespace": "name"</span></div><div class="line">    &#125;,</div><div class="line">    <span class="string">"spec"</span>: &#123;</div><div class="line">        <span class="string">"containers"</span>: [</div><div class="line">            &#123;</div><div class="line">                <span class="string">"name"</span>: <span class="string">"glusterfs"</span>,</div><div class="line">                <span class="string">"image"</span>: <span class="string">"nginx"</span>,</div><div class="line">                <span class="string">"volumeMounts"</span>: [</div><div class="line">                    &#123;</div><div class="line">                        <span class="string">"mountPath"</span>: <span class="string">"/mnt/glusterfs"</span>,</div><div class="line">                        <span class="string">"name"</span>: <span class="string">"glusterfsvol"</span></div><div class="line">                    &#125;</div><div class="line">                ]</div><div class="line">            &#125;</div><div class="line">        ],</div><div class="line">        <span class="string">"volumes"</span>: [</div><div class="line">            &#123;</div><div class="line">                <span class="string">"name"</span>: <span class="string">"glusterfsvol"</span>,</div><div class="line">                <span class="string">"glusterfs"</span>: &#123;</div><div class="line">                    <span class="string">"endpoints"</span>: <span class="string">"glusterfs-cluster"</span>,</div><div class="line">                    <span class="string">"path"</span>: <span class="string">"k8s-volume"</span>,     <span class="comment">#改成创建glusterfs卷时设置的名称</span></div><div class="line">                    <span class="string">"readOnly"</span>: <span class="literal">true</span></div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        ]</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f glusterfs-pod.json</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods -o wide</span></div><div class="line">NAME                               READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">glusterfs                          1/1       Running   0          6m        172.30.41.6   192.168.1.199</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl exec glusterfs mount | grep gluster</span></div><div class="line">192.168.1.195:k8s-volume on /mnt/glusterfs <span class="built_in">type</span> fuse.glusterfs (ro,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="Glusterfs持久卷管理"><a href="#Glusterfs持久卷管理" class="headerlink" title="Glusterfs持久卷管理"></a>Glusterfs持久卷管理</h4><p>注：PV和PVC是一一对应的关系，一个PV只能绑定一个PVC，如果PVC删除，对应的PV状态也会变成<code>Released</code>或者<code>Failed</code>状态，这时如果新建PVC则不能注册到以前的PV上<br>如果一个PV状态从READY(Available)变成其他状态(Bound/Released/Failed)，则不可继续被申请<br>参考：<a href="https://www.slahser.com/2017/03/22/k8s%E5%90%8E%E6%97%A5%E8%B0%88-glusterfs%E4%B8%8Epv/" target="_blank" rel="external">https://www.slahser.com/2017/03/22/k8s%E5%90%8E%E6%97%A5%E8%B0%88-glusterfs%E4%B8%8Epv/</a>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pv,pvc</span></div><div class="line">No resources found.</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>PersistentVolume（PV，持久卷）：对存储抽象实现，使得存储作为集群中的资源<br>PersistentVolumeClaim（PVC，持久卷申请）：PVC消费PV的资源<br>PVC 是持久卷申请  pvc 消费 pv<br>PV和PVC类似于VG和LV关系  </p>
<table>
<thead>
<tr>
<th>存储Plugin</th>
<th>ReadWriteOnce</th>
<th>ReadOnlyMany</th>
<th>ReadWriteMany</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWSElasticBlockStore</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>AzureFile</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>AzureDisk</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>Cinder</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>FlexVolume</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>Flocker</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>GCEPersistentDisk</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>PhotonPersistentDisk</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>Quobyte</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>PortworxVolume</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>ScaleIO</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>FC(Fibre Channel)</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>NFS</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>ISCSI</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>RBD(Ceph Block Device)</td>
<td>支持</td>
<td>支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>CephFS</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>Cinder(OpenStack Block Storage)</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>Glusterfs</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td></td>
</tr>
<tr>
<td>VsphereVolume</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td></td>
</tr>
<tr>
<td>HostPath</td>
<td>支持</td>
<td>不支持</td>
<td>不支持</td>
<td>只支持单节点，不支持跨节点</td>
</tr>
</tbody>
</table>
<p>三种PV的访问模式  </p>
<ul>
<li>ReadWriteOnce：是最基本的方式，可读可写，但只支持被单个Pod挂载  </li>
<li>ReadOnlyMany：可以以只读的方式被多个Pod挂载  </li>
<li>ReadWriteMany：这种存储可以以读写的方式被多个Pod共享  </li>
</ul>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/29706309" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29706309</a>  </p>
<p>卷的状态：  </p>
<ul>
<li>Available – a free resource that is not yet bound to a claim  </li>
<li>Bound – the volume is bound to a claim  </li>
<li>Released – the claim has been deleted, but the resource is not yet reclaimed by the cluster  </li>
<li>Failed – the volume has failed its automatic reclamation  </li>
</ul>
<p>参考：<a href="https://www.kubernetes.org.cn/pvpvcstorageclass" target="_blank" rel="external">https://www.kubernetes.org.cn/pvpvcstorageclass</a>  </p>
<p>glusterfs 创建pv<br><a href="https://note.youdao.com/yws/api/personal/file/7F506ED50381494E9202DE11D37986CA?method=download&amp;shareKey=9f4a718687f394aa7d8a09fdcaad1ce0" target="_blank" rel="external">gfs-pv.yml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat gfs-pv.yml </span></div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: <span class="string">"gluster-pv"</span></div><div class="line">  <span class="comment">#namespace: "name"</span></div><div class="line">spec:</div><div class="line">  capacity:</div><div class="line">    storage: 250Gi                   <span class="comment">#再挂载测试时可以看到gfs卷的大小</span></div><div class="line">  accessModes:</div><div class="line">    - ReadWriteMany</div><div class="line">  glusterfs:</div><div class="line">    endpoints: <span class="string">"glusterfs-cluster"</span>    <span class="comment">#通过kubectl get endpoints查询Name</span></div><div class="line">    path: <span class="string">"k8s-volume"</span>                <span class="comment">#通过gluster volume info查询Volume Name</span></div><div class="line">    <span class="built_in">read</span>Only: <span class="literal">false</span></div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f gfs-pv.yml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pv,pvc</span></div><div class="line">NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</div><div class="line">pv/gluster-pv   250Gi      RWX            Retain           Available                                      12m</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>glusterfs  创建pvc<br><a href="https://note.youdao.com/yws/api/personal/file/51FD8C64D782486C9164A007629536D2?method=download&amp;shareKey=5f3a7944fb7663ed930f7441ebbbbd68" target="_blank" rel="external">gfs-pvc.yml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat gfs-pvc.yml </span></div><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolumeClaim</div><div class="line">metadata:</div><div class="line">  name: <span class="string">"glusterfs-pvc"</span></div><div class="line">  <span class="comment">#namespace: "name"</span></div><div class="line">spec:</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteMany</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 250Gi</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f gfs-pvc.yml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pv,pvc --all-namespaces</span></div><div class="line">NAMESPACE   NAME        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                       STORAGECLASS   REASON    AGE</div><div class="line">            pv/gfs-pv   300Gi      RWX            Retain           Bound     default/gfs-pvc                            7m</div><div class="line"></div><div class="line">NAMESPACE           NAME          STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</div><div class="line">default   pvc/gfs-pvc   Bound     gfs-pv    300Gi      RWX                           6m</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p><img src="https://note.youdao.com/yws/api/personal/file/3D015DC3B0394554BA63094C30FD2DA0?method=download&amp;shareKey=e91cb9232660cfa2eec9fff50f1a8e99" alt="k8s-gfs">  </p>
<p>glusterfs 创建pod 应用<br><a href="https://note.youdao.com/yws/api/personal/file/D02C088CA5534C0F85671828732BE559?method=download&amp;shareKey=ca010761cd2d76392fa30b552c0b60c8" target="_blank" rel="external">gfs-pvc-pod.yml</a><br>yaml文件另一种写法：<a href="https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-pod.json" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/examples/volumes/glusterfs/glusterfs-pod.json</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: <span class="built_in">test</span></div><div class="line">  namespace: <span class="string">"dev-00-www-pphuishou-com"</span></div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    - name: nginx</div><div class="line">      image: nginx:1.13</div><div class="line">      volumeMounts:</div><div class="line">      - mountPath: <span class="string">"/var/log/nginx/"</span>   <span class="comment">#把glusterfs(k8s-volume)挂载到/usr/share/nginx/html/目录下</span></div><div class="line">        name: <span class="built_in">log</span></div><div class="line">  volumes:</div><div class="line">    - name: <span class="built_in">log</span></div><div class="line">      persistentVolumeClaim:</div><div class="line">        claimName: glusterfs-pvc</div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: <span class="built_in">test</span>-nginx</div><div class="line">  labels:</div><div class="line">    app: nginx</div><div class="line">spec:</div><div class="line">  <span class="built_in">type</span>: NodePort</div><div class="line">  ports:</div><div class="line">  - port: 80</div><div class="line">    targetPort: 80</div><div class="line">    nodePort: 29000</div><div class="line">  selector:</div><div class="line">    app: nginx</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f gfs-pvc-pod.yml</span></div></pre></td></tr></table></figure>
<p>验证<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods -o wide</span></div><div class="line">NAME                               READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">mypod-gfs                          1/1       Running   0          1d        172.30.57.3   192.168.1.198</div><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc -o wide</span></div><div class="line">NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE       SELECTOR</div><div class="line">nginx-service       NodePort    172.16.215.237   &lt;none&gt;        88:8527/TCP   20d       app=nginx</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>gluster存储在宿主机<code>/data/gfs_data</code>目录下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># gluster volume info</span></div><div class="line">Volume Name: k8s-volume</div><div class="line">...</div><div class="line">Brick1: master1.example.com:/data/gfs_data</div><div class="line">Brick2: master2.example.com:/data/gfs_data</div><div class="line">Brick3: master3.example.com:/data/gfs_data</div><div class="line">Brick4: node1.example.com:/data/gfs_data</div><div class="line">Brick5: node2.example.com:/data/gfs_data</div><div class="line">...</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>向gluster存储目录上传文件(待验证用)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cd /data/gfs_data/</span></div><div class="line">[root@master1 gfs_data]<span class="comment"># ls</span></div><div class="line">index.html</div><div class="line">[root@master1 gfs_data]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>验证通过<br>应用nginx-service使用的是gluster存储<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># docker ps -a</span></div><div class="line">CONTAINER ID        IMAGE      COMMAND                  CREATED          STATUS         PORTS          NAMES</div><div class="line">9dd7a0db5281        nginx      <span class="string">"nginx -g 'daemon of…"</span>   41 hours ago     Up 41 hours                   k8s_nginx_mypod-gfs_default_586a45c2-194d-11e8-b996-1e2d0a5bc3f5_0</div><div class="line">[root@node1 ~]<span class="comment"># docker exec -it 9dd7a0db5281 /bin/bash</span></div><div class="line">root@mypod-gfs:/<span class="comment"># df -h</span></div><div class="line">Filesystem                Size  Used Avail Use% Mounted on</div><div class="line">192.168.1.195:k8s-volume  250G  163M  250G   1% /usr/share/nginx/html</div><div class="line">root@mypod-gfs:/<span class="comment"># </span></div><div class="line">root@mypod-gfs:/<span class="comment"># cd /usr/share/nginx/html/</span></div><div class="line">root@mypod-gfs:/usr/share/nginx/html<span class="comment"># ls</span></div><div class="line">index.html</div><div class="line">root@mypod-gfs:/usr/share/nginx/html<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>Deployment写法<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: <span class="string">"filebeat-deployment"</span></div><div class="line">  namespace: <span class="string">"name"</span></div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: filebeat</div><div class="line">    spec:</div><div class="line">      nodeName: 192.168.2.161</div><div class="line">      containers:</div><div class="line">      - name: filebeat</div><div class="line">        image: yfshare/filebeat:6.2.2</div><div class="line">        imagePullPolicy: Always</div><div class="line">        ports:</div><div class="line">        env:</div><div class="line">        volumeMounts:</div><div class="line">          - name: filebeat-time</div><div class="line">            mountPath: <span class="string">"/etc/localtime"</span></div><div class="line">            <span class="built_in">read</span>Only: <span class="literal">true</span></div><div class="line">          - name: filebeat-yml</div><div class="line">            mountPath: <span class="string">"/etc/filebeat/filebeat.yml"</span></div><div class="line">            <span class="built_in">read</span>Only: <span class="literal">true</span></div><div class="line">          - name: filebeat-logs</div><div class="line">            mountPath: <span class="string">"/var/log/filebeat/"</span></div><div class="line">            <span class="built_in">read</span>Only: <span class="literal">false</span></div><div class="line">          - name: <span class="built_in">read</span>-logs</div><div class="line">            mountPath: <span class="string">"/data"</span></div><div class="line">            <span class="built_in">read</span>Only: <span class="literal">false</span></div><div class="line">      volumes:</div><div class="line">      - name: filebeat-time</div><div class="line">        hostPath:</div><div class="line">          path: <span class="string">"/etc/localtime"</span></div><div class="line">      - name: filebeat-yml</div><div class="line">        hostPath:</div><div class="line">          path: <span class="string">"/root/k8s_elk/filebeat/filebeat.yml"</span></div><div class="line">      - name: filebeat-logs</div><div class="line">        hostPath:</div><div class="line">          path: <span class="string">"/data/elk/filebeat"</span></div><div class="line">      - name: <span class="built_in">read</span>-logs</div><div class="line">        persistentVolumeClaim:</div><div class="line">          claimName: gfs-pvc</div></pre></td></tr></table></figure></p>
<h4 id="删除卷（Delete-Volume）"><a href="#删除卷（Delete-Volume）" class="headerlink" title="删除卷（Delete Volume）"></a>删除卷（Delete Volume）</h4><p>停止卷<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume stop VOLNAME</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume delete VOLNAME</div></pre></td></tr></table></figure>
<h4 id="glusterfs扩容卷（Expanding-Volume）"><a href="#glusterfs扩容卷（Expanding-Volume）" class="headerlink" title="glusterfs扩容卷（Expanding Volume）"></a>glusterfs扩容卷（Expanding Volume）</h4><p>把新增的存储服务器加入存储池<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster peer probe HOSTNAME</div></pre></td></tr></table></figure></p>
<p>把brick加入卷中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume add-brick k8s-volume gfs1.example.com:/mnt/gfs_data force</div></pre></td></tr></table></figure></p>
<p>重新平衡卷<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume rebalance k8s-volume start</div></pre></td></tr></table></figure></p>
<p>检查rebalance状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume rebalance k8s-volume status</div></pre></td></tr></table></figure></p>
<h4 id="glusterfs缩小卷（Shrinking-Volume）"><a href="#glusterfs缩小卷（Shrinking-Volume）" class="headerlink" title="glusterfs缩小卷（Shrinking Volume）"></a>glusterfs缩小卷（Shrinking Volume）</h4><p>把brick从卷中移除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume remove-brick k8s-volume gfs1.example.com:/mnt/gfs_data force</div></pre></td></tr></table></figure></p>
<p>重新平衡卷<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume rebalance k8s-volume start</div></pre></td></tr></table></figure></p>
<p>检查rebalance状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume rebalance k8s-volume status</div></pre></td></tr></table></figure></p>
<h4 id="在线迁移数据（Migrating-Data）"><a href="#在线迁移数据（Migrating-Data）" class="headerlink" title="在线迁移数据（Migrating Data）"></a>在线迁移数据（Migrating Data）</h4><p>迁移数据到另一个brick中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume replace-brick VOLNAME BRICK NEWBRICK start</div></pre></td></tr></table></figure></p>
<p>查看迁移进度状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume replace-brick VOLNAME BRICK NEWBRICK status</div></pre></td></tr></table></figure></p>
<p>提交迁移数据<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume replace-brick VOLNAME BRICK NEWBRICK commit</div></pre></td></tr></table></figure></p>
<h4 id="gluster常用命令"><a href="#gluster常用命令" class="headerlink" title="gluster常用命令"></a>gluster常用命令</h4><p>列出集群中的所有卷<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume list</div></pre></td></tr></table></figure></p>
<p>查看集群中的卷信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gluster volume info [all]</div></pre></td></tr></table></figure></p>
<p>查看集群中的卷状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">gluster volume status [all]</div><div class="line">gluster volume status &lt;VOLNAME&gt; [detail| clients | mem | inode | fd]</div></pre></td></tr></table></figure></p>
<p>参考：<a href="http://blog.csdn.net/i_chips/article/details/12656527" target="_blank" rel="external">http://blog.csdn.net/i_chips/article/details/12656527</a>  </p>
<h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><p>查看pod的日志信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl describe pods <span class="built_in">test</span></div></pre></td></tr></table></figure></p>
<p>如果报下面的错误，则检查下k8s的node节点是否支持glusterfs文件系统<br>通过下面命令可以检查，如果报<code>mount: 未知的文件系统类型“glusterfs”</code>，则说明不支持glusterfs文件系统<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># mount -t glusterfs gfs1.example.com:k8s_gfs /media/</span></div></pre></td></tr></table></figure></p>
<p>解决方法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install -y glusterfs glusterfs-server glusterfs-fuse glust</div></pre></td></tr></table></figure></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/FDF47AC0022C4443BFE638A118C76DEE?method=download&amp;shareKey=575fc1a22ea4fc9754df286701444026" alt="k8s_gfs_error">  </p>
<p>如果报下面的错误，则检查下k8s创建gfs-endpoint和gfs-server时配置文件是否正确<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#查看Volume Name</span></div><div class="line">gluster volume info</div><div class="line"></div><div class="line"><span class="comment">#创建test.yml时，检查创建pv,pvc时配置是否正确</span></div><div class="line">kubectl create <span class="_">-f</span> test.yml</div></pre></td></tr></table></figure></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/091A4A4E1D484F0280C84A1B88B55ABB?method=download&amp;shareKey=a3608e79be82b6bccc2aa52dd30ef327" alt="k8s_gfs_error">  </p>
<p>正确配置如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># kubectl describe pods test</span></div><div class="line"></div><div class="line"><span class="comment"># kubectl get pods -o wide</span></div><div class="line">NAME        READY     STATUS    RESTARTS   AGE       IP             NODE</div><div class="line"><span class="built_in">test</span>        1/1       Running   0          29m       172.20.40.5    192.168.2.11</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/BB6EB5193B4B48DE9ECB82A5EC7B15FF?method=download&amp;shareKey=ce136f4e0364397379cf13366b88586d" alt="k8s_gfs">  </p>
<p>如果报下面的错误，则需要检查下K8s Node节点状态和hosts解析<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#获取当前k8s的Node节点信息，需要看STATUS值是否是Ready</span></div><div class="line">kubectl get nodes</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#需要把gfs所有节点的域名做hosts解析，否则应用挂载时可能找不到gfs节点而导致挂载失败</span></div><div class="line">[root@master1 ~]<span class="comment"># tail -5 /etc/hosts</span></div><div class="line">192.168.1.195 master1.example.com master1</div><div class="line">192.168.1.196 master2.example.com master2</div><div class="line">192.168.1.197 master3.example.com master3</div><div class="line">192.168.1.198 node1.example.com node1</div><div class="line">192.168.1.199 node2.example.com node2</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p><img src="https://note.youdao.com/yws/api/personal/file/8104D32DA6294354A19B3AA7F11D1607?method=download&amp;shareKey=2b523d5d27edccaf2c7b0a7ff19d361b" alt="k8s_gfs_error"><br><img src="https://note.youdao.com/yws/api/personal/file/D8D9344A9F774080AFCAECA50D55C879?method=download&amp;shareKey=205ae14dcd36c6bdfcbdc135d67b436f" alt="k8s_gfs_error">  </p>
<p>参考：<a href="https://jimmysong.io/kubernetes-handbook/practice/using-glusterfs-for-persistent-storage.html" target="_blank" rel="external">https://jimmysong.io/kubernetes-handbook/practice/using-glusterfs-for-persistent-storage.html</a><br><a href="http://blog.csdn.net/i_chips/article/details/12656527" target="_blank" rel="external">http://blog.csdn.net/i_chips/article/details/12656527</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/03/17/glusterfs做持久化存储/">http://www.yfshare.vip/2018/03/17/glusterfs做持久化存储/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Glusterfs简介&lt;br&gt;GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBandRDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="k8s" scheme="http://www.yfshare.vip/tags/k8s/"/>
    
      <category term="Kubernetes" scheme="http://www.yfshare.vip/tags/Kubernetes/"/>
    
      <category term="glusterfs" scheme="http://www.yfshare.vip/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus监控TLS Kubernetes集群</title>
    <link href="http://www.yfshare.vip/2018/03/14/Prometheus%E7%9B%91%E6%8E%A7TLS-Kubernetes%E9%9B%86%E7%BE%A4/"/>
    <id>http://www.yfshare.vip/2018/03/14/Prometheus监控TLS-Kubernetes集群/</id>
    <published>2018-03-14T15:23:44.000Z</published>
    <updated>2018-10-29T15:42:02.146Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>当我们完成Kubernetes集群环境部署后，就需要提取Kubernets集群中POD的日志提取和监控。当集群内的N台服务器在Kubernets的管理下自动创建和销毁POD，但在这种情况下，我们就不方便及时获取所有POD和服务器的运行状态及资源消耗状态，给我们感觉是，驾驶着一辆没有仪表盘的跑车在高速公路上飙车，给人一种心慌的感觉。<br>在以前的工作中，用过Nagios，Cacti，zabbix等监控工具。但在Kubernets集群中，这些工具并不适用。因此，我们需要引入新的监控工具Prometheus。<br><a id="more"></a></p>
<h4 id="Prometheus简介"><a href="#Prometheus简介" class="headerlink" title="Prometheus简介"></a>Prometheus简介</h4><p>Prometheus是SoundCloud开源的一款监控软件。它的实现参考了Google内部的监控实现， 与同样源自Google的Kubernetes项目十分搭配。Prometheus集成了数据采集，存储，异常告警多项功能，是一款一体化的完整方案。它针对大规模的集群环境设计了拉取式的数据采集方式、多维度数据存储格式以及服务发现等创新功能。<br>与传统监控工具相比，Prometheus 可以通过服务发现掌握集群内部已经暴露的监控点，然后主动拉取所有监控数据。通过这样的架构设计，我们仅需要向Kubernetes集群中部署一份Prometheus实例，它就可以通过向<code>apiserver</code>查询集群状态，然后向所有已经支持Prometheus metrics的kubelet获取所有Pod的运行数据。如果我们想采集底层服务器运行状态，通过DaemonSet在所有服务器上运行配套的node-exporter之后，Prometheus就可以自动采集到新的这部分数据。<br>这种动态发现的架构，非常适合服务器和程序都不固定的Kubernetes集群环境，同时也大大降低了运维的负担。  </p>
<p>Prometheus官网：<a href="https://prometheus.io/" target="_blank" rel="external">https://prometheus.io/</a><br>Prometheus官方下载地址：<a href="https://prometheus.io/download/" target="_blank" rel="external">https://prometheus.io/download/</a><br>Prometheus官方文档地址：<a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="external">https://prometheus.io/docs/introduction/overview/</a>  </p>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><p>环境：<br>　　　Prometheus v2.2.0<br>　　　node-exporter v0.15.2<br>　　　Kubernetes v1.8.2<br>　　　Centos 7.4  </p>
<table>
<thead>
<tr>
<th>角色</th>
<th>IP</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8s master</td>
<td>192.168.1.195</td>
<td>k8s master</td>
</tr>
<tr>
<td>k8s node</td>
<td>192.168.1.198</td>
<td>k8s node、Prometheus、node-exporter</td>
</tr>
<tr>
<td>k8s node</td>
<td>192.168.1.199</td>
<td>k8s node、Prometheus、node-exporter</td>
</tr>
</tbody>
</table>
<h4 id="部署node-exporter"><a href="#部署node-exporter" class="headerlink" title="部署node-exporter"></a>部署node-exporter</h4><p>node-exporter可以用于监控底层的服务器指标<br>下面是官方解释：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Prometheus exporter <span class="keyword">for</span> hardware and OS metrics exposed by *NIX kernels, written <span class="keyword">in</span> Go with pluggable metric collectors.</div><div class="line">The WMI exporter is recommended <span class="keyword">for</span> Windows users.</div></pre></td></tr></table></figure></p>
<p>node-exporter Github：<a href="https://github.com/prometheus/node_exporter" target="_blank" rel="external">https://github.com/prometheus/node_exporter</a>  </p>
<p>为了能够收集每个节点的信息，这里使用<code>DaemonSet</code>的形式部署PODS  </p>
<p>node-exporter.yaml：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: DaemonSet</div><div class="line">metadata:</div><div class="line">  name: node-exporter</div><div class="line">  namespace: kube-ops</div><div class="line">  labels:</div><div class="line">    k8s-app: node-exporter</div><div class="line">spec:</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        k8s-app: node-exporter</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - image: prom/node-exporter:latest</div><div class="line">        name: node-exporter</div><div class="line">        ports:</div><div class="line">        - containerPort: 9100</div><div class="line">          hostPort: 9100</div><div class="line">          protocol: TCP</div><div class="line">          name: http</div><div class="line">        volumeMounts:</div><div class="line">          - name: time</div><div class="line">            mountPath: /etc/localtime</div><div class="line">            <span class="built_in">read</span>Only: <span class="literal">true</span></div><div class="line">      volumes:</div><div class="line">        - name: time</div><div class="line">          hostPath:</div><div class="line">            path: /etc/localtime</div><div class="line">---</div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    k8s-app: node-exporter</div><div class="line">  name: node-exporter</div><div class="line">  namespace: kube-ops</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">  - name: http</div><div class="line">    port: 9100</div><div class="line">    targetPort: 9100</div><div class="line">    protocol: TCP</div><div class="line">  selector:</div><div class="line">    k8s-app: node-exporter</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl create namespace kube-ops</span></div><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f node-exporter.yaml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get pod -o wide -n kube-ops</span></div><div class="line">NAME                         READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">node-exporter-8d66t          1/1       Running   0          1h        172.30.41.5   192.168.1.199</div><div class="line">node-exporter-xn5ss          1/1       Running   0          1h        172.30.57.6   192.168.1.198</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get svc -o wide -n kube-ops</span></div><div class="line">NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE       SELECTOR</div><div class="line">node-exporter   ClusterIP   172.16.152.14   &lt;none&gt;        9100/TCP   1h        k8s-app=node-exporter</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="部署Service-Account"><a href="#部署Service-Account" class="headerlink" title="部署Service Account"></a>部署Service Account</h4><p>Kubernetes在1.8.0之后启用了RBAC特性，因此我们需要先通过RBAC授权，然后Prometheus通过RBAC连接Kubernetes集群，否则被拒绝后，将无法连接到K8s的API-SERVER<br>参考：<a href="https://kubernetes.io/docs/admin/authorization/rbac/" target="_blank" rel="external">https://kubernetes.io/docs/admin/authorization/rbac/</a>  </p>
<p>prometheus-service-account.yml：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ServiceAccount</div><div class="line">metadata:</div><div class="line">  name: prometheus</div><div class="line">  namespace: kube-ops</div><div class="line">---</div><div class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</div><div class="line">kind: ClusterRole</div><div class="line">metadata:</div><div class="line">  name: prometheus</div><div class="line">  namespace: kube-ops</div><div class="line">rules:</div><div class="line">- apiGroups: [<span class="string">""</span>]</div><div class="line">  resources:</div><div class="line">  - nodes</div><div class="line">  - nodes/proxy</div><div class="line">  - services</div><div class="line">  - endpoints</div><div class="line">  - pods</div><div class="line">  verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</div><div class="line">- nonResourceURLs: [<span class="string">"/metrics"</span>]</div><div class="line">  verbs: [<span class="string">"get"</span>]</div><div class="line">---</div><div class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</div><div class="line">kind: ClusterRoleBinding</div><div class="line">metadata:</div><div class="line">  name: prometheus</div><div class="line">  namespace: kube-ops</div><div class="line">roleRef:</div><div class="line">  apiGroup: rbac.authorization.k8s.io</div><div class="line">  kind: ClusterRole</div><div class="line">  name: prometheus</div><div class="line">subjects:</div><div class="line">- kind: ServiceAccount</div><div class="line">  name: prometheus</div><div class="line">  namespace: kube-ops</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f prometheus-service-account.yml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get ServiceAccount -n kube-ops</span></div><div class="line">NAME         SECRETS   AGE</div><div class="line">default      1         1d</div><div class="line">prometheus   1         55m</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="部署Prometheus-alertmanager配置文件"><a href="#部署Prometheus-alertmanager配置文件" class="headerlink" title="部署Prometheus alertmanager配置文件"></a>部署Prometheus alertmanager配置文件</h4><p>使用ConfigMap的形式来设置Prometheus的配置文件<br>参考：<a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/" target="_blank" rel="external">https://prometheus.io/docs/prometheus/latest/configuration/configuration/</a>  </p>
<p>prometheus-alertmanager-config.yml：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">kind: ConfigMap</div><div class="line">apiVersion: v1</div><div class="line">metadata:</div><div class="line">  name: alertmanager</div><div class="line">  namespace: kube-ops</div><div class="line">data:</div><div class="line">  config.yml: |-</div><div class="line">    global:</div><div class="line">      smtp_smarthost: <span class="string">'smtp.exmail.qq.com:465'</span></div><div class="line">      smtp_from: <span class="string">'user1@example.com'</span></div><div class="line">      smtp_auth_username: <span class="string">'user1@example.com'</span></div><div class="line">      smtp_auth_password: <span class="string">'password'</span></div><div class="line">      smtp_require_tls: <span class="literal">false</span></div><div class="line">  </div><div class="line">      resolve_timeout: 5m</div><div class="line"></div><div class="line">    templates:</div><div class="line">      - <span class="string">'/etc/alertmanager/*.tmpl'</span></div><div class="line"></div><div class="line">    route:</div><div class="line">      receiver: email</div><div class="line">      group_wait: 30s</div><div class="line">      group_interval: 5m</div><div class="line">      repeat_interval: 10d</div><div class="line">      group_by: [alertname]</div><div class="line">      routes:</div><div class="line">      - receiver: email</div><div class="line">        group_wait: 10s</div><div class="line">        match:</div><div class="line">          team: node</div><div class="line">    receivers:</div><div class="line">    - name: email</div><div class="line">      email_configs:</div><div class="line">      - send_resolved: <span class="literal">true</span></div><div class="line">        to: <span class="string">'user2@example.com,user3@example.com'</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>repeat_interval</code>：指定告警发送间隔时间  </li>
<li><code>to: &#39;user2@example.com,user3@example.com&#39;</code>指定多个收件人，每个收件人邮箱之间用<code>逗号</code>隔开  </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhst prometheus]<span class="comment"># kubectl apply -f prometheus-alertmanager-config.yml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhst prometheus]<span class="comment"># kubectl get ConfigMap -n kube-ops</span></div><div class="line">NAME                DATA      AGE</div><div class="line">alertmanager        2         21h</div><div class="line">[root@localhst prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li>需要修改<code>global.smtp*</code>和<code>receivers.name.email_configs</code>相关的邮件信息  </li>
</ul>
<h4 id="部署Prometheus的配置文件"><a href="#部署Prometheus的配置文件" class="headerlink" title="部署Prometheus的配置文件"></a>部署Prometheus的配置文件</h4><p>prometheus-config.yaml：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ConfigMap</div><div class="line">metadata:</div><div class="line">  name: prometheus-config</div><div class="line">  namespace: kube-ops</div><div class="line">data:</div><div class="line">  prometheus.yml: |</div><div class="line">    global:</div><div class="line">      scrape_interval: 30s</div><div class="line">      scrape_timeout: 30s</div><div class="line">    alerting:</div><div class="line">      alertmanagers:</div><div class="line">        - static_configs:</div><div class="line">            - targets: [<span class="string">"192.168.1.198:9093"</span>]</div><div class="line">    rule_files:</div><div class="line">      - <span class="string">"rules.yml"</span></div><div class="line">    scrape_configs:</div><div class="line">    - job_name: <span class="string">'prometheus'</span></div><div class="line">      static_configs:</div><div class="line">        - targets: [<span class="string">'localhost:9090'</span>]</div><div class="line"></div><div class="line">    - job_name: <span class="string">'kubernetes-apiservers'</span></div><div class="line">      kubernetes_sd_configs:</div><div class="line">      - role: endpoints</div><div class="line">      scheme: https</div><div class="line">      tls_config:</div><div class="line">        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt</div><div class="line">      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token</div><div class="line">      relabel_configs:</div><div class="line">      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]</div><div class="line">        action: keep</div><div class="line">        regex: default;kubernetes;https</div><div class="line"></div><div class="line">    - job_name: <span class="string">'kubernetes-nodes'</span></div><div class="line">      scheme: https</div><div class="line">      tls_config:</div><div class="line">        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt</div><div class="line">      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token</div><div class="line">      kubernetes_sd_configs:</div><div class="line">      - role: node</div><div class="line">      relabel_configs:</div><div class="line">      - action: labelmap</div><div class="line">        regex: __meta_kubernetes_node_label_(.+)</div><div class="line">      - target_label: __address__</div><div class="line">        replacement: 192.168.1.195:6443</div><div class="line">      - source_labels: [__meta_kubernetes_node_name]</div><div class="line">        regex: (.+)</div><div class="line">        target_label: __metrics_path__</div><div class="line">        replacement: /api/v1/nodes/<span class="variable">$&#123;1&#125;</span>/proxy/metrics</div><div class="line"></div><div class="line">    - job_name: <span class="string">'kubernetes-cadvisor'</span></div><div class="line">      scheme: https</div><div class="line">      tls_config:</div><div class="line">        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt</div><div class="line">      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token</div><div class="line">      kubernetes_sd_configs:</div><div class="line">      - role: node</div><div class="line">      relabel_configs:</div><div class="line">      - action: labelmap</div><div class="line">        regex: __meta_kubernetes_node_label_(.+)</div><div class="line">      - target_label: __address__</div><div class="line">        replacement: 192.168.1.195:6443</div><div class="line">      - source_labels: [__meta_kubernetes_node_name]</div><div class="line">        regex: (.+)</div><div class="line">        target_label: __metrics_path__</div><div class="line">        replacement: /api/v1/nodes/<span class="variable">$&#123;1&#125;</span>/proxy/metrics/cadvisor</div><div class="line"></div><div class="line">    - job_name: <span class="string">'kubernetes-node-exporter'</span></div><div class="line">      scheme: http</div><div class="line">      tls_config:</div><div class="line">        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt</div><div class="line">      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token</div><div class="line">      kubernetes_sd_configs:</div><div class="line">      - role: node</div><div class="line">      relabel_configs:</div><div class="line">      - action: labelmap</div><div class="line">        regex: __meta_kubernetes_node_label_(.+)</div><div class="line">      - source_labels: [__meta_kubernetes_role]</div><div class="line">        action: replace</div><div class="line">        target_label: kubernetes_role</div><div class="line">      - source_labels: [__address__]</div><div class="line">        regex: <span class="string">'(.*):10250'</span></div><div class="line">        replacement: <span class="string">'$&#123;1&#125;:9100'</span></div><div class="line">        target_label: __address__</div><div class="line"></div><div class="line">  rules.yml: |</div><div class="line">    groups:</div><div class="line">    - name: rule</div><div class="line">      rules:</div><div class="line">      - alert: NodeFilesystemUsage</div><div class="line">        expr: (node_filesystem_size&#123;device=<span class="string">"rootfs"</span>&#125; - node_filesystem_free&#123;device=<span class="string">"rootfs"</span>&#125;) / node_filesystem_size&#123;device=<span class="string">"rootfs"</span>&#125; * 100 &gt; 80</div><div class="line">        <span class="keyword">for</span>: 2m</div><div class="line">        labels:</div><div class="line">          team: node</div><div class="line">        annotations:</div><div class="line">          summary: <span class="string">"&#123;&#123;<span class="variable">$labels</span>.instance&#125;&#125;: High Filesystem usage detected"</span></div><div class="line">          description: <span class="string">"&#123;&#123;<span class="variable">$labels</span>.instance&#125;&#125;: Filesystem usage is above 80% (current value is: &#123;&#123; <span class="variable">$value</span> &#125;&#125;"</span></div><div class="line">      - alert: NodeMemoryUsage</div><div class="line">        expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80</div><div class="line">        <span class="keyword">for</span>: 2m</div><div class="line">        labels:</div><div class="line">          team: node</div><div class="line">        annotations:</div><div class="line">          summary: <span class="string">"&#123;&#123;<span class="variable">$labels</span>.instance&#125;&#125;: High Memory usage detected"</span></div><div class="line">          description: <span class="string">"&#123;&#123;<span class="variable">$labels</span>.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; <span class="variable">$value</span> &#125;&#125;"</span></div><div class="line">      - alert: NodeCPUUsage</div><div class="line">        expr: (100 - (avg by (instance) (irate(node_cpu&#123;job=<span class="string">"kubernetes-node-exporter"</span>,mode=<span class="string">"idle"</span>&#125;[5m])) * 100)) &gt; 80</div><div class="line">        <span class="keyword">for</span>: 2m</div><div class="line">        labels:</div><div class="line">          team: node</div><div class="line">        annotations:</div><div class="line">          summary: <span class="string">"&#123;&#123;<span class="variable">$labels</span>.instance&#125;&#125;: High CPU usage detected"</span></div><div class="line">          description: <span class="string">"&#123;&#123;<span class="variable">$labels</span>.instance&#125;&#125;: CPU usage is above 80% (current value is: &#123;&#123; <span class="variable">$value</span> &#125;&#125;"</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>job_name: &#39;kubernetes-node-exporter&#39;</code>中替换31672端口为9100，该端口是<code>node-exporter</code>暴露的<code>NodePort</code>端口，这里需要根据实际情况填写<br>在前面node-exporter.yaml中指定了<code>targetPort: 9100</code>，所以这里的端口需要修改为9100  </li>
<li><code>kubernetes.default.svc:443</code>为k8s api地址，如果安装k8s时不是使用的默认DNS，则需要手动修改  </li>
<li>新增了Prometheus alertmanagers，需要修改<code>alerting.alertmanagers.static_configs.targets</code>的IP地址，这时Prometheus和alertmanagers是两个docker容器，IP为运行alertmanagers的宿主机的IP地址  </li>
<li>新增了Prometheus alertmanagers告警规则，添加<code>rule_files</code>(指定报警规则)，并增加三条规则(rules.yml)  </li>
<li>这里新增的三条报警规则分别是：节点的文件系统，节点内存，CPU的使用量。如果大于了80%的话就触发label为<code>team=node</code>的<code>receiver</code>(alertmanager  配置文件中配置)，可以看到上面的配置就会匹配<code>email</code>这个<code>receiver</code>  </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f prometheus-config.yaml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@localhst prometheus]<span class="comment"># kubectl get ConfigMap -n kube-ops</span></div><div class="line">NAME                DATA      AGE</div><div class="line">alertmanager        2         21h</div><div class="line">prometheus-config   1         21h</div><div class="line">[root@localhst prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h4><p>使用Deployment的形式来设置Prometheus<br>参考：<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="external">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a>  </p>
<p>创建Node Label:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># kubectl label node 192.168.1.198 "appNodes=pro-00-monitor"</span></div><div class="line">node <span class="string">"192.168.1.198"</span> labeled</div><div class="line">[root@localhost ~]<span class="comment"># kubectl get node -a -l "appNodes=pro-00-monitor"</span></div><div class="line">NAME            STATUS    ROLES     AGE       VERSION</div><div class="line">192.168.1.198   Ready     &lt;none&gt;    24m       v1.9.2</div><div class="line">[root@localhost ~]<span class="comment"># </span></div><div class="line">[root@localhost ~]<span class="comment"># mkdir -p /data/monitor   #创建数据挂载目录</span></div></pre></td></tr></table></figure></p>
<p>prometheus-deploy.yaml:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    k8s-app: prometheus</div><div class="line">  name: prometheus</div><div class="line">  namespace: kube-ops</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        k8s-app: prometheus</div><div class="line">    spec:</div><div class="line">      nodeSelector:</div><div class="line">        appNodes: pro-00-monitor</div><div class="line">      securityContext:</div><div class="line">        runAsUser: 0</div><div class="line">      serviceAccountName: prometheus</div><div class="line">      containers:</div><div class="line">      - image: prom/prometheus:v2.2.0</div><div class="line">        name: prometheus</div><div class="line">        <span class="built_in">command</span>:</div><div class="line">        - <span class="string">"/bin/prometheus"</span></div><div class="line">        args:</div><div class="line">        - <span class="string">"--config.file=/etc/prometheus/prometheus.yml"</span></div><div class="line">        - <span class="string">"--storage.tsdb.path=/prometheus"</span></div><div class="line">        - <span class="string">"--storage.tsdb.retention=15d"</span></div><div class="line">        ports:</div><div class="line">        - containerPort: 9090</div><div class="line">          hostPort: 9090</div><div class="line">          protocol: TCP</div><div class="line">          name: http</div><div class="line">        volumeMounts:</div><div class="line">        - mountPath: <span class="string">"/prometheus"</span></div><div class="line">          name: data</div><div class="line">          subPath: prometheus/data</div><div class="line">        - mountPath: <span class="string">"/etc/prometheus"</span></div><div class="line">          name: config-volume</div><div class="line">        - mountPath: <span class="string">"/etc/localtime"</span></div><div class="line">          name: time</div><div class="line">          <span class="built_in">read</span>Only: <span class="literal">true</span></div><div class="line">        resources:</div><div class="line">          requests:</div><div class="line">            cpu: 1</div><div class="line">            memory: 1Gi</div><div class="line">          limits:</div><div class="line">            cpu: 1</div><div class="line">            memory: 2Gi</div><div class="line">      - image: prom/alertmanager:v0.14.0</div><div class="line">        name: alertmanager</div><div class="line">        args: </div><div class="line">        - <span class="string">"--config.file=/etc/alertmanager/config.yml"</span></div><div class="line">        - <span class="string">"--storage.path=/alertmanager"</span></div><div class="line">        ports:</div><div class="line">        - containerPort: 9093</div><div class="line">          hostPort: 9093</div><div class="line">          protocol: TCP</div><div class="line">          name: http</div><div class="line">        volumeMounts:</div><div class="line">        - name: alertmanager-config-volume</div><div class="line">          mountPath: /etc/alertmanager</div><div class="line">        resources:</div><div class="line">          requests:</div><div class="line">            memory: 500Mi</div><div class="line">          limits:</div><div class="line">            memory: 1024Mi</div><div class="line"></div><div class="line">      volumes:</div><div class="line">      - name: data</div><div class="line">        hostPath:</div><div class="line">          path: <span class="string">"/data/monitor"</span></div><div class="line">      - name: time</div><div class="line">        hostPath:</div><div class="line">          path: <span class="string">"/etc/localtime"</span></div><div class="line">      - configMap:</div><div class="line">          name: prometheus-config</div><div class="line">        name: config-volume</div><div class="line">      - name: alertmanager-config-volume</div><div class="line">        configMap:</div><div class="line">          name: alertmanager</div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f prometheus-deploy.yaml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get pod -o wide -n kube-ops</span></div><div class="line">prometheus-fc7685cc7-rwlc7   1/1       Running   0          34s       172.30.57.7   192.168.1.198</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhst ~]<span class="comment"># netstat -tunlp |egrep '9090|9093'</span></div><div class="line">tcp6       0      0 :::9090                 :::*                    LISTEN      4023/docker-proxy   </div><div class="line">tcp6       0      0 :::9093                 :::*                    LISTEN      3983/docker-proxy   </div><div class="line">[root@localhst ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="访问prometheus"><a href="#访问prometheus" class="headerlink" title="访问prometheus"></a>访问prometheus</h4><p>prometheus启动成功后，我们就可以打开prometheus dashboard查看了，访问 <a href="http://ip:9090/graph" target="_blank" rel="external">http://ip:9090/graph</a> ，点<code>status</code>–&gt;<code>Targets</code><br>可以看到prometheus已经成功访问到k8s api-server并获取到监控指标<br><img src="https://note.youdao.com/yws/api/personal/file/5A5C1E0628B743DFA4F4B2B73F2429EC?method=download&amp;shareKey=202dd33859c3cb11ad6ae23b375830c0" alt="prometheus_dashboard">  </p>
<h4 id="访问Alertmanager"><a href="#访问Alertmanager" class="headerlink" title="访问Alertmanager"></a>访问Alertmanager</h4><p>alertmanager启动后，我们可以打开alertmanager Dashboard查看，访问 <a href="http://ip:9093" target="_blank" rel="external">http://ip:9093</a><br>当然在prometheus dashboard也可以查看，在status –&gt; Runtime &amp; Build Information 最底部<br><img src="https://note.youdao.com/yws/api/personal/file/98EFE5C15FC540FA908C5C1497D91920?method=download&amp;shareKey=0b20c6a63adc15aeaa13dd0d4a5ea71b" alt="prometheus_dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/2D1F17C7438142699F963870A6390F11?method=download&amp;shareKey=2402d8b386b7043bd5f1f786f887a6d7" alt="alertmanager_dashboard">  </p>
<h4 id="告警规则"><a href="#告警规则" class="headerlink" title="告警规则"></a>告警规则</h4><p>在Prometheus定义的rules生效后，可以在Status –&gt;Rules 这里看到<br><img src="https://note.youdao.com/yws/api/personal/file/9B322CA7B9D94AC2A64A4D0F6EF7782E?method=download&amp;shareKey=ec1e3d77582b99329a81377f5b06af3c" alt="Prometheus_rules">  </p>
<ul>
<li>点击的<code>expr</code>会直接跳转到<code>Prometheus graph</code>页面查询，在制定报警规则的时候，可以先在Prometheus中测试表达式  </li>
</ul>
<p>在Prometheus的Alerts这里可以看到触发告警规则的状态<br><img src="https://note.youdao.com/yws/api/personal/file/BFEF5FDAB48243208845A6B9ECB73E82?method=download&amp;shareKey=cb282de4e9235539cfd9de06c25bb8f8" alt="Prometheus_alerts">  </p>
<p>目前有三台主机成功触发规则<br><img src="https://note.youdao.com/yws/api/personal/file/99BC064346EF4145B7BFC379074D6E95?method=download&amp;shareKey=dbb2f00e073c3decc6ac212e89afde20" alt="Prometheus_alerts"><br>一个报警信息在生命周期内有下面3中状态：  </p>
<ul>
<li><code>inactive</code>: 表示当前报警信息既不是<code>firing</code>状态也不是<code>pending</code>状态  </li>
<li><code>pending</code>: 表示在设置的阈值时间范围内被激活。这时Prometheus处于等待状态，大概等待3分钟左右，整合所有的告警条目，等待集中发送给Alertmanager  </li>
<li><code>firing</code>: 表示超过设置的阈值时间被激活。这时Prometheus处于发送告警到Alertmanager阶段，也是最终状态  </li>
</ul>
<p>触发规则后，也可以在Alertmanager Dashboard上看到<br><img src="https://note.youdao.com/yws/api/personal/file/286134D8EF3840CEA3007194B8DC3F94?method=download&amp;shareKey=0fed791095cd1092ac1edd0d5371afc2" alt="Alertmanager_dashboard">  </p>
<p>最后来一张，我们成功收到Alertmanager的告警邮件截图<br><img src="https://note.youdao.com/yws/api/personal/file/76A8767C9A4A4D918619F8FAEBC1D5C6?method=download&amp;shareKey=f1e80c27014145cc5690093c25dda3d1" alt="Alertmanager_mail">  </p>
<h5 id="查询监控数据"><a href="#查询监控数据" class="headerlink" title="查询监控数据"></a>查询监控数据</h5><p>Prometheus提供了<code>API</code>的方式进行数据查询，同样可以使用<code>query语言</code>进行复杂的查询任务<br>点<code>Graph</code><br>查询每个POD的CPU使用情况，输入：<br><code>sum by (pod_name)( rate(container_cpu_usage_seconds_total{image!=&quot;&quot;, pod_name!=&quot;&quot;}[1m] ) )</code><br><img src="https://note.youdao.com/yws/api/personal/file/B481FAA4C243432B866A7F842B360E5D?method=download&amp;shareKey=29a94a564c7abd86ff5f1317ffb76abe" alt="prometheus_dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/6ED534C0E64B4E12A72B5BD9E3851277?method=download&amp;shareKey=440bb39fc1c4e47e8e812b283553d30e" alt="prometheus_dashboard">  </p>
<p>更多查询条件参考：<br><a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank" rel="external">https://prometheus.io/docs/prometheus/latest/querying/basics/</a><br><a href="https://prometheus.io/docs/prometheus/latest/querying/api/" target="_blank" rel="external">https://prometheus.io/docs/prometheus/latest/querying/api/</a><br><a href="https://prometheus.io/docs/prometheus/latest/querying/examples/" target="_blank" rel="external">https://prometheus.io/docs/prometheus/latest/querying/examples/</a>  </p>
<h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><p>Question：<br>如果遇到下面的报错：<br><img src="https://note.youdao.com/yws/api/personal/file/8657660A593F4FFBBC965F98A79D973E?method=download&amp;shareKey=7eb28760c6b0ae77dabd41cadb223118" alt="prometheus_error"><br><img src="https://note.youdao.com/yws/api/personal/file/1CFA492189C44463B3D098625571B0BB?method=download&amp;shareKey=2b33b854b21704a8f10d677b3e645ee2" alt="prometheus_error">  </p>
<p>Answer：<br>在<code>prometheus-deploy.yaml</code>中<code>spec.spec.</code>下增加如下配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">securityContext:</div><div class="line">  runAsUser: 0</div></pre></td></tr></table></figure></p>
<p>详细配置参考上面prometheus-deploy.yaml配置文件<br>参考：<a href="https://github.com/prometheus/prometheus/issues/2939" target="_blank" rel="external">https://github.com/prometheus/prometheus/issues/2939</a>  </p>
<p>Question：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">level=error ts=2018-04-23T13:08:34.417214948Z <span class="built_in">caller</span>=notify.go:303 component=dispatcher msg=<span class="string">"Error on notify"</span> err=<span class="string">"dial t</span></div><div class="line">cp 14.18.245.164:25: getsockopt: connection timed out"level=error ts=2018-04-23T13:08:34.417316796Z <span class="built_in">caller</span>=dispatch.go:266 component=dispatcher msg=<span class="string">"Notify for alerts failed"</span> </div><div class="line">num_alerts=3 err=<span class="string">"dial tcp 14.18.245.164:25: getsockopt: connection timed out"</span></div></pre></td></tr></table></figure></p>
<p>Answer：<br>遇到上面的报错，首先先检查下docker容器是否联网，再测试下与smtp服务器是否正常通讯<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">telnet smtp.qq.com 25</div></pre></td></tr></table></figure></p>
<p>在测试过程中发现，腾讯邮箱(个人QQ邮件+企业邮箱)，只支持SMTP SSL 465端口，且不支持TLS，<code>smtp_require_tls</code>这个参数官方默认是<code>true</code>，这里需要设置为 <code>smtp_require_tls: false</code>，使用不同邮箱的SMTP 需要具体测试。  </p>
<h4 id="Prometheus监控Kubernetes-HTTP集群"><a href="#Prometheus监控Kubernetes-HTTP集群" class="headerlink" title="Prometheus监控Kubernetes HTTP集群"></a>Prometheus监控Kubernetes HTTP集群</h4><p>环境：<br>　　　Prometheus v1.0.1<br>　　　node-exporter v0.15.2<br>　　　Kubernetes v1.8.2  </p>
<p>服务器环境如上  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#获取k8s kube-apiserver地址和端口</span></div><div class="line">[root@localhost ~]<span class="comment"># netstat -tunlp |grep -i kube-apiserver</span></div><div class="line">tcp        0      0 192.168.1.195:6443      0.0.0.0:*               LISTEN      4555/kube-apiserver </div><div class="line">tcp        0      0 192.168.1.195:8080      0.0.0.0:*               LISTEN      4555/kube-apiserver </div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f node-exporter.yaml </span></div><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f prometheus-config.yaml </span></div><div class="line">[root@localhost prometheus]<span class="comment"># kubectl apply -f prometheus-deploy.yaml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get pod -n kube-ops -o wide</span></div><div class="line">NAME                          READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">node-exporter-hjgds           1/1       Running   0          23m       172.30.41.5   192.168.1.199</div><div class="line">node-exporter-zmlcg           1/1       Running   0          23m       172.30.57.6   192.168.1.198</div><div class="line">prometheus-5f86bc8bc5-tbnxp   1/1       Running   0          10m       172.30.41.6   192.168.1.199</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get svc -n kube-ops -o wide</span></div><div class="line">NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE       SELECTOR</div><div class="line">node-exporter   ClusterIP   172.16.40.83   &lt;none&gt;        9100/TCP   24m       k8s-app=node-exporter</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost prometheus]<span class="comment"># kubectl get ConfigMap -n kube-ops -o wide</span></div><div class="line">NAME                DATA      AGE</div><div class="line">prometheus-config   1         22m</div><div class="line">[root@localhost prometheus]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p><img src="https://note.youdao.com/yws/api/personal/file/15FCF902C5EB4A54A3728C01830296A5?method=download&amp;shareKey=29ebc89d7b0cc24633dbcf0918e2d9ab" alt="prometheus_dashboard_http">  </p>
<p>其他的同上，Prometheus监控Kubernetes HTTP集群配置文件见附件<br>注：经测试，目前只发现<code>Prometheus v1.0.1</code>支持，测试其他版本都有报错  </p>
<p>参考：<br><a href="https://blog.qikqiak.com/post/kubernetes-monitor-prometheus-grafana/" target="_blank" rel="external">https://blog.qikqiak.com/post/kubernetes-monitor-prometheus-grafana/</a><br><a href="https://blog.qikqiak.com/post/update-prometheus-2-in-kubernetes/" target="_blank" rel="external">https://blog.qikqiak.com/post/update-prometheus-2-in-kubernetes/</a><br><a href="https://github.com/cnych/k8s-repo/tree/master/prometheus" target="_blank" rel="external">https://github.com/cnych/k8s-repo/tree/master/prometheus</a><br><a href="https://blog.qikqiak.com/post/alertmanager-of-prometheus-in-practice/" target="_blank" rel="external">https://blog.qikqiak.com/post/alertmanager-of-prometheus-in-practice/</a><br><a href="https://blog.csdn.net/qq_21398167/article/details/76008594?locationnum=10&amp;fps=1" target="_blank" rel="external">https://blog.csdn.net/qq_21398167/article/details/76008594?locationnum=10&amp;fps=1</a><br><a href="https://segmentfault.com/a/1190000008695463" target="_blank" rel="external">https://segmentfault.com/a/1190000008695463</a><br><a href="https://prometheus.io/docs/alerting/overview/" target="_blank" rel="external">https://prometheus.io/docs/alerting/overview/</a>  </p>
<p>附件：<br><a href="https://note.youdao.com/yws/api/personal/file/E0F2C93038D3412091FDA362DB3B5C77?method=download&amp;shareKey=debbdb748ebf0483c64e3ee2e3e5eb0d" target="_blank" rel="external">Prometheus监控TLS K8S配置文件.zip</a><br><a href="https://note.youdao.com/yws/api/personal/file/E14D629E3CD24DB180F879AC4B8D89F3?method=download&amp;shareKey=ec648c66a75a232d80cd93ed540b3960" target="_blank" rel="external">Prometheus监控K8S HTTP配置文件.zip</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/03/14/Prometheus监控TLS-Kubernetes集群/">http://www.yfshare.vip/2018/03/14/Prometheus监控TLS-Kubernetes集群/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们完成Kubernetes集群环境部署后，就需要提取Kubernets集群中POD的日志提取和监控。当集群内的N台服务器在Kubernets的管理下自动创建和销毁POD，但在这种情况下，我们就不方便及时获取所有POD和服务器的运行状态及资源消耗状态，给我们感觉是，驾驶着一辆没有仪表盘的跑车在高速公路上飙车，给人一种心慌的感觉。&lt;br&gt;在以前的工作中，用过Nagios，Cacti，zabbix等监控工具。但在Kubernets集群中，这些工具并不适用。因此，我们需要引入新的监控工具Prometheus。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Prometheus" scheme="http://www.yfshare.vip/tags/Prometheus/"/>
    
      <category term="Kubernetes" scheme="http://www.yfshare.vip/tags/Kubernetes/"/>
    
      <category term="K8S" scheme="http://www.yfshare.vip/tags/K8S/"/>
    
  </entry>
  
  <entry>
    <title>部署TLS k8s</title>
    <link href="http://www.yfshare.vip/2018/02/23/%E9%83%A8%E7%BD%B2TLS-k8s/"/>
    <id>http://www.yfshare.vip/2018/02/23/部署TLS-k8s/</id>
    <published>2018-02-23T12:44:12.000Z</published>
    <updated>2018-05-28T11:20:32.122Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Kubernetes是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效(powerful)，Kubernetes提供了应用部署，规划，更新，维护的一种机制。Kubernetes一个核心的特点就是能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行。<br><a id="more"></a><br>环境：<br>　　　Centos 7.4.1708<br>　　　dockers 18.02.0-ce-rc1<br>　　　kubernetes v1.9.2<br>　　　etcd 3.2.15  </p>
<p>k8s下载地址：<a href="https://github.com/kubernetes/kubernetes/releases" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/releases</a><br><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.9.md#v192" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.9.md#v192</a>  </p>
<h4 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h4><p>同步时间，所有节点均操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime -a</span></div><div class="line"><span class="comment"># ntpdate s1a.time.edu.cn</span></div><div class="line"><span class="comment"># crontab -l</span></div><div class="line">* */3 * * * ntpdate s1a.time.edu.cn &amp;&gt; /dev/null</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>设置主机名<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># hostname master1.example.com</span></div><div class="line">[root@master2 ~]<span class="comment"># hostname master2.example.com</span></div><div class="line">[root@master3 ~]<span class="comment"># hostname master3.example.com</span></div><div class="line">[root@node1 ~]<span class="comment"># hostname node1.example.com</span></div><div class="line">[root@node2 ~]<span class="comment"># hostname node2.example.com</span></div><div class="line"></div><div class="line"><span class="comment">#三个节点均做hosts解析</span></div><div class="line"><span class="comment"># tail -5 /etc/hosts</span></div><div class="line">192.168.1.195 master1.example.com master1</div><div class="line">192.168.1.196 master2.example.com master2</div><div class="line">192.168.1.197 master3.example.com master3</div><div class="line">192.168.1.198 node1.example.com node1</div><div class="line">192.168.1.199 node2.example.com node2</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>关闭防火墙和Selinux，所有节点都操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">iptables -F</div><div class="line">systemctl stop firewalld</div><div class="line">systemctl <span class="built_in">disable</span> firewalld</div><div class="line">setenforce 0</div></pre></td></tr></table></figure></p>
<h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><table>
<thead>
<tr>
<th>角色</th>
<th>IP</th>
<th>组件</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kube-Master + ETCD1</td>
<td>192.168.1.195</td>
<td>etcd、kube-apiserver、kube-controller-manager、kube-scheduler、Flannel</td>
</tr>
<tr>
<td>ETCD2</td>
<td>192.168.1.196</td>
<td>etcd</td>
</tr>
<tr>
<td>ETCD3</td>
<td>192.168.1.197</td>
<td>etcd</td>
</tr>
<tr>
<td>Kube-Node1</td>
<td>192.168.1.198</td>
<td>kubelet、kube-proxy、docker、Flannel</td>
</tr>
<tr>
<td>Kube-Node2</td>
<td>192.168.1.199</td>
<td>kubelet、kube-proxy、docker、Flannel</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>名称</th>
<th>网段/地址</th>
<th>参数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Service_CIDR</td>
<td>172.16.0.0/16</td>
<td><code>--service-cluster-ip-range</code></td>
<td>服务 网段</td>
</tr>
<tr>
<td>Cluster_CIDR</td>
<td>172.30.0.0/16</td>
<td><code>--cluster-cidr</code></td>
<td>POD 网段</td>
</tr>
<tr>
<td>CLUSTER_KUBERNETES_SVC_IP</td>
<td>172.16.0.1</td>
<td>-</td>
<td>kubernetes 服务IP，SERVICE_CIDR 中第一个IP</td>
</tr>
<tr>
<td>CLUSTER_DNS_SVC_IP</td>
<td>172.16.0.2</td>
<td>-</td>
<td>集群DNS 服务IP，SERVICE_CIDR 中第二个IP</td>
</tr>
<tr>
<td>NODE_PORT_RANGE</td>
<td>8400-9000</td>
<td>服务端口 范围</td>
</tr>
<tr>
<td>CLUSTER_DNS_DOMAIN</td>
<td>-</td>
<td><code>cluster.local.</code></td>
<td>集群DNS域名</td>
</tr>
<tr>
<td>FLANNEL_ETCD_PREFIX</td>
<td>-</td>
<td><code>/kubernetes/network</code></td>
<td>flanneld 网络配置前缀</td>
</tr>
</tbody>
</table>
<p>安装Docker<br>在node1/node2这2个节点都安装docker引擎<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">yum remove docker docker-common docker-selinux docker-engine -y</div><div class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</div><div class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</div><div class="line">yum-config-manager --enable docker-ce-edge</div><div class="line">yum-config-manager --enable docker-ce-test</div><div class="line">yum install docker-ce -y</div><div class="line">systemctl <span class="built_in">enable</span> docker</div><div class="line">systemctl start docker</div></pre></td></tr></table></figure></p>
<h4 id="创建-CA-证书和秘钥"><a href="#创建-CA-证书和秘钥" class="headerlink" title="创建 CA 证书和秘钥"></a>创建 CA 证书和秘钥</h4><p>kubernetes 系统各组件需要使用 TLS 证书对通信进行加密，这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件，CA 是自签名的证书，用来签名后续创建的其它 TLS 证书  </p>
<h5 id="安装-CFSSL"><a href="#安装-CFSSL" class="headerlink" title="安装 CFSSL"></a>安装 CFSSL</h5><p>cfssl下载地址：<a href="https://github.com/cloudflare/cfssl/releases" target="_blank" rel="external">https://github.com/cloudflare/cfssl/releases</a><br><a href="https://note.youdao.com/yws/api/personal/file/E9CE8AF7520D47F69337DCE8F6DBAC9E?method=download&amp;shareKey=240e513d965d3476e1aeaae363ca8689" target="_blank" rel="external">cfssl R1.2工具包本地下载</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl</span></div><div class="line">[root@master1 ~]<span class="comment"># wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson</span></div><div class="line">[root@master1 ~]<span class="comment"># wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfo</span></div><div class="line">[root@master1 ~]<span class="comment"># chmod +x /usr/local/bin/cfssl*</span></div><div class="line">[root@master1 ~]<span class="comment"># mkdir ssl &amp;&amp; cd ssl</span></div><div class="line"></div><div class="line"><span class="comment">#测试</span></div><div class="line">[root@master1 ssl]<span class="comment"># cfssl print-defaults config &gt; config.json</span></div><div class="line">[root@master1 ssl]<span class="comment"># cfssl print-defaults csr &gt; csr.json</span></div></pre></td></tr></table></figure></p>
<h5 id="创建-CA-Certificate-Authority"><a href="#创建-CA-Certificate-Authority" class="headerlink" title="创建 CA (Certificate Authority)"></a>创建 CA (Certificate Authority)</h5><table>
<thead>
<tr>
<th>证书名称</th>
<th>配置文件</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>ca.pem</td>
<td>ca-config.json</td>
<td>CA 配置文件</td>
</tr>
<tr>
<td>etcd.pem</td>
<td>ca-csr.json</td>
<td>CA 证书</td>
</tr>
</tbody>
</table>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat ca-config.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"signing"</span>: &#123;</div><div class="line">    <span class="string">"default"</span>: &#123;</div><div class="line">      <span class="string">"expiry"</span>: <span class="string">"8760h"</span></div><div class="line">    &#125;,</div><div class="line">    <span class="string">"profiles"</span>: &#123;</div><div class="line">      <span class="string">"kubernetes"</span>: &#123;</div><div class="line">        <span class="string">"usages"</span>: [</div><div class="line">            <span class="string">"signing"</span>,</div><div class="line">            <span class="string">"key encipherment"</span>,</div><div class="line">            <span class="string">"server auth"</span>,</div><div class="line">            <span class="string">"client auth"</span></div><div class="line">        ],</div><div class="line">        <span class="string">"expiry"</span>: <span class="string">"8760h"</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li>ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；</li>
<li>signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；</li>
<li>server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证；</li>
<li>client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证；</li>
</ul>
<h5 id="创建-CA-证书签名请求"><a href="#创建-CA-证书签名请求" class="headerlink" title="创建 CA 证书签名请求"></a>创建 CA 证书签名请求</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat ca-csr.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</div><div class="line">  <span class="string">"key"</span>: &#123;</div><div class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</div><div class="line">    <span class="string">"size"</span>: 2048</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"names"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</div><div class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</div><div class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li>“CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；</li>
<li>“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；</li>
</ul>
<h5 id="生成-CA-证书和私钥"><a href="#生成-CA-证书和私钥" class="headerlink" title="生成 CA 证书和私钥"></a>生成 CA 证书和私钥</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span></div><div class="line"></div><div class="line">[root@master1 ssl]<span class="comment"># ls ca*</span></div><div class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="分发证书"><a href="#分发证书" class="headerlink" title="分发证书"></a>分发证书</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># mkdir -p /etc/kubernetes/ssl</span></div><div class="line">[root@master1 ssl]<span class="comment"># cp ca* /etc/kubernetes/ssl</span></div><div class="line">[root@master1 ssl]<span class="comment"># ls /etc/kubernetes/ssl/</span></div><div class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># ssh root@192.168.1.196 "mkdir -p /etc/kubernetes/ssl"</span></div><div class="line">[root@master1 ssl]<span class="comment"># scp /etc/kubernetes/ssl/ca* root@192.168.1.196:/etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># ssh root@192.168.1.197 "mkdir -p /etc/kubernetes/ssl"</span></div><div class="line">[root@master1 ssl]<span class="comment"># scp /etc/kubernetes/ssl/ca* root@192.168.1.197:/etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># ssh root@192.168.1.198 "mkdir -p /etc/kubernetes/ssl"</span></div><div class="line">[root@master1 ssl]<span class="comment"># scp /etc/kubernetes/ssl/ca* root@192.168.1.198:/etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># ssh root@192.168.1.199 "mkdir -p /etc/kubernetes/ssl"</span></div><div class="line">[root@master1 ssl]<span class="comment"># scp /etc/kubernetes/ssl/ca* root@192.168.1.199:/etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure>
<h4 id="部署高可用-etcd-集群"><a href="#部署高可用-etcd-集群" class="headerlink" title="部署高可用 etcd 集群"></a>部署高可用 etcd 集群</h4><p>需要关闭 selinux，关闭防火墙，ntpdate 时间同步<br>kuberntes 系统使用 etcd 存储所有数据，这里和kuberntes master安装到一起<br>三个etcd分别取名为：etcd1、etcd2、etcd3  </p>
<table>
<thead>
<tr>
<th>集群名称</th>
<th>IP</th>
<th>集群地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd1</td>
<td>192.168.1.195</td>
<td><a href="https://192.168.1.195:2379" target="_blank" rel="external">https://192.168.1.195:2379</a></td>
</tr>
<tr>
<td>etcd2</td>
<td>192.168.1.196</td>
<td><a href="https://192.168.1.196:2379" target="_blank" rel="external">https://192.168.1.196:2379</a></td>
</tr>
<tr>
<td>etcd3</td>
<td>192.168.1.197</td>
<td><a href="https://192.168.1.197:2379" target="_blank" rel="external">https://192.168.1.197:2379</a></td>
</tr>
</tbody>
</table>
<h5 id="安装-Etcd"><a href="#安装-Etcd" class="headerlink" title="安装 Etcd"></a>安装 Etcd</h5><p><strong>三个节点master均安装</strong><br>etcd下载地址：<a href="https://github.com/coreos/etcd/releases" target="_blank" rel="external">https://github.com/coreos/etcd/releases</a><br><a href="https://note.youdao.com/yws/api/personal/file/A48C40B53E5442D0A87D037CF9683155?method=download&amp;shareKey=1925129c3689f9cf58802e996e8103bb" target="_blank" rel="external">etcd-v3.2.15本地下载</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://github.com/coreos/etcd/releases/download/v3.2.15/etcd-v3.2.15-linux-amd64.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># tar -zxf etcd-v3.2.15-linux-amd64.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># cp -a etcd-v3.2.15-linux-amd64/etcd* /usr/local/bin/</span></div></pre></td></tr></table></figure></p>
<h5 id="创建-TLS-秘钥和证书"><a href="#创建-TLS-秘钥和证书" class="headerlink" title="创建 TLS 秘钥和证书"></a>创建 TLS 秘钥和证书</h5><p>为了保证通信安全，客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的通信需要使用 TLS 加密  </p>
<p>创建 etcd 证书签名请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># mkdir -p ssl/</span></div><div class="line">[root@master1 ~]<span class="comment"># cd ssl/</span></div><div class="line">[root@master1 ssl]<span class="comment"># cat etcd-csr.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</div><div class="line">  <span class="string">"hosts"</span>: [</div><div class="line">    <span class="string">"127.0.0.1"</span>,</div><div class="line">    <span class="string">"192.168.1.195"</span>,</div><div class="line">    <span class="string">"192.168.1.196"</span>,</div><div class="line">    <span class="string">"192.168.1.197"</span></div><div class="line">  ],</div><div class="line">  <span class="string">"key"</span>: &#123;</div><div class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</div><div class="line">    <span class="string">"size"</span>: 2048</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"names"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</div><div class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</div><div class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li>hosts 字段指定授权使用该证书的 etcd 节点 IP</li>
</ul>
<p>生成 etcd 证书和私钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubernetes/ssl/ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span></div><div class="line">[root@master1 ssl]<span class="comment"># ls etcd*</span></div><div class="line">etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem</div><div class="line">[root@master1 ssl]<span class="comment"># mkdir -p /etc/etcd/ssl</span></div><div class="line">[root@master1 ssl]<span class="comment"># cp etcd*.pem /etc/etcd/ssl/</span></div></pre></td></tr></table></figure></p>
<p>创建 etcd 的 systemd unit 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># mkdir -p /var/lib/etcd</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat etcd.service </span></div><div class="line">[Unit]</div><div class="line">Description=Etcd Server</div><div class="line">After=network.target</div><div class="line">After=network-online.target</div><div class="line">Wants=network-online.target</div><div class="line">Documentation=https://github.com/coreos</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line">WorkingDirectory=/var/lib/etcd/</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/etcd \</div><div class="line">  --name=etcd1 \</div><div class="line">  --cert-file=/etc/etcd/ssl/etcd.pem \</div><div class="line">  --key-file=/etc/etcd/ssl/etcd-key.pem \</div><div class="line">  --peer-cert-file=/etc/etcd/ssl/etcd.pem \</div><div class="line">  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \</div><div class="line">  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --initial-advertise-peer-urls=https://192.168.1.195:2380 \</div><div class="line">  --listen-peer-urls=https://192.168.1.195:2380 \</div><div class="line">  --listen-client-urls=https://192.168.1.195:2379,http://127.0.0.1:2379 \</div><div class="line">  --advertise-client-urls=https://192.168.1.195:2379 \</div><div class="line">  --initial-cluster-token=etcd-cluster-0 \</div><div class="line">  --initial-cluster=etcd1=https://192.168.1.195:2380,etcd2=https://192.168.1.196:2380,etcd3=https://192.168.1.197:2380 \</div><div class="line">  --initial-cluster-state=new \</div><div class="line">  --data-dir=/var/lib/etcd</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line">LimitNOFILE=65536</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li>指定 etcd 的工作目录和数据目录为<code>/var/lib/etcd</code>，需在启动服务前创建这个目录</li>
<li><code>--name</code>为当前etcd的名称，<strong>每个etcd节点名字不能相同</strong></li>
<li><code>--initial-advertise-peer-urls</code>，<code>--listen-peer-urls</code>，<code>--listen-client-urls</code>，<code>--advertise-client-urls</code>这四个参数需要修改为<strong>当前节点的IP地址</strong>  </li>
<li><code>--initial-cluster</code>参数为，etcd集群所有节点的IP地址</li>
<li>为了保证通信安全，需要指定 etcd 的公私钥(<code>cert-file</code>和<code>key-file</code>)、Peers 通信的公私钥和 CA 证书(<code>peer-cert-file</code>、<code>peer-key-file</code>、<code>peer-trusted-ca-file</code>)、客户端的CA证书（<code>trusted-ca-file</code>）</li>
<li><code>--initial-cluster-state</code>值为 new 时，<code>--name</code> 的参数值必须位于 <code>--initial-cluster</code> 列表中</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># mv etcd.service /etc/systemd/system/</span></div></pre></td></tr></table></figure>
<p>分发etcd证书<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.196 "mkdir -p /var/lib/etcd"</span></div><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.196 "mkdir -p /etc/etcd/ssl"</span></div><div class="line">[root@master1 ~]<span class="comment"># scp /etc/etcd/ssl/etcd* root@192.168.1.196:/etc/etcd/ssl/</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.197 "mkdir -p /var/lib/etcd"</span></div><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.197 "mkdir -p /etc/etcd/ssl"</span></div><div class="line">[root@master1 ~]<span class="comment"># scp /etc/etcd/ssl/etcd* root@192.168.1.197:/etc/etcd/ssl/</span></div></pre></td></tr></table></figure>
<p>分发etcd.service<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># scp /etc/systemd/system/etcd.service root@192.168.1.196:/etc/systemd/system/</span></div><div class="line">[root@master1 ~]<span class="comment"># scp /etc/systemd/system/etcd.service root@192.168.1.197:/etc/systemd/system/</span></div></pre></td></tr></table></figure></p>
<p>启动 etcd 服务<br>依次启动所有节点的etcd服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl daemon-reload</div><div class="line">systemctl <span class="built_in">enable</span> etcd</div><div class="line">systemctl start etcd</div></pre></td></tr></table></figure></p>
<ul>
<li>最先启动的 etcd 进程会卡住一段时间，再等待其它节点上的 etcd 进程加入集群，这是正常现象</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># export ETCDCTL_API=3</span></div><div class="line">[root@master1 ~]<span class="comment"># etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.1.195:2379,https://192.168.1.196:2379,https://192.168.1.197:2379 endpoint health</span></div><div class="line">https://192.168.1.197:2379 is healthy: successfully committed proposal: took = 1.631081ms</div><div class="line">https://192.168.1.195:2379 is healthy: successfully committed proposal: took = 1.187637ms</div><div class="line">https://192.168.1.196:2379 is healthy: successfully committed proposal: took = 1.461928ms</div></pre></td></tr></table></figure>
<h4 id="部署Flannel-网络"><a href="#部署Flannel-网络" class="headerlink" title="部署Flannel 网络"></a>部署Flannel 网络</h4><p>Flannel是 CoreOS 团队针对 Kubernetes 设计的一个覆盖网络（Overlay Network）工具，其目的在于帮助每一个使用 Kuberentes 的 CoreOS 主机拥有一个完整的子网。简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址，因为在默认的Docker配置中，每个节点上的Docker服务会分别负责所在节点容器的IP分配。这样会导致一个问题，不同节点上容器可能获得相同的内外IP地址。Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。   </p>
<p>Flannel通过Etcd服务维护了一张节点间的路由表<br>参考：<a href="http://dockone.io/article/618" target="_blank" rel="external">http://dockone.io/article/618</a>  </p>
<p>flannel下载地址：<a href="https://github.com/coreos/flannel/releases" target="_blank" rel="external">https://github.com/coreos/flannel/releases</a><br><a href="https://note.youdao.com/yws/api/personal/file/5447ED8CA54D48EF86FB0668FDE54DFF?method=download&amp;shareKey=a9a38141e456fc88a678fdc1309a3dc9" target="_blank" rel="external">flannel-v0.10.0本地下载</a>  </p>
<table>
<thead>
<tr>
<th>节点名称</th>
<th>节点IP</th>
<th>分配地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>node1</td>
<td>192.168.1.198</td>
<td>172.30.57.0</td>
</tr>
<tr>
<td>node2</td>
<td>192.168.1.199</td>
<td>172.30.41.0</td>
</tr>
</tbody>
</table>
<p>在安装有cfssl工具的服务器上操作  </p>
<h5 id="创建-TLS-秘钥和证书-1"><a href="#创建-TLS-秘钥和证书-1" class="headerlink" title="创建 TLS 秘钥和证书"></a>创建 TLS 秘钥和证书</h5><p>etcd 集群启用了双向 TLS 认证，所以需要为 flanneld 指定与 etcd 集群通信的 CA 和秘钥  </p>
<p>创建 CA 配置文件<br>创建 flanneld 证书签名请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat flanneld-csr.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"CN"</span>: <span class="string">"flanneld"</span>,</div><div class="line">  <span class="string">"hosts"</span>: [],</div><div class="line">  <span class="string">"key"</span>: &#123;</div><div class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</div><div class="line">    <span class="string">"size"</span>: 2048</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"names"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</div><div class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</div><div class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li>hosts 字段为空  </li>
</ul>
<p>生成 flanneld 证书和私钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubernetes/ssl/ca-config.json -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld</span></div><div class="line">[root@master1 ssl]<span class="comment"># ls flanneld*</span></div><div class="line">flanneld.csr  flanneld-csr.json  flanneld-key.pem  flanneld.pem</div><div class="line">[root@master1 ssl]<span class="comment"># mkdir -p /etc/flanneld/ssl</span></div><div class="line">[root@master1 ssl]<span class="comment"># cp -a flanneld*.pem /etc/flanneld/ssl</span></div></pre></td></tr></table></figure></p>
<p>分发flanneld证书<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># ssh root@192.168.1.198 "mkdir -p /etc/flanneld/ssl"</span></div><div class="line">[root@master1 ssl]<span class="comment"># scp /etc/flanneld/ssl/flanneld* root@192.168.1.198:/etc/flanneld/ssl/</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># ssh root@192.168.1.199 "mkdir -p /etc/flanneld/ssl"</span></div><div class="line">[root@master1 ssl]<span class="comment"># scp /etc/flanneld/ssl/flanneld* root@192.168.1.199:/etc/flanneld/ssl/</span></div></pre></td></tr></table></figure>
<p>向 etcd 写入集群 Pod 网段信息<br>使用 etcd v2 API 写入配置<br>前面执行了<code>export ETCDCTL_API=3</code>，这个命令会调用<code>etcd v3 API</code>，所以需要重新打开一个xshell窗口，否则会报错<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># etcdctl --endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/flanneld/ssl/flanneld.pem --key-file=/etc/flanneld/ssl/flanneld-key.pem set /kubernetes/network/config '&#123;"Network":"'172.30.0.0/16'", "SubnetLen": 24, "Backend": &#123;"Type": "vxlan"&#125;&#125;'</span></div><div class="line">&#123;<span class="string">"Network"</span>:<span class="string">"172.30.0.0/16"</span>, <span class="string">"SubnetLen"</span>: 24, <span class="string">"Backend"</span>: &#123;<span class="string">"Type"</span>: <span class="string">"vxlan"</span>&#125;&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#查看POD网段信息</span></div><div class="line">[root@master1 ~]<span class="comment"># etcdctl --endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/flanneld/ssl/flanneld.pem --key-file=/etc/flanneld/ssl/flanneld-key.pem get /kubernetes/network/config</span></div><div class="line">&#123;<span class="string">"Network"</span>:<span class="string">"172.30.0.0/16"</span>, <span class="string">"SubnetLen"</span>: 24, <span class="string">"Backend"</span>: &#123;<span class="string">"Type"</span>: <span class="string">"vxlan"</span>&#125;&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>安装和配置 flanneld<br>在node1/node2节点安装，master不安装flannel<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz</span></div><div class="line">[root@node1 ~]<span class="comment"># mkdir flannel</span></div><div class="line">[root@node1 ~]<span class="comment"># tar -zxf flannel-v0.10.0-linux-amd64.tar.gz -C flannel</span></div><div class="line">[root@node1 ~]<span class="comment"># cp -a flannel/&#123;flanneld,mk-docker-opts.sh&#125; /usr/local/bin/</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># cat flanneld.service </span></div><div class="line">[Unit]</div><div class="line">Description=Flanneld overlay address etcd agent</div><div class="line">After=network.target</div><div class="line">After=network-online.target</div><div class="line">Wants=network-online.target</div><div class="line">After=etcd.service</div><div class="line">Before=docker.service</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/flanneld \</div><div class="line">  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  -etcd-certfile=/etc/flanneld/ssl/flanneld.pem \</div><div class="line">  -etcd-keyfile=/etc/flanneld/ssl/flanneld-key.pem \</div><div class="line">  -etcd-endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 \</div><div class="line">  -etcd-prefix=/kubernetes/network</div><div class="line">ExecStartPost=/usr/<span class="built_in">local</span>/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS <span class="_">-d</span> /run/flannel/docker</div><div class="line">Restart=on-failure</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">RequiredBy=docker.service</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li>mk-docker-opts.sh脚本将分配给 flanneld 的 Pod 子网网段信息写入到 /run/flannel/docker 文件中，后续 docker 启动时使用这个文件中参数值设置 docker0 网桥；</li>
<li>flanneld 使用系统缺省路由所在的接口和其它节点通信，对于有多个网络接口的机器（如，内网和公网），可以用 –iface 选项值指定通信接口(上面的 systemd unit 文件没指定这个选项)，如本着 Vagrant + Virtualbox，就要指定–iface=enp0s8；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># cp flanneld.service /etc/systemd/system/</span></div></pre></td></tr></table></figure>
<p>启动 flanneld<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl daemon-reload</div><div class="line">systemctl <span class="built_in">enable</span> flanneld</div><div class="line">systemctl start flanneld</div></pre></td></tr></table></figure></p>
<h6 id="docker集成flanneld网络"><a href="#docker集成flanneld网络" class="headerlink" title="docker集成flanneld网络"></a>docker集成flanneld网络</h6><p>让docker0网卡使用flanneld网络网段地址<br><a href="https://note.youdao.com/yws/api/personal/file/2137351870BB4393A12F4BEC6388DA44?method=download&amp;shareKey=c2e7067c0a4236332b684c8de6881018" target="_blank" rel="external">docker.service-yum</a>安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># grep -iv '#' /usr/lib/systemd/system/docker.service</span></div><div class="line">[Unit]</div><div class="line">Description=Docker Application Container Engine</div><div class="line">Documentation=https://docs.docker.com</div><div class="line">After=network-online.target firewalld.service</div><div class="line">Wants=network-online.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line"></div><div class="line"><span class="comment">#新增下面两个参数</span></div><div class="line">ExecStart=/usr/bin/dockerd <span class="variable">$DOCKER_NETWORK_OPTIONS</span></div><div class="line">EnvironmentFile=/run/flannel/docker</div><div class="line"></div><div class="line">ExecReload=/bin/<span class="built_in">kill</span> <span class="_">-s</span> HUP <span class="variable">$MAINPID</span></div><div class="line">LimitNOFILE=infinity</div><div class="line">LimitNPROC=infinity</div><div class="line">LimitCORE=infinity</div><div class="line">TimeoutStartSec=0</div><div class="line">Delegate=yes</div><div class="line">KillMode=process</div><div class="line">Restart=on-failure</div><div class="line">StartLimitBurst=3</div><div class="line">StartLimitInterval=60s</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># systemctl restart docker</span></div></pre></td></tr></table></figure>
<p>检查 flanneld 服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># ifconfig flannel.1</span></div><div class="line">flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</div><div class="line">        inet 172.30.57.0  netmask 255.255.255.255  broadcast 0.0.0.0</div><div class="line">        inet6 fe80::7890:e0ff:fe20:836e  prefixlen 64  scopeid 0x20&lt;link&gt;</div><div class="line">        ether 7a:90:e0:20:83:6e  txqueuelen 0  (Ethernet)</div><div class="line">        RX packets 0  bytes 0 (0.0 B)</div><div class="line">        RX errors 0  dropped 0  overruns 0  frame 0</div><div class="line">        TX packets 0  bytes 0 (0.0 B)</div><div class="line">        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0</div><div class="line"></div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li>其他节点安装flanneld同上</li>
</ul>
<p>检查分配给各 flanneld 的 Pod 网段信息<br>查看集群 Pod 网段(/16)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># etcdctl --endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/flanneld/ssl/flanneld.pem --key-file=/etc/flanneld/ssl/flanneld-key.pem get /kubernetes/network/config</span></div><div class="line">&#123;<span class="string">"Network"</span>:<span class="string">"172.30.0.0/16"</span>, <span class="string">"SubnetLen"</span>: 24, <span class="string">"Backend"</span>: &#123;<span class="string">"Type"</span>: <span class="string">"vxlan"</span>&#125;&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看已分配的 Pod 子网段列表(/24)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># etcdctl --endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/flanneld/ssl/flanneld.pem --key-file=/etc/flanneld/ssl/flanneld-key.pem ls /kubernetes/network/subnets</span></div><div class="line">/kubernetes/network/subnets/172.30.57.0-24</div></pre></td></tr></table></figure></p>
<p>查看某一 Pod 网段对应的 flanneld 进程监听的 IP 和网络参数<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># etcdctl --endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc</span></div><div class="line">/flanneld/ssl/flanneld.pem --key-file=/etc/flanneld/ssl/flanneld-key.pem get /kubernetes/network/subnets/172.30.57.0-24</div><div class="line">&#123;<span class="string">"PublicIP"</span>:<span class="string">"192.168.1.198"</span>,<span class="string">"BackendType"</span>:<span class="string">"vxlan"</span>,<span class="string">"BackendData"</span>:&#123;<span class="string">"VtepMAC"</span>:<span class="string">"7a:90:e0:20:83:6e"</span>&#125;&#125;</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>确保各节点间 Pod 网段能互联互通<br>在各节点上部署完 Flannel 后，查看已分配的 Pod 子网段列表(/24)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># etcdctl --endpoints=https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379 --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/flanneld/ssl/flanneld.pem --key-file=/etc/flanneld/ssl/flanneld-key.pem ls /kubernetes/network/subnets</span></div><div class="line">/kubernetes/network/subnets/172.30.41.0-24</div><div class="line">/kubernetes/network/subnets/172.30.57.0-24</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>当前两个节点分配的 Pod 网段分别是：172.30.42.0-24、172.30.100.0-24<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># ip a | egrep -i 'docker0|flannel.1'</span></div><div class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </div><div class="line">    inet 172.30.57.1/24 brd 172.30.57.255 scope global docker0</div><div class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </div><div class="line">    inet 172.30.57.0/32 scope global flannel.1</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@node2 ~]<span class="comment"># ip a | egrep -i 'docker0|flannel.1'</span></div><div class="line">3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </div><div class="line">    inet 172.30.42.0/32 scope global flannel.1</div><div class="line">4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN </div><div class="line">    inet 172.30.42.1/24 brd 172.30.41.255 scope global docker0</div><div class="line">[root@node2 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>在各节点上分配 ping 这两个网段的网关地址，确保能通<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># ping 172.30.57.0 -c 4</span></div><div class="line">[root@node1 ~]<span class="comment"># ping 172.30.42.0 -c 4</span></div></pre></td></tr></table></figure></p>
<h4 id="部署-k8s-master-节点"><a href="#部署-k8s-master-节点" class="headerlink" title="部署 k8s master 节点"></a>部署 k8s master 节点</h4><p>kubernetes master 节点包含的组件:  </p>
<ul>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kube-controller-manager</li>
</ul>
<p>这三个组件需要部署在同一台机器上  </p>
<ul>
<li><code>kube-scheduler</code>、<code>kube-controller-manager</code> 和 <code>kube-apiserver</code> 三者的功能紧密相关；</li>
<li>同时只能有一个 <code>kube-scheduler</code>、<code>kube-controller-manager</code> 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader；</li>
</ul>
<p>kubernetes发布版 tarball(下载脚本) 下载地址：<a href="https://github.com/kubernetes/kubernetes/releases" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/releases</a><br>kubernetes CHANGELOG(server/client) 下载地址：<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md</a>  </p>
<table>
<thead>
<tr>
<th>Service</th>
<th>通讯地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>kube-apiserver</td>
<td>192.168.1.195:6443</td>
</tr>
<tr>
<td>kube-controll</td>
<td>127.0.0.1:10252</td>
</tr>
<tr>
<td>kube-schedule</td>
<td>127.0.0.1:10251</td>
</tr>
</tbody>
</table>
<p><a href="https://pan.baidu.com/s/1sngFMlZ" target="_blank" rel="external">百度网盘packages目录，密码：nwzk</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://dl.k8s.io/v1.9.2/kubernetes-server-linux-amd64.tar.gz -O kubernetes-server-linux-amd64-v1.9.2.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># tar -zxf kubernetes-server-linux-amd64-v1.9.2.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># cd kubernetes</span></div><div class="line">[root@master1 kubernetes]<span class="comment"># tar -zxf kubernetes-src.tar.gz </span></div><div class="line">[root@master1 kubernetes]<span class="comment"># cp -r server/bin/&#123;kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet&#125; /usr/local/bin/</span></div></pre></td></tr></table></figure></p>
<h5 id="创建-kubernetes-证书"><a href="#创建-kubernetes-证书" class="headerlink" title="创建 kubernetes 证书"></a>创建 kubernetes 证书</h5><p>创建 kubernetes 证书签名请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cd ~/ssl/</span></div><div class="line">[root@master1 ssl]<span class="comment"># cat kubernetes-csr.json</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</div><div class="line">  <span class="string">"hosts"</span>: [</div><div class="line">    <span class="string">"127.0.0.1"</span>,</div><div class="line">    <span class="string">"192.168.1.195"</span>,</div><div class="line">    <span class="string">"172.16.0.1"</span>,</div><div class="line">    <span class="string">"kubernetes"</span>,</div><div class="line">    <span class="string">"kubernetes.default"</span>,</div><div class="line">    <span class="string">"kubernetes.default.svc"</span>,</div><div class="line">    <span class="string">"kubernetes.default.svc.cluster"</span>,</div><div class="line">    <span class="string">"kubernetes.default.svc.cluster.local"</span></div><div class="line">  ],</div><div class="line">  <span class="string">"key"</span>: &#123;</div><div class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</div><div class="line">    <span class="string">"size"</span>: 2048</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"names"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</div><div class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</div><div class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>192.168.1.195</code>为当前部署的 master 机器 IP  </li>
<li><code>172.16.0.1</code>kubernetes 服务 IP (预分配，一般是 SERVICE_CIDR 中第一个IP)  </li>
<li>如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，所以上面分别指定了当前部署的 master 节点主机 IP；  </li>
<li>还需要添加 kube-apiserver 注册的名为 kubernetes 的服务 IP (Service Cluster IP)，一般是 kube-apiserver –service-cluster-ip-range 选项值指定的网段的第一个IP，如 “172.16.0.1”；  </li>
</ul>
<p>生成 kubernetes 证书和私钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cd ssl/</span></div><div class="line">[root@master1 ssl]<span class="comment"># cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubernetes/ssl/ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span></div><div class="line">[root@master1 ssl]<span class="comment"># ls kubernetes*</span></div><div class="line">kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem</div><div class="line">[root@master1 ssl]<span class="comment"># mkdir -p /etc/kubernetes/ssl/</span></div><div class="line">[root@master1 ssl]<span class="comment"># mv kubernetes*.pem /etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure></p>
<h5 id="配置和启动-kube-apiserver"><a href="#配置和启动-kube-apiserver" class="headerlink" title="配置和启动 kube-apiserver"></a>配置和启动 kube-apiserver</h5><p>创建 kube-apiserver 使用的客户端 token 文件<br>kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token.csv 一致，如果一致则自动为 kubelet生成证书和秘钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#生成TLS Bootstrapping 使用的 Token</span></div><div class="line">[root@master1 ~]<span class="comment"># head -c 16 /dev/urandom | od -An -t x | tr -d ' '</span></div><div class="line">6240b18d950d086ff9eb596e215d243f</div><div class="line">[root@master1 ssl]<span class="comment"># cat token.csv </span></div><div class="line">6240b18d950d086ff9eb596e215d243f,kubelet-bootstrap,10001,<span class="string">"system:kubelet-bootstrap"</span></div><div class="line">[root@master1 ssl]<span class="comment"># mv token.csv /etc/kubernetes/</span></div><div class="line">[root@master1 ssl]<span class="comment"># cat basic-auth.csv </span></div><div class="line">admin,admin@123,1</div><div class="line"><span class="built_in">readonly</span>,<span class="built_in">readonly</span>,2</div><div class="line">[root@master1 ssl]<span class="comment"># mv basic-auth.csv /etc/kubernetes/</span></div></pre></td></tr></table></figure></p>
<p>创建 kube-apiserver 的 systemd unit 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat kube-apiserver.service </span></div><div class="line">[Unit]</div><div class="line">Description=Kubernetes API Server</div><div class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line">After=network.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/kube-apiserver \</div><div class="line">  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \</div><div class="line">  --advertise-address=192.168.1.195 \</div><div class="line">  --bind-address=192.168.1.195 \</div><div class="line">  --insecure-bind-address=192.168.1.195 \</div><div class="line">  --authorization-mode=RBAC \</div><div class="line">  --runtime-config=rbac.authorization.k8s.io/v1alpha1 \</div><div class="line">  --kubelet-https=<span class="literal">true</span> \</div><div class="line">  --token-auth-file=/etc/kubernetes/token.csv \</div><div class="line">  --service-cluster-ip-range=172.16.0.0/16 \</div><div class="line">  --service-node-port-range=8400-9000 \</div><div class="line">  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</div><div class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</div><div class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \</div><div class="line">  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \</div><div class="line">  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \</div><div class="line">  --etcd-servers=https://192.168.1.195:2379,https://192.168.1.196:2379,https://192.168.1.197:2379 \</div><div class="line">  --enable-swagger-ui=<span class="literal">true</span> \</div><div class="line">  --allow-privileged=<span class="literal">true</span> \</div><div class="line">  --apiserver-count=3 \</div><div class="line">  --audit-log-maxage=30 \</div><div class="line">  --audit-log-maxbackup=3 \</div><div class="line">  --audit-log-maxsize=100 \</div><div class="line">  --audit-log-path=/var/lib/audit.log \</div><div class="line">  --event-ttl=1h \</div><div class="line">  --v=2</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line">Type=notify</div><div class="line">LimitNOFILE=65536</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>--bind-address=192.168.1.195</code>为当前部署的 master 机器 IP，但不能为 <code>127.0.0.1</code>  </li>
<li><code>--insecure-bind-address=192.168.1.195</code>为当前部署的 master 机器 IP  </li>
<li><code>--service-cluster-ip-range</code> 指定 Service Cluster IP 地址段，该地址段不能路由可达  </li>
<li><code>--service-node-port-range=&quot;8400-9000&quot;</code> 指定 NodePort 的端口范围  </li>
<li><code>--etcd-servers=&quot;https://192.168.1.195:2379,https://192.168.1.195:2379,https://192.168.1.195:2379&quot;</code>为etcd 集群服务地址列表  </li>
<li>kube-apiserver 1.6 版本开始使用 etcd v3 API 和存储格式  </li>
<li><code>--authorization-mode=RBAC</code> 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求  </li>
<li>kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信  </li>
<li>kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的  </li>
<li>如果使用了 kubelet TLS Boostrap 机制，则不能再指定<code>--kubelet-certificate-authority</code>，<code>--kubelet-client-certificate</code>和<code>--kubelet-client-key</code>选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误  </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cp -a kube-apiserver.service /etc/systemd/system/</span></div></pre></td></tr></table></figure>
<p>启动kube-apiserver<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># systemctl daemon-reload</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl enable kube-apiserver</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl start kube-apiserver</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># netstat -tunlp |grep kube-apiserve</span></div><div class="line">tcp        0      0 192.168.1.195:6443      0.0.0.0:*               LISTEN      19206/kube-apiserve </div><div class="line">tcp        0      0 192.168.1.195:8080      0.0.0.0:*               LISTEN      19206/kube-apiserve </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#不能有任何报错</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl status kube-apiserver</span></div><div class="line">● kube-apiserver.service - Kubernetes API Server</div><div class="line">   Loaded: loaded (/etc/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Mon 2018-01-29 10:53:14 CST; 15min ago</div><div class="line">     Docs: https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"> Main PID: 19206 (kube-apiserver)</div><div class="line">   CGroup: /system.slice/kube-apiserver.service</div><div class="line">           └─19206 /usr/<span class="built_in">local</span>/bin/kube-apiserver --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota --advertise-address=...</div><div class="line"></div><div class="line">Jan 29 11:08:14 master1.example.com kube-apiserver[19206]: I0129 11:08:14.947346   19206 wrap.go:42] PUT /apis/apiregistration.k8s.io/v1beta1/apiservices/v1beta...95:42960]</div><div class="line">Jan 29 11:08:14 master1.example.com kube-apiserver[19206]: I0129 11:08:14.947470   19206 wrap.go:42] PUT /apis/apiregistration.k8s.io/v1beta1/apiservices/v1beta...95:42960]</div><div class="line">Jan 29 11:08:14 master1.example.com kube-apiserver[19206]: I0129 11:08:14.948643   19206 wrap.go:42] PUT /apis/apiregistration.k8s.io/v1beta1/apiservices/v1beta...95:42960]</div><div class="line">Jan 29 11:08:14 master1.example.com kube-apiserver[19206]: I0129 11:08:14.997905   19206 wrap.go:42] GET /api/v1/services: (1.191044ms) 200 [[kube-apiserver/v1....95:42960]</div><div class="line">Jan 29 11:08:15 master1.example.com kube-apiserver[19206]: I0129 11:08:15.003220   19206 wrap.go:42] GET /api/v1/services: (1.016741ms) 200 [[kube-apiserver/v1....95:42960]</div><div class="line">Jan 29 11:08:15 master1.example.com kube-apiserver[19206]: I0129 11:08:15.054109   19206 wrap.go:42] GET /api/v1/namespaces/kube-system: (1.388292ms) 200 [[kube...95:42960]</div><div class="line">Jan 29 11:08:15 master1.example.com kube-apiserver[19206]: I0129 11:08:15.055377   19206 wrap.go:42] GET /api/v1/namespaces/kube-public: (1.053295ms) 200 [[kube...95:42960]</div><div class="line">Jan 29 11:08:15 master1.example.com kube-apiserver[19206]: I0129 11:08:15.405477   19206 wrap.go:42] GET /api/v1/namespaces/default: (1.48537ms) 200 [[kube-apis...95:42960]</div><div class="line">Jan 29 11:08:15 master1.example.com kube-apiserver[19206]: I0129 11:08:15.407091   19206 wrap.go:42] GET /api/v1/namespaces/default/services/kubernetes: (1.0559...95:42960]</div><div class="line">Jan 29 11:08:15 master1.example.com kube-apiserver[19206]: I0129 11:08:15.408295   19206 wrap.go:42] GET /api/v1/namespaces/default/endpoints/kubernetes: (971.4….195:42960</div><div class="line">]Hint: Some lines were ellipsized, use <span class="_">-l</span> to show <span class="keyword">in</span> full.</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="配置和启动-kube-controller-manager"><a href="#配置和启动-kube-controller-manager" class="headerlink" title="配置和启动 kube-controller-manager"></a>配置和启动 kube-controller-manager</h5><p>创建 kube-controller-manager 的 systemd unit 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat kube-controller-manager.service</span></div><div class="line">[Unit]</div><div class="line">Description=Kubernetes Controller Manager</div><div class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"></div><div class="line">[Service]</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/kube-controller-manager \</div><div class="line">  --address=127.0.0.1 \</div><div class="line">  --master=http://192.168.1.195:8080 \</div><div class="line">  --allocate-node-cidrs=<span class="literal">true</span> \</div><div class="line">  --service-cluster-ip-range=172.16.0.0/16 \</div><div class="line">  --cluster-cidr=172.30.0.0/16 \</div><div class="line">  --cluster-name=kubernetes \</div><div class="line">  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \</div><div class="line">  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</div><div class="line">  --root-ca-file=/etc/kubernetes/ssl/ca.pem \</div><div class="line">  --leader-elect=<span class="literal">true</span> \</div><div class="line">  --v=2</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>--address</code>值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器</li>
<li><code>--master=http://192.168.1.195:8080</code>：使用非安全 8080 端口与 kube-apiserver 通信</li>
<li><code>--cluster-cidr</code> 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flanneld保证)</li>
<li><code>--service-cluster-ip-range</code> 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致</li>
<li><code>--cluster-signing-*</code> 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥</li>
<li><code>--root-ca-file</code> 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件</li>
<li><code>--leader-elect=true</code> 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cp kube-controller-manager.service /etc/systemd/system/</span></div></pre></td></tr></table></figure>
<p>启动 kube-controller-manager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># systemctl daemon-reload</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl enable kube-controller-manager</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl start kube-controller-manager</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># netstat -tunlp | grep -i kube-controll</span></div><div class="line">tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      19300/kube-controll </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># systemctl status kube-controller-manager</span></div><div class="line">● kube-controller-manager.service - Kubernetes Controller Manager</div><div class="line">   Loaded: loaded (/etc/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Thu 2018-02-01 14:57:01 CST; 41s ago</div><div class="line">     Docs: https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"> Main PID: 7798 (kube-controller)</div><div class="line">   CGroup: /system.slice/kube-controller-manager.service</div><div class="line">           └─7798 /usr/<span class="built_in">local</span>/bin/kube-controller-manager --address=127.0.0.1 --master=http://192.168.1.195:8080 --allocate-node-cidrs=<span class="literal">true</span> --service-cluster-ip-range=172...</div><div class="line"></div><div class="line">Jan 29 11:10:14 master1.example.com kube-controller-manager[7798]: I0201 14:57:12.832889    7798 controller_utils.go:1019] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> cidrall...ntroller</div><div class="line">Jan 29 11:10:14 master1.example.com kube-controller-manager[7798]: I0201 14:57:12.832920    7798 taint_controller.go:181] Starting NoExecuteTaintManager</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:12.932953    7798 controller_utils.go:1026] Caches are synced <span class="keyword">for</span> cidrallocator controller</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:13.549661    7798 resource_quota_controller.go:434] syncing resource quota control... &#123;apps v</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:13.549766    7798 controller_utils.go:1019] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> resourc...ntroller</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:13.649852    7798 controller_utils.go:1026] Caches are synced <span class="keyword">for</span> resource quota controller</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:13.696714    7798 garbagecollector.go:182] syncing garbage collector with updated ...onregist</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:15.046578    7798 controller_utils.go:1019] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> garbage...ntroller</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:15.146718    7798 controller_utils.go:1026] Caches are synced <span class="keyword">for</span> garbage collecto...ntroller</div><div class="line">Jan 29 11:10:15 master1.example.com kube-controller-manager[7798]: I0201 14:57:15.146731    7798 garbagecollector.go:219] synced garbage collector</div><div class="line">Hint: Some lines were ellipsized, use <span class="_">-l</span> to show <span class="keyword">in</span> full.</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="配置和启动-kube-scheduler"><a href="#配置和启动-kube-scheduler" class="headerlink" title="配置和启动 kube-scheduler"></a>配置和启动 kube-scheduler</h5><p>创建 kube-scheduler 的 systemd unit 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat kube-scheduler.service </span></div><div class="line">[Unit]</div><div class="line">Description=Kubernetes Scheduler</div><div class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"></div><div class="line">[Service]</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/kube-scheduler \</div><div class="line">  --address=127.0.0.1 \</div><div class="line">  --master=http://192.168.1.195:8080 \</div><div class="line">  --leader-elect=<span class="literal">true</span> \</div><div class="line">  --v=2</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li><code>--address</code> 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器</li>
<li><code>--master=http:/192.168.1.195:8080</code>：使用非安全 8080 端口与 kube-apiserver 通信</li>
<li><code>--leader-elect=true</code>部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cp -a kube-scheduler.service /etc/systemd/system/</span></div></pre></td></tr></table></figure>
<p>启动 kube-scheduler<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># systemctl daemon-reload</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl enable kube-scheduler</span></div><div class="line">[root@master1 ~]<span class="comment"># systemctl start kube-scheduler</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># systemctl status kube-scheduler</span></div><div class="line">● kube-scheduler.service - Kubernetes Scheduler</div><div class="line">   Loaded: loaded (/etc/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Mon 2018-01-29 11:27:07 CST; 2min 34s ago</div><div class="line">     Docs: https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"> Main PID: 19360 (kube-scheduler)</div><div class="line">   CGroup: /system.slice/kube-scheduler.service</div><div class="line">           └─19360 /usr/<span class="built_in">local</span>/bin/kube-scheduler --address=127.0.0.1 --master=http://192.168.1.195:8080 --leader-elect=<span class="literal">true</span> --v=2</div><div class="line"></div><div class="line">Jan 29 11:27:07 master1.example.com systemd[1]: Starting Kubernetes Scheduler...</div><div class="line">Jan 29 11:27:07 master1.example.com kube-scheduler[19360]: W0129 11:27:07.324795   19360 server.go:159] WARNING: all flags than --config are deprecated. Please ...ile ASAP.</div><div class="line">Jan 29 11:27:07 master1.example.com kube-scheduler[19360]: I0129 11:27:07.325298   19360 server.go:551] Version: v1.9.2</div><div class="line">Jan 29 11:27:07 master1.example.com kube-scheduler[19360]: I0129 11:27:07.325412   19360 factory.go:837] Creating scheduler from algorithm provider <span class="string">'DefaultProvider'</span></div><div class="line">Jan 29 11:27:07 master1.example.com kube-scheduler[19360]: I0129 11:27:07.325419   19360 factory.go:898] Creating scheduler with fit predicates <span class="string">'map[CheckNodeDi...&#123;&#125; NoDisk</span></div><div class="line">Jan 29 11:27:07 master1.example.com kube-scheduler[19360]: I0129 11:27:07.325564   19360 server.go:570] starting healthz server on 127.0.0.1:10251</div><div class="line">Jan 29 11:27:08 master1.example.com kube-scheduler[19360]: I0129 11:27:08.126353   19360 controller_utils.go:1019] Waiting for caches to sync for scheduler controller</div><div class="line">Jan 29 11:27:08 master1.example.com kube-scheduler[19360]: I0129 11:27:08.226450   19360 controller_utils.go:1026] Caches are synced for scheduler controller</div><div class="line">Jan 29 11:27:08 master1.example.com kube-scheduler[19360]: I0129 11:27:08.226468   19360 leaderelection.go:174] attempting to acquire leader lease...</div><div class="line">Jan 29 11:27:08 master1.example.com kube-scheduler[19360]: I0129 11:27:08.232415   19360 leaderelection.go:184] successfully acquired lease kube-system/kube-scheduler</div><div class="line">Hint: Some lines were ellipsized, use -l to show in full.</div><div class="line">[root@master1 ~]#</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># netstat -tunlp | grep -i kube-schedule</span></div><div class="line">tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      19360/kube-schedule </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="部署-kubectl-client节点"><a href="#部署-kubectl-client节点" class="headerlink" title="部署 kubectl client节点"></a>部署 kubectl client节点</h4><p>kubernetes发布版 tarball(下载脚本) 下载地址：<a href="https://github.com/kubernetes/kubernetes/releases" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/releases</a><br>kubernetes CHANGELOG(server/client) 下载地址：<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md" target="_blank" rel="external">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md</a><br><a href="https://pan.baidu.com/s/1sngFMlZ" target="_blank" rel="external">百度网盘packages目录，密码：nwzk</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://dl.k8s.io/v1.9.2/kubernetes-client-linux-amd64.tar.gz -O kubernetes-client-linux-amd64-v1.9.2.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># tar -zxf kubernetes-client-linux-amd64-v1.9.2.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># cp -a kubernetes/client/bin/kube* /usr/local/bin/</span></div></pre></td></tr></table></figure></p>
<h5 id="创建-admin-证书"><a href="#创建-admin-证书" class="headerlink" title="创建 admin 证书"></a>创建 admin 证书</h5><p>kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥  </p>
<p>创建 admin 证书签名请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat admin-csr.json</span></div><div class="line">&#123;</div><div class="line">  <span class="string">"CN"</span>: <span class="string">"admin"</span>,</div><div class="line">  <span class="string">"hosts"</span>: [],</div><div class="line">  <span class="string">"key"</span>: &#123;</div><div class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</div><div class="line">    <span class="string">"size"</span>: 2048</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"names"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</div><div class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"O"</span>: <span class="string">"system:masters"</span>,</div><div class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<ul>
<li><code>kube-apiserver</code> 使用 <code>RBAC</code> 对客户端(如 <code>kubelet</code>、<code>kube-proxy</code>、<code>Pod</code>)请求进行授权  </li>
<li><code>kube-apiserver</code> 预定义了一些 <code>RBAC</code> 使用的 <code>RoleBindings</code>，如 <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予了调用<code>kube-apiserver</code> <strong>所有 API</strong>的权限  </li>
<li>O 指定该证书的 Group 为 <code>system:masters</code>，<code>kubelet</code> 使用该证书访问 <code>kube-apiserver</code> 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 <code>system:masters</code>，所以被授予访问所有 API 的权限  </li>
<li>hosts 属性值为空列表  </li>
</ul>
<p>生成admin证书和私钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubernetes/ssl/ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin</span></div><div class="line">[root@master1 ssl]<span class="comment"># ls admin*</span></div><div class="line">admin.csr  admin-csr.json  admin-key.pem  admin.pem</div><div class="line">[root@master1 ssl]<span class="comment"># mv admin*.pem /etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure></p>
<p>创建 kubectl kubeconfig 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置集群参数</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=https://192.168.1.195:6443</span></div><div class="line"><span class="comment"># 设置客户端认证参数</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl config set-credentials admin --client-certificate=/etc/kubernetes/ssl/admin.pem --embed-certs=true --client-key=/etc/kubernetes/ssl/admin-key.pem</span></div><div class="line"><span class="comment"># 设置上下文参数</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl config set-context kubernetes --cluster=kubernetes --user=admin</span></div><div class="line"><span class="comment"># 设置默认上下文</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl config use-context kubernetes</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># ls ~/.kube/</span></div><div class="line">config</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li>设置集群参数和客户端认证参数时 <code>--embed-certs</code> 都为 <code>true</code>，这会将 <code>certificate-authority</code>、<code>client-certificate</code> 和 <code>client-key</code> 指向的证书文件内容写入到生成的 <code>kube-proxy.kubeconfig</code> 文件中  </li>
<li><code>kube-proxy.pem</code> 证书中 CN 为 <code>system:kube-proxy</code>，<code>kube-apiserver</code> 预定义的 RoleBinding <code>cluster-admin</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限  </li>
<li><code>admin.pem</code> 证书 O 字段值为 <code>system:masters</code>，<code>kube-apiserver</code> 预定义的 RoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予了调用<code>kube-apiserver</code> 相关 API 的权限  </li>
<li>生成的 kubeconfig 被保存到 <code>~/.kube/config</code> 文件  </li>
</ul>
<p>分发 kubeconfig 文件<br>将 <code>~/.kube/config</code> 文件拷贝到运行 kubelet 命令的机器的 <code>~/.kube/</code> 目录下</p>
<p>其他服务器部署kubectl-client工具<br>先在需要解压安装kubernetes-client-linux-amd64-v1.9.2.tar.gz<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.196 "mkdir -p /etc/kubernetes/ssl/"</span></div><div class="line">[root@master1 ~]<span class="comment"># scp /etc/kubernetes/ssl/admin* root@192.168.1.196:/etc/kubernetes/ssl/</span></div><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.196 "mkdir -p ~/.kube/"</span></div><div class="line">[root@master1 ~]<span class="comment"># scp ~/.kube/config root@192.168.1.196:~/.kube/</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@master2 ~]<span class="comment"># kubectl get componentstatuses</span></div><div class="line">NAME                 STATUS    MESSAGE              ERROR</div><div class="line">controller-manager   Healthy   ok                   </div><div class="line">scheduler            Healthy   ok                   </div><div class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">[root@master2 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>如果遇到下面的错误，需要检查kubectl是否可以与api-server通讯<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># kubectl get componentstatuses</span></div><div class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</div></pre></td></tr></table></figure></p>
<h6 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h6><p>查看k8s 查看各组件信息<br>需要先安装kubectl命令(kubernetes-client)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get componentstatuses</span></div><div class="line">NAME                 STATUS    MESSAGE              ERROR</div><div class="line">controller-manager   Healthy   ok                   </div><div class="line">scheduler            Healthy   ok                   </div><div class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>或<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl -s http://192.168.1.195:8080 get componentstatuses</span></div><div class="line">NAME                 STATUS    MESSAGE              ERROR</div><div class="line">scheduler            Healthy   ok                   </div><div class="line">controller-manager   Healthy   ok                   </div><div class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;   </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看k8s svc地址(服务虚拟IP)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc kubernetes</span></div><div class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</div><div class="line">kubernetes   ClusterIP   172.16.0.1   &lt;none&gt;        443/TCP   1d</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>或<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get service</span></div><div class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</div><div class="line">kubernetes   ClusterIP   172.16.0.1   &lt;none&gt;        443/TCP   1d</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>查看集群信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl cluster-info</span></div><div class="line">Kubernetes master is running at https://192.168.1.195:6443</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h4 id="部署-K8S-Node-节点"><a href="#部署-K8S-Node-节点" class="headerlink" title="部署 K8S Node 节点"></a>部署 K8S Node 节点</h4><p>kubernetes Node 节点包含如下组件：  </p>
<ul>
<li>flanneld</li>
<li>docker</li>
<li>kubelet</li>
<li>kube-proxy</li>
</ul>
<h5 id="安装和配置-kubelet"><a href="#安装和配置-kubelet" class="headerlink" title="安装和配置 kubelet"></a>安装和配置 kubelet</h5><p>kubelet 启动时向 <code>kube-apiserver</code> 发送 <code>TLS bootstrapping</code> 请求，需要先将 <code>bootstrap token</code> 文件中的 <code>kubelet-bootstrap</code> 用户赋予 <code>system:node-bootstrapper</code> 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)  </p>
<p><strong>kubelet 和kube-proxy 在node1/node2上部署</strong><br><a href="https://pan.baidu.com/s/1sngFMlZ" target="_blank" rel="external">百度网盘packages目录，密码：nwzk</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># wget https://dl.k8s.io/v1.9.2/kubernetes-server-linux-amd64.tar.gz -O kubernetes-server-linux-amd64-v1.9.2.tar.gz</span></div><div class="line">[root@node1 ~]<span class="comment"># tar -zxf kubernetes-server-linux-amd64-v1.9.2.tar.gz</span></div><div class="line">[root@node1 ~]<span class="comment"># cp -a kubernetes/server/bin/&#123;kube-proxy,kubelet&#125; /usr/local/bin/</span></div></pre></td></tr></table></figure></p>
<p>把在安装有cfssl 工具的服务器(这里是在master1)上生成的admin相关证书和key拷贝到node1上<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.198 "mkdir -p /etc/kubernetes/ssl/"</span></div><div class="line">[root@master1 ~]<span class="comment"># scp /etc/kubernetes/ssl/admin* root@192.168.1.198:/etc/kubernetes/ssl/</span></div><div class="line">[root@master1 ~]<span class="comment"># ssh root@192.168.1.198 "mkdir -p ~/.kube/"</span></div><div class="line">[root@master1 ~]<span class="comment"># scp ~/.kube/config root@192.168.1.198:~/.kube/</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#如果kubectl命令没有，需要安装kubernetes-client-linux-amd64-v1.9.2.tar.gz</span></div><div class="line"><span class="comment">#如果有多个节点，这条命令只执行一次</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap</span></div></pre></td></tr></table></figure>
<ul>
<li><code>--user=kubelet-bootstrap</code> 是master服务器文件 <code>/etc/kubernetes/token.csv</code> 中指定的用户名，同时也写入了文件 <code>/etc/kubernetes/bootstrap.kubeconfig</code> </li>
</ul>
<p>如果报下面错误，需要先安装kubectl工具，参考上面操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@node2 ~]<span class="comment"># kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap</span></div><div class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</div><div class="line">[root@node2 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>创建 kubelet bootstrapping kubeconfig 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置集群参数</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=https://192.168.1.195:6443 --kubeconfig=bootstrap.kubeconfig</span></div><div class="line"><span class="comment"># 设置客户端认证参数</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config set-credentials kubelet-bootstrap --token=6240b18d950d086ff9eb596e215d243f --kubeconfig=bootstrap.kubeconfig</span></div><div class="line"><span class="comment"># 设置上下文参数</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config set-context default --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=bootstrap.kubeconfig</span></div><div class="line"><span class="comment"># 设置默认上下文</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span></div><div class="line">[root@node1 ~]<span class="comment"># ls bootstrap.kubeconfig </span></div><div class="line">bootstrap.kubeconfig</div><div class="line">[root@node1 ~]<span class="comment"># mv bootstrap.kubeconfig /etc/kubernetes/</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># mkdir -p /var/lib/kubelet</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># cat kubelet.service </span></div><div class="line">[Unit]</div><div class="line">Description=Kubernetes Kubelet</div><div class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line">After=docker.service</div><div class="line">Requires=docker.service</div><div class="line"></div><div class="line">[Service]</div><div class="line">WorkingDirectory=/var/lib/kubelet</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/kubelet \</div><div class="line">  --address=192.168.1.198 \</div><div class="line">  --hostname-override=192.168.1.198 \</div><div class="line">  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \</div><div class="line">  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</div><div class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</div><div class="line">  --require-kubeconfig \</div><div class="line">  --cert-dir=/etc/kubernetes/ssl \</div><div class="line">  --cluster-dns=223.5.5.5,223.6.6.6,8.8.8.8 \</div><div class="line">  --cluster-domain=aliyun.com. \</div><div class="line">  --hairpin-mode promiscuous-bridge \</div><div class="line">  --allow-privileged=<span class="literal">true</span> \</div><div class="line">  --serialize-image-pulls=<span class="literal">false</span> \</div><div class="line">  --logtostderr=<span class="literal">true</span> \</div><div class="line">  --v=2</div><div class="line"><span class="comment">#kubelet cAdvisor 默认在所有接口监听 4194 端口的请求, 以下iptables限制内网访问</span></div><div class="line">ExecStartPost=/sbin/iptables -A INPUT <span class="_">-s</span> 172.30.0.0/16 -p tcp --dport 4194 -j ACCEPT</div><div class="line">ExecStartPost=/sbin/iptables -A INPUT <span class="_">-s</span> 172.16.0.0/16 -p tcp --dport 4194 -j ACCEPT</div><div class="line">ExecStartPost=/sbin/iptables -A INPUT <span class="_">-s</span> 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT</div><div class="line">ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li><code>--address</code> 不能设置为 <code>127.0.0.1</code>，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet。<code>--address</code>设置为当前部署的节点 IP  </li>
<li>如果设置了 <code>--hostname-override</code> 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况。<code>--hostname-override</code>设置为当前部署的节点 IP  </li>
<li><code>--cluster-dns</code>设置为集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)，此环境中使用的 SERVICE_CIDR为172.16.0.0/16，可在master的kube-apiserver配置文件中看到  </li>
<li><code>--cluster-domain</code>设置为集群 DNS 域名  </li>
<li><code>--experimental-bootstrap-kubeconfig</code> 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求  </li>
<li>管理员通过了 CSR 请求后，kubelet 自动在 <code>--cert-dir</code> 目录创建证书和私钥文件(<code>kubelet-client.crt</code> 和 <code>kubelet-client.key</code>)，然后写入 <code>--kubeconfig</code> 文件(自动创建 <code>--kubeconfig</code> 指定的文件)  </li>
<li>建议在 <code>--kubeconfig</code> 配置文件中指定 <code>kube-apiserver</code> 地址，如果未指定 <code>--api-servers</code> 选项，则必须指定 <code>--require-kubeconfig</code> 选项后才从配置文件中读取 kue-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），<code>kubectl get nodes</code> 不会返回对应的 Node 信息  </li>
<li><code>--cluster-dns</code> 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，<code>--cluster-domain</code> 指定域名后缀，这两个参数同时指定后才会生效  </li>
<li>kubelet cAdvisor 默认在<strong>所有接口</strong>监听 4194 端口的请求，对于有外网的机器来说不安全，<code>ExecStartPost</code> 选项指定的 iptables 规则只允许内网机器访问 4194 端口  </li>
<li><code>--cluster-dns</code>：集群DNS服务IP(从 SERVICE_CIDR 中预分配)。经测试，如果k8s内部没有搭建DNS，建议使用外部公共DNS地址。如果该DNS不存在，将会影响业务对域名的解析  </li>
<li><code>--cluster-domain</code>：集群 DNS 域名。经测试，此DNS域名后缀必须为真实域名后缀，k8s会写入到POD的/etc/resolv.conf文件中。如果使用默认的DNS后缀(cluster.local.)，因该DNS后缀不存在，故在解析DNS时会超时，会严重影响业务</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># mv kubelet.service /etc/systemd/system/kubelet.service</span></div></pre></td></tr></table></figure>
<p>启动 kubelet<br>v1.8版后，需要手动绑定 system:nodes组到 system:node的clusterrole<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#kubelet-node-clusterbinding为名称，可自定义。如果有多个节点加入，名称不能相同</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl create clusterrolebinding kubelet-node-clusterbinding --clusterrole=system:node --user=system:node:192.168.1.198</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#启动kubelet需要关闭swap分区  </span></div><div class="line">[root@node1 ~]<span class="comment"># swapoff -a</span></div><div class="line">[root@node1 ~]<span class="comment"># systemctl daemon-reload</span></div><div class="line">[root@node1 ~]<span class="comment"># systemctl enable kubelet</span></div><div class="line">[root@node1 ~]<span class="comment"># systemctl start kubelet</span></div></pre></td></tr></table></figure>
<p>需要关闭swap分区，否则会报下面的错误<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Feb  1 16:56:59 k8s-4 kubelet: error: failed to run Kubelet: Running with swap on is not supported, please <span class="built_in">disable</span> swap! or <span class="built_in">set</span> --fail-swap-on flag to false. /proc/swaps contained: [Filename<span class="comment">#011#011#011#011Type#011#011Size#011Used#011Priority /dev/dm-1 partition#0112097148#01116#011-1]</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># systemctl status kubelet</span></div><div class="line">● kubelet.service - Kubernetes Kubelet</div><div class="line">   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Thu 2018-02-01 16:41:49 CST; 7min ago</div><div class="line">     Docs: https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line">  Process: 30456 ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP (code=exited, status=0/SUCCESS)</div><div class="line">  Process: 30452 ExecStartPost=/sbin/iptables -A INPUT <span class="_">-s</span> 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT (code=exited, status=0/SUCCESS)</div><div class="line">  Process: 30451 ExecStartPost=/sbin/iptables -A INPUT <span class="_">-s</span> 172.16.0.0/16 -p tcp --dport 4194 -j ACCEPT (code=exited, status=0/SUCCESS)</div><div class="line">  Process: 30446 ExecStartPost=/sbin/iptables -A INPUT <span class="_">-s</span> 172.30.0.0/16 -p tcp --dport 4194 -j ACCEPT (code=exited, status=0/SUCCESS)</div><div class="line"> Main PID: 30445 (kubelet)</div><div class="line">   Memory: 11.7M</div><div class="line">   CGroup: /system.slice/kubelet.service</div><div class="line">           └─30445 /usr/<span class="built_in">local</span>/bin/kubelet --address=192.168.1.198 --hostname-override=192.168.1.198 --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infr...</div><div class="line"></div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.807803   30445 controller.go:114] kubelet config controller: starting controller</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.807806   30445 controller.go:118] kubelet config controller: validating combination of defaults and flags</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.812805   30445 mount_linux.go:202] Detected OS with systemd</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: W0201 16:41:49.812875   30445 cni.go:171] Unable to update cni config: No networks found <span class="keyword">in</span> /etc/cni/net.d</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.815926   30445 server.go:182] Version: v1.9.2</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.815950   30445 feature_gate.go:220] feature gates: &amp;&#123;&#123;&#125; map[]&#125;</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: W0201 16:41:49.816011   30445 server.go:280] --require-kubeconfig is deprecated. Set --kubeconfig without usi...ubeconfig.</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.816019   30445 plugins.go:101] No cloud provider specified.</div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.816025   30445 server.go:303] No cloud provider specified: <span class="string">""</span> from the config file: <span class="string">""</span></div><div class="line">Feb 01 16:41:49 node1.example.com kubelet[30445]: I0201 16:41:49.816036   30445 bootstrap.go:58] Using bootstrap kubeconfig to generate TLS client cert, key an...onfig file</div><div class="line">Hint: Some lines were ellipsized, use <span class="_">-l</span> to show <span class="keyword">in</span> full.</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h6 id="Q-amp-A-master日志报RBAC-DENY-user-“kubelet-bootstrap”-groups错误"><a href="#Q-amp-A-master日志报RBAC-DENY-user-“kubelet-bootstrap”-groups错误" class="headerlink" title="Q&amp;A master日志报RBAC DENY: user “kubelet-bootstrap” groups错误"></a>Q&amp;A master日志报RBAC DENY: user “kubelet-bootstrap” groups错误</h6><p>启动kubelet服务后，需要看master的kube-apiserver日志(/var/log/messages)中没有如下报错，才算启动成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Feb  1 03:41:53 master1 kube-apiserver: I0201 16:41:53.245112    8202 rbac.go:116] RBAC DENY: user <span class="string">"kubelet-bootstrap"</span> groups [<span class="string">"system:kubelet-bootstrap"</span> <span class="string">"system:authentica</span></div><div class="line">ted"] cannot <span class="string">"create"</span> resource <span class="string">"certificatesigningrequests.certificates.k8s.io/nodeclient"</span> cluster-wide</div></pre></td></tr></table></figure></p>
<p>如果在启动node时，master的/var/log/messages日志报上面的错误，原因如下：<br>1.8版本之前，开启<code>rbac</code>后，apiserver默认绑定<code>system:nodes</code>组到<code>system:node</code>的clusterrole。但v1.8之后，此绑定默认不存在，<strong>需要手工绑定</strong>，否则kubelet启动后会报认证错误，使用kubectl get nodes查看无法成为Ready状态。  </p>
<p>默认角色与默认角色绑定<br>API Server会创建一组默认的ClusterRole和<code>ClusterRoleBinding</code>对象。这些默认对象中有许多包含<code>system:前缀</code>，表明这些资源由Kubernetes基础组件“拥有”。 对这些资源的修改可能导致非功能性集群(<code>non-functional cluster</code>)<br>这个角色定义了kubelets的权限。如果这个角色被修改，可能会导致kubelets无法正常工作。<br>所有默认的<code>ClusterRole</code>和<code>ClusterRoleBinding</code>对象都会被标记为<code>kubernetes.io/bootstrapping=rbac-defaults</code>  </p>
<p><code>kubectl get clusterrolebinding</code>和<code>kubectl get clusterrole</code>可以查看系统中的角色与角色绑定<br><code>kubectl get clusterrolebindings system:node -o yaml</code>或<code>kubectl describe clusterrolebindings system:node</code>查看<code>system:node</code>角色绑定的详细信息  </p>
<p>查看 system:node 角色绑定的详细信息<br>system:node角色默认绑定为空<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl describe clusterrolebindings system:node</span></div><div class="line">Name:         system:node</div><div class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</div><div class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=<span class="literal">true</span></div><div class="line">Role:</div><div class="line">  Kind:  ClusterRole</div><div class="line">  Name:  system:node</div><div class="line">Subjects:</div><div class="line">  Kind  Name  Namespace</div><div class="line">  ----  ----  ---------</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>在整个集群范围内将 system:node ClusterRole 授予用户<code>system:node:192.168.1.198</code>或组<code>system:nodes</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl describe clusterrolebindings kubelet-node-clusterbinding  </span></div><div class="line">Name:         kubelet-node-clusterbinding</div><div class="line">Labels:       &lt;none&gt;</div><div class="line">Annotations:  &lt;none&gt;</div><div class="line">Role:</div><div class="line">  Kind:  ClusterRole</div><div class="line">  Name:  system:node</div><div class="line">Subjects:</div><div class="line">  Kind  Name                       Namespace</div><div class="line">  ----  ----                       ---------</div><div class="line">  User  system:node:192.168.1.198  </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h5 id="通过-kubelet-的-TLS-证书请求"><a href="#通过-kubelet-的-TLS-证书请求" class="headerlink" title="通过 kubelet 的 TLS 证书请求"></a>通过 kubelet 的 TLS 证书请求</h5><p>kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群  </p>
<p>查看未授权的 CSR 请求:<br>下面如果没有获取到也代表kubelet没有启动成功<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get csr</span></div><div class="line">NAME                                                   AGE       REQUESTOR           CONDITION</div><div class="line">node-csr-DeFIxWS7IZimyAUaZGtIh8q4sp_CiNHL2bO1cwEm26U   9m        kubelet-bootstrap   Pending</div><div class="line">[root@master1 ~]<span class="comment"># </span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl get nodes</span></div><div class="line">No resources found.</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>通过 CSR 请求:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl certificate approve node-csr-DeFIxWS7IZimyAUaZGtIh8q4sp_CiNHL2bO1cwEm26U</span></div><div class="line">certificatesigningrequest <span class="string">"node-csr-DeFIxWS7IZimyAUaZGtIh8q4sp_CiNHL2bO1cwEm26U"</span> approved</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get csr</span></div><div class="line">NAME                                                   AGE       REQUESTOR           CONDITION</div><div class="line">node-csr-DeFIxWS7IZimyAUaZGtIh8q4sp_CiNHL2bO1cwEm26U   13m       kubelet-bootstrap   Approved,Issued</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>自动生成了 kubelet kubeconfig 文件和公私钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># ls -l /etc/kubernetes/kubelet.kubeconfig</span></div><div class="line">-rw------- 1 root root 2280 Jan 31 19:19 /etc/kubernetes/kubelet.kubeconfig</div><div class="line">[root@node1 ~]<span class="comment"># ls -l /etc/kubernetes/ssl/kubelet*</span></div><div class="line">-rw-r--r-- 1 root root 1046 Jan 31 19:19 /etc/kubernetes/ssl/kubelet-client.crt</div><div class="line">-rw------- 1 root root  227 Jan 31 19:18 /etc/kubernetes/ssl/kubelet-client.key</div><div class="line">-rw-r--r-- 1 root root 1115 Jan 31 19:15 /etc/kubernetes/ssl/kubelet.crt</div><div class="line">-rw------- 1 root root 1675 Jan 31 19:15 /etc/kubernetes/ssl/kubelet.key</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>如果节点状态变为Ready才算成功，查看日志都已经正常<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get nodes</span></div><div class="line">NAME            STATUS    ROLES     AGE       VERSION</div><div class="line">192.168.1.198   Ready     &lt;none&gt;    25m       v1.9.2</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>参考：<a href="http://blog.csdn.net/zhaihaifei/article/details/79098564" target="_blank" rel="external">http://blog.csdn.net/zhaihaifei/article/details/79098564</a>  </p>
<h6 id="多个Node节点加入"><a href="#多个Node节点加入" class="headerlink" title="多个Node节点加入"></a>多个Node节点加入</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl describe clusterrolebindings kubelet-node199-clusterbinding  </span></div><div class="line">Name:         kubelet-node199-clusterbinding</div><div class="line">Labels:       &lt;none&gt;</div><div class="line">Annotations:  &lt;none&gt;</div><div class="line">Role:</div><div class="line">  Kind:  ClusterRole</div><div class="line">  Name:  system:node</div><div class="line">Subjects:</div><div class="line">  Kind  Name                       Namespace</div><div class="line">  ----  ----                       ---------</div><div class="line">  User  system:node:192.168.1.199  </div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get nodes</span></div><div class="line">NAME            STATUS    ROLES     AGE       VERSION</div><div class="line">192.168.1.198   Ready     &lt;none&gt;    3h        v1.9.2</div><div class="line">192.168.1.199   Ready     &lt;none&gt;    2m        v1.9.2</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="配置-kube-proxy"><a href="#配置-kube-proxy" class="headerlink" title="配置 kube-proxy"></a>配置 kube-proxy</h5><p>创建 kube-proxy 证书签名请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cat kube-proxy-csr.json </span></div><div class="line">&#123;</div><div class="line">  <span class="string">"CN"</span>: <span class="string">"system:kube-proxy"</span>,</div><div class="line">  <span class="string">"hosts"</span>: [],</div><div class="line">  <span class="string">"key"</span>: &#123;</div><div class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</div><div class="line">    <span class="string">"size"</span>: 2048</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"names"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</div><div class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</div><div class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</div><div class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li>CN 指定该证书的 User 为 <code>system:kube-proxy</code>；</li>
<li>kube-apiserver 预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li>
<li>hosts 属性值为空列表</li>
</ul>
<p>生成 kube-proxy 客户端证书和私钥<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem -ca-key=/etc/kubernetes/ssl/ca-key.pem -config=/etc/kubernetes/ssl/ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy</span></div><div class="line">[root@master1 ssl]<span class="comment"># ls kube-proxy*</span></div><div class="line">kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem</div><div class="line">[root@master1 ssl]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>把在安装有cfssl 工具的服务器(这里是在master1)上生成的kube-proxy 相关证书和key拷贝到node1上<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ssl]<span class="comment"># scp kube-proxy*.pem root@192.168.1.198:/etc/kubernetes/ssl/</span></div></pre></td></tr></table></figure></p>
<p>创建 kube-proxy kubeconfig 文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设置集群参数</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=https://192.168.1.195:6443 --kubeconfig=kube-proxy.kubeconfig</span></div><div class="line"><span class="comment"># 设置客户端认证参数</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config set-credentials kube-proxy --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig</span></div><div class="line"><span class="comment">#设置上下文参数</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig</span></div><div class="line"><span class="comment">#设置默认上下文</span></div><div class="line">[root@node1 ~]<span class="comment"># kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># mv kube-proxy.kubeconfig /etc/kubernetes/</span></div></pre></td></tr></table></figure>
<ul>
<li>设置集群参数和客户端认证参数时 <code>--embed-certs</code> 都为 <code>true</code>，这会将 <code>certificate-authority</code>、<code>client-certificate</code> 和 <code>client-key</code> 指向的证书文件内容写入到生成的 <code>kube-proxy.kubeconfig</code> 文件中</li>
<li><code>kube-proxy.pem</code> 证书中 CN 为 <code>system:kube-proxy</code>，<code>kube-apiserver</code> 预定义的 RoleBinding <code>cluster-admin</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver Proxy</code> 相关 API 的权限</li>
</ul>
<p>创建 kube-proxy 的 systemd unit 文件<br>创建工作目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># mkdir -p /var/lib/kube-proxy</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># cat kube-proxy.service </span></div><div class="line">[Unit]</div><div class="line">Description=Kubernetes Kube-Proxy Server</div><div class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line">After=network.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">WorkingDirectory=/var/lib/kube-proxy</div><div class="line">ExecStart=/usr/<span class="built_in">local</span>/bin/kube-proxy \</div><div class="line">  --bind-address=192.168.1.198 \</div><div class="line">  --hostname-override=192.168.1.198 \</div><div class="line">  --cluster-cidr=172.30.0.0/16 \</div><div class="line">  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</div><div class="line">  --logtostderr=<span class="literal">true</span> \</div><div class="line">  --v=2</div><div class="line">Restart=on-failure</div><div class="line">RestartSec=5</div><div class="line">LimitNOFILE=65536</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<ul>
<li><code>--hostname-override</code> 参数值<strong>必须</strong>与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则  </li>
<li><code>--cluster-cidr</code> <strong>必须</strong>与 kube-controller-manager 的 <code>--cluster-cidr</code> 选项值一致，172.30.0.0/16  </li>
<li>kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT  </li>
<li><code>--kubeconfig</code> 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息  </li>
<li>预定义的 RoleBinding <code>cluster-admin</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限</li>
</ul>
<p>启动 kube-proxy<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># cp kube-proxy.service /etc/systemd/system/</span></div><div class="line">[root@node1 ~]<span class="comment"># systemctl daemon-reload</span></div><div class="line">[root@node1 ~]<span class="comment"># systemctl enable kube-proxy</span></div><div class="line">[root@node1 ~]<span class="comment"># systemctl start kube-proxy</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># systemctl status kube-proxy</span></div><div class="line">● kube-proxy.service - Kubernetes Kube-Proxy Server</div><div class="line">   Loaded: loaded (/etc/systemd/system/kube-proxy.service; enabled; vendor preset: disabled)</div><div class="line">   Active: active (running) since Fri 2018-02-02 14:24:10 CST; 15s ago</div><div class="line">     Docs: https://github.com/GoogleCloudPlatform/kubernetes</div><div class="line"> Main PID: 3660 (kube-proxy)</div><div class="line">   Memory: 8.7M</div><div class="line">   CGroup: /system.slice/kube-proxy.service</div><div class="line">           ‣ 3660 /usr/<span class="built_in">local</span>/bin/kube-proxy --bind-address=192.168.1.198 --hostname-override=192.168.1.198 --cluster-cidr=172.30.0.0/16 --kubeconfig=/etc/kubernetes/kube...</div><div class="line"></div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.223982    3660 conntrack.go:98] Set sysctl <span class="string">'net/netfilter/nf_conntrack_tcp_timeout_established'</span> to 86400</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.224981    3660 conntrack.go:98] Set sysctl <span class="string">'net/netfilter/nf_conntrack_tcp_timeout_close_wait'</span> to 3600</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.225415    3660 config.go:202] Starting service config controller</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.225427    3660 controller_utils.go:1019] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> service config controller</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.225514    3660 config.go:102] Starting endpoints config controller</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.225523    3660 controller_utils.go:1019] Waiting <span class="keyword">for</span> caches to sync <span class="keyword">for</span> endpoints config controller</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.325575    3660 controller_utils.go:1026] Caches are synced <span class="keyword">for</span> service config controller</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.325624    3660 proxier.go:984] Not syncing iptables until Services and Endpoints have been re...om master</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.326087    3660 controller_utils.go:1026] Caches are synced <span class="keyword">for</span> endpoints config controller</div><div class="line">Feb 02 14:24:10 node1.example.com kube-proxy[3660]: I0202 14:24:10.326163    3660 proxier.go:329] Adding new service port <span class="string">"default/kubernetes:https"</span> at 172.16.0.1:443/TCP</div><div class="line">Hint: Some lines were ellipsized, use <span class="_">-l</span> to show <span class="keyword">in</span> full.</div><div class="line">[root@node1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h5 id="验证集群功能"><a href="#验证集群功能" class="headerlink" title="验证集群功能"></a>验证集群功能</h5><p>定义文件:<br><a href="https://note.youdao.com/yws/api/personal/file/DBEF82CCEFC040ACAE8F919C9E381633?method=download&amp;shareKey=8f85df5224bf4e4a7480fd6dd60fd5d6" target="_blank" rel="external">nginx-ds.yml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># cat nginx-ds.yml </span></div><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: nginx-ds</div><div class="line">  labels:</div><div class="line">    app: nginx-ds</div><div class="line">spec:</div><div class="line">  <span class="built_in">type</span>: NodePort</div><div class="line">  selector:</div><div class="line">    app: nginx-ds</div><div class="line">  ports:</div><div class="line">  - name: http</div><div class="line">    port: 80</div><div class="line">    targetPort: 80</div><div class="line"></div><div class="line">---</div><div class="line"></div><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: DaemonSet</div><div class="line">metadata:</div><div class="line">  name: nginx-ds</div><div class="line">  labels:</div><div class="line">    addonmanager.kubernetes.io/mode: Reconcile</div><div class="line">spec:</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: nginx-ds</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: my-nginx</div><div class="line">        image: nginx:1.7.9</div><div class="line">        ports:</div><div class="line">        - containerPort: 80</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f nginx-ds.yml</span></div></pre></td></tr></table></figure>
<p>检查各 Node 上的 Pod IP 连通性<br>当前k8s上有两个Node，在nginx-ds.yml中使用的是<code>DaemonSet</code>模式，所以会在每个Node上启动这个Pod<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#DaemonSet官方解释</span></div><div class="line">A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.</div></pre></td></tr></table></figure></p>
<p>参考：<a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#what-is-a-daemonset" target="_blank" rel="external">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#what-is-a-daemonset</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#两个nginx容器的IP地址不同</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods  -o wide</span></div><div class="line">NAME             READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-ds-9p9pl   1/1       Running   0          4s        172.30.57.2   192.168.1.198</div><div class="line">nginx-ds-tvp6b   1/1       Running   0          4s        172.30.41.2   192.168.1.199</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>检查服务 IP 和端口可达性<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc |grep nginx-ds</span></div><div class="line">nginx-ds     NodePort    172.16.117.40   &lt;none&gt;        80:8446/TCP   5m</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li>服务IP：172.16.117.40</li>
<li>服务端口：80</li>
<li>NodePort端口：8446</li>
</ul>
<p>在所有 Node 上执行，IP为nginx-ds的CLUSTER-IP(执行<code>kubectl get svc |grep nginx-ds</code>命令获取)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@node1 ~]<span class="comment"># curl 172.16.117.40</span></div></pre></td></tr></table></figure></p>
<p>检查服务的 NodePort 可达性<br>在外部机器的浏览器访问 <a href="http://192.168.1.198:8446/" target="_blank" rel="external">http://192.168.1.198:8446/</a><br><img src="https://note.youdao.com/yws/api/personal/file/58CE3B07349943BCAF3AD33C2186D3E0?method=download&amp;shareKey=2d79cfcc3db594d9e8beac43af44de55" alt="k8s_nginx"><br>预期输出 nginx 欢迎页面内容</p>
<h4 id="部署-kubedns-插件"><a href="#部署-kubedns-插件" class="headerlink" title="部署 kubedns 插件"></a>部署 kubedns 插件</h4><p>官方文件目录：kubernetes/cluster/addons/dns<br><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods-dns-config" target="_blank" rel="external">https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods-dns-config</a>  </p>
<h5 id="系统预定义的-RoleBinding"><a href="#系统预定义的-RoleBinding" class="headerlink" title="系统预定义的 RoleBinding"></a>系统预定义的 RoleBinding</h5><p>预定义的 RoleBinding <code>system:kube-dns</code> 将 kube-system 命名空间的 <code>kube-dns</code> ServiceAccount 与 <code>system:kube-dns</code> Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get clusterrolebindings system:kube-dns -o yaml</span></div><div class="line">apiVersion: rbac.authorization.k8s.io/v1</div><div class="line">kind: ClusterRoleBinding</div><div class="line">metadata:</div><div class="line">  annotations:</div><div class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></div><div class="line">  creationTimestamp: 2018-01-31T11:03:07Z</div><div class="line">  labels:</div><div class="line">    kubernetes.io/bootstrapping: rbac-defaults</div><div class="line">  name: system:kube-dns</div><div class="line">  resourceVersion: <span class="string">"86"</span></div><div class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Akube-dns</div><div class="line">  uid: 51871810-0676-11e8-8cb0-1e2d0a5bc3f5</div><div class="line">roleRef:</div><div class="line">  apiGroup: rbac.authorization.k8s.io</div><div class="line">  kind: ClusterRole</div><div class="line">  name: system:kube-dns</div><div class="line">subjects:</div><div class="line">- kind: ServiceAccount</div><div class="line">  name: kube-dns</div><div class="line">  namespace: kube-system</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p><code>kubedns-controller.yaml</code> 中定义的 Pods 时使用了 <code>kubedns-sa.yaml</code> 文件定义的 <code>kube-dns</code> ServiceAccount，所以具有访问 kube-apiserver DNS 相关 API 的权限  </p>
<h5 id="配置-kube-dns-服务"><a href="#配置-kube-dns-服务" class="headerlink" title="配置 kube-dns 服务"></a>配置 kube-dns 服务</h5><p><a href="https://note.youdao.com/yws/api/personal/file/71B930661BD94F2495278744F67152ED?method=download&amp;shareKey=9545d5ea1a76180ceb41f96c8ad20e88" target="_blank" rel="external">busybox.yaml</a><br><a href="https://note.youdao.com/yws/api/personal/file/12BDDDB36A4E4C40BFBFCF45CA3136F5?method=download&amp;shareKey=e3dfda08a42733a5c60b05e86ece7e75" target="_blank" rel="external">kube-dns.yaml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@master1 dns]<span class="comment"># pwd</span></div><div class="line">/root/dns</div><div class="line">[root@master1 dns]<span class="comment"># ls</span></div><div class="line">busybox.yaml  kube-dns.yaml</div><div class="line">[root@master1 dns]<span class="comment"># egrep -i 'clusterIP|image|domain|server=/cluster' kube-dns.yaml </span></div><div class="line">  clusterIP: 172.16.0.2</div><div class="line">        image: registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-kube-dns-amd64:1.14.7</div><div class="line">        - --domain=cluster.local.</div><div class="line">        image: registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.7</div><div class="line">        - --server=/cluster.local/127.0.0.1<span class="comment">#10053</span></div><div class="line">        image: registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-sidecar-amd64:1.14.7</div><div class="line">[root@master1 dns]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<ul>
<li>需要将 <code>clusterIP</code> 设置为集群环境变量中变量 <code>CLUSTER_DNS_SVC_IP</code> 值，这个 IP 需要和 kubelet 的 <code>—cluster-dns</code> 参数值一致；</li>
</ul>
<p>配置 kube-dns Deployment  </p>
<ul>
<li><code>--domain</code> 为集群环境文档 变量 CLUSTER_DNS_DOMAIN 的值  </li>
<li>使用系统已经做了 RoleBinding 的 <code>kube-dns</code> ServiceAccount，该账户具有访问 kube-apiserver DNS 相关 API 的权限</li>
</ul>
<p>创建DNS<br><code>kubectl create -f kube-dns.yaml</code>创建dns，<code>kubectl delete -f kube-dns.yaml</code>删除dns<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f dns/busybox.yaml</span></div><div class="line"></div><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods -o wide</span></div><div class="line">NAME                               READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">busybox                            1/1       Running   0          19s       172.30.57.2   192.168.1.198</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f dns/kube-dns.yaml</span></div><div class="line">[root@master1 ~]<span class="comment"># </span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods -o wide -n kube-system</span></div><div class="line">NAME                                    READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">kube-dns-9d8b5fb76-vz6ll                3/3       Running   0          1h        172.30.41.5   192.168.1.199</div><div class="line">[root@master1 ~]<span class="comment"># </span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc -o wide -n kube-system</span></div><div class="line">NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE       SELECTOR</div><div class="line">kube-dns               ClusterIP   172.16.0.2       &lt;none&gt;        53/UDP,53/TCP   1h        k8s-app=kube-dns</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>如果报错了，可以通过这条命令查看报错日志<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl describe pod kube-dns-9d8b5fb76-vz6ll --namespace=kube-system</span></div></pre></td></tr></table></figure></p>
<h5 id="创建nginx-服务测试dns"><a href="#创建nginx-服务测试dns" class="headerlink" title="创建nginx 服务测试dns"></a>创建nginx 服务测试dns</h5><p><a href="https://note.youdao.com/yws/api/personal/file/8782A994D7724D0988B354247623586A?method=download&amp;shareKey=54f9b7b40dd83181ad97d0f87a541c48" target="_blank" rel="external">nginx-deployment.yaml</a><br><a href="https://note.youdao.com/yws/api/personal/file/D71364D682964ECCB6B291105F5004DE?method=download&amp;shareKey=12e6ed9da3fe5b732853490da4267c32" target="_blank" rel="external">nginx-service.yaml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f nginx-deployment.yaml</span></div><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f nginx-service.yaml</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods -o wide</span></div><div class="line">NAME                               READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">nginx-deployment<span class="_">-d</span>8d99448f-rb57v   1/1       Running   0          11m       172.30.41.2   192.168.1.199</div><div class="line">[root@master1 ~]<span class="comment">#</span></div><div class="line"></div><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc -o wide</span></div><div class="line">NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE       SELECTOR</div><div class="line">kubernetes      ClusterIP   172.16.0.1       &lt;none&gt;        443/TCP       7d        &lt;none&gt;</div><div class="line">nginx-service   NodePort    172.16.215.237   &lt;none&gt;        88:8527/TCP   2d        app=nginx</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl exec busybox -it nslookup nginx-service</span></div><div class="line">Server:    172.16.0.2</div><div class="line">Address 1: 172.16.0.2 kube-dns.kube-system.svc.cluster.local</div><div class="line"></div><div class="line">Name:      nginx-service</div><div class="line">Address 1: 172.16.215.237 nginx-service.default.svc.cluster.local</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="部署-heapster-插件"><a href="#部署-heapster-插件" class="headerlink" title="部署 heapster 插件"></a>部署 heapster 插件</h4><p>heapster release下载页面：<a href="https://github.com/kubernetes/heapster/releases" target="_blank" rel="external">https://github.com/kubernetes/heapster/releases</a><br><a href="https://note.youdao.com/yws/api/personal/file/1D279E2FC1834C328D2A32586B1BA43E?method=download&amp;shareKey=7466236d2b9cb7747d6657d0b2da0392" target="_blank" rel="external">heapster-v1.5.0.tar.gz</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># wget https://codeload.github.com/kubernetes/heapster/tar.gz/v1.5.0 -O heapster-v1.5.0.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># tar -zxf heapster-v1.5.0.tar.gz</span></div><div class="line">[root@master1 ~]<span class="comment"># cd heapster-1.5.0/deploy/kube-config/influxdb</span></div><div class="line">[root@master1 influxdb]<span class="comment"># ls</span></div><div class="line">grafana.yaml  heapster.yaml  influxdb.yaml</div><div class="line">[root@master1 influxdb]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h5 id="配置rbac"><a href="#配置rbac" class="headerlink" title="配置rbac"></a>配置rbac</h5><p><a href="https://note.youdao.com/yws/api/personal/file/CD179F4A7D4B47CA9F0FF2A55F033F6D?method=download&amp;shareKey=d80b32bb27faf201ba5ede4282aff019" target="_blank" rel="external">heapster-rbac.yaml</a><br>无需修改配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># pwd</span></div><div class="line">/root/heapster-1.5.0/deploy/kube-config/influxdb</div><div class="line">[root@master1 influxdb]<span class="comment"># kubectl create -f ../rbac/heapster-rbac.yaml</span></div></pre></td></tr></table></figure></p>
<p>如果不执行heapster-rbac.yaml文件，则会报下面的错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">E0518 06:08:09.927460       1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list *v1.Node: nodes is forbidden: User <span class="string">"system:serviceaccount:kube-system:heapster"</span> cannot list nodes at the cluster scope</div></pre></td></tr></table></figure></p>
<p>如果在启动docker容器时，一直处于<code>ContainerCreating</code>状态，且最后报下面的错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Warning  FailedCreatePodSandBox  34s (x11 over 43s)  kubelet, 192.168.1.204  Failed create pod sandbox.</div><div class="line">Normal   SandboxChanged          33s (x11 over 43s)  kubelet, 192.168.1.204  Pod sandbox changed, it will be killed and re-created.</div></pre></td></tr></table></figure></p>
<p>执行<code>journalctl --since 01:02:00 -u kubelet</code>查看日志发现，正在下载<code>registry.access.redhat.com/rhel7/pod-infrastructure:latest</code>docker镜像。<br>解决办法是：kube-node节点缺少<code>registry.access.redhat.com/rhel7/pod-infrastructure:latest</code>docker镜像。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker pull registry.access.redhat.com/rhel7/pod-infrastructure:latest</div></pre></td></tr></table></figure></p>
<h5 id="配置-influxdb-deployment"><a href="#配置-influxdb-deployment" class="headerlink" title="配置 influxdb-deployment"></a>配置 influxdb-deployment</h5><p><a href="https://note.youdao.com/yws/api/personal/file/BD2A861AB9B54032889A0C3D1E983190?method=download&amp;shareKey=516e9d2b54547090128c4c6d5aa220b5" target="_blank" rel="external">influxdb.yaml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># egrep -i 'image|nodeport' influxdb.yaml </span></div><div class="line">        image: lvanneo/heapster-influxdb-amd64:v1.1.1        <span class="comment">#修改image镜像</span></div><div class="line">  <span class="built_in">type</span>: NodePort</div><div class="line">[root@master1 influxdb]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># pwd</span></div><div class="line">/root/heapster-1.5.0/deploy/kube-config/influxdb</div><div class="line">[root@master1 influxdb]<span class="comment"># kubectl create -f influxdb.yaml</span></div></pre></td></tr></table></figure>
<p>查看influxdb数据库集群地址和端口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># kubectl get svc -o wide -n kube-system</span></div><div class="line">NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE       SELECTOR</div><div class="line">monitoring-influxdb   NodePort    172.16.38.42   &lt;none&gt;        8086:8898/TCP   14s       k8s-app=influxdb</div><div class="line">[root@master1 influxdb]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h5 id="配置-heapster-deployment"><a href="#配置-heapster-deployment" class="headerlink" title="配置 heapster-deployment"></a>配置 heapster-deployment</h5><p><a href="https://note.youdao.com/yws/api/personal/file/92E6D9F454164EA1A1EAB607CF87B17D?method=download&amp;shareKey=02e9edb636da9a1807d040d9fcb5e01a" target="_blank" rel="external">heapster.yaml</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># egrep -i 'image:|source|sink' heapster.yaml </span></div><div class="line">        image: lvanneo/heapster-amd64:v1.3.0-beta.1            <span class="comment">#修改image镜像</span></div><div class="line">        - --source=kubernetes:https://192.168.1.195:6443      <span class="comment">#指定kube-apiserver认证地址</span></div><div class="line">        - --sink=influxdb:http://172.16.38.42:8086       <span class="comment">#指定influxdb数据库CLUSTER-IP地址</span></div><div class="line">[root@master1 influxdb]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># pwd</span></div><div class="line">/root/heapster-1.5.0/deploy/kube-config/influxdb</div><div class="line">[root@master1 influxdb]<span class="comment"># kubectl create -f heapster.yaml</span></div></pre></td></tr></table></figure>
<p>查看heapster集群地址和端口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">    [root@master1 influxdb]<span class="comment"># kubectl get svc -o wide -n kube-system</span></div><div class="line">NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE       SELECTOR</div><div class="line">heapster              ClusterIP   172.16.202.225   &lt;none&gt;        80/TCP          12s       k8s-app=heapster</div><div class="line">[root@master1 influxdb]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h5 id="配置-grafana-deployment"><a href="#配置-grafana-deployment" class="headerlink" title="配置 grafana-deployment"></a>配置 grafana-deployment</h5><p><a href="https://note.youdao.com/yws/api/personal/file/F2F784AEBF0145E2964FC3C412F6AB73?method=download&amp;shareKey=b94e4b769899c636ae4761f33b97afde" target="_blank" rel="external">grafana.yaml</a><br>修改如下配置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># egrep -i 'image|value: /|type: nodeport' grafana.yaml </span></div><div class="line">        image: lvanneo/heapster-grafana-amd64:v4.0.2           <span class="comment">#修改image镜像</span></div><div class="line">          <span class="comment"># value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy    #默认</span></div><div class="line">          value: /         <span class="comment">#默认</span></div><div class="line">  <span class="built_in">type</span>: NodePort           <span class="comment">#去掉注释</span></div><div class="line">[root@master1 influxdb]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># pwd</span></div><div class="line">/root/heapster-1.5.0/deploy/kube-config/influxdb</div><div class="line">[root@master1 influxdb]<span class="comment"># kubectl create -f grafana.yaml</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 influxdb]<span class="comment"># kubectl get svc -o wide -n kube-system</span></div><div class="line">NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE       SELECTOR</div><div class="line">monitoring-grafana     NodePort    172.16.207.209   &lt;none&gt;        80:8642/TCP     31m       k8s-app=grafana</div></pre></td></tr></table></figure>
<p>检查 Deployment<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get deployments -n kube-system | grep -E 'heapster|monitoring'</span></div><div class="line">heapster              1         1         1            1           25m</div><div class="line">monitoring-grafana    1         1         1            1           53s</div><div class="line">monitoring-influxdb   1         1         1            1           28m</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>检查 Pods<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pods -n kube-system -o wide </span></div><div class="line">NAME                                    READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">heapster-5<span class="built_in">fc</span>8f648dc-pn4lm               1/1       Running   0          1h        172.30.57.4   192.168.1.198</div><div class="line">monitoring-grafana-57b8fcd7b4-67r2j     1/1       Running   0          28m       172.30.57.5   192.168.1.198</div><div class="line">monitoring-influxdb-68d87d45f5-5d7qs    1/1       Running   0          1h        172.30.41.3   192.168.1.199</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>需要逐一检查下各docker的logs有没有报错信息  </p>
<h6 id="访问grafana"><a href="#访问grafana" class="headerlink" title="访问grafana"></a>访问grafana</h6><p>访问 <a href="http://192.168.1.198:8642/" target="_blank" rel="external">http://192.168.1.198:8642/</a><br>配置influxdb-datasource<br><img src="https://note.youdao.com/yws/api/personal/file/A4614D24871C427489359C1827DC7548?method=download&amp;shareKey=abbb42575786c75c1b28c88c32d985b7" alt="grafana"><br><img src="https://note.youdao.com/yws/api/personal/file/91D96DCEF1D849729511049E94482122?method=download&amp;shareKey=55ebe8b0392e107d9344d58f3bd7ede1" alt="grafana"><br><img src="https://note.youdao.com/yws/api/personal/file/66317BF04C6A4C8281095808373DD48E?method=download&amp;shareKey=82c36a103ee293f3bdc6c03c3b997674" alt="grafana"><br><img src="https://note.youdao.com/yws/api/personal/file/534929F4400246E7ACA46CEB42D4540D?method=download&amp;shareKey=0a8aa8882c217c30dabb72e8f9050480" alt="grafana"><br><img src="https://note.youdao.com/yws/api/personal/file/E765DF64401B40A7BA3ACC9EF4BFFB98?method=download&amp;shareKey=9e289a2a510957a395f3abb799f2601f" alt="grafana">  </p>
<h4 id="部署-dashboard-插件"><a href="#部署-dashboard-插件" class="headerlink" title="部署 dashboard 插件"></a>部署 dashboard 插件</h4><p><a href="https://note.youdao.com/yws/api/personal/file/A1CFC0139B514BF7B6C0CD3D6F53531B?method=download&amp;shareKey=c70aa3e74b26c22fa42337538f8fba36" target="_blank" rel="external">kubernetes-dashboard.yaml</a><br>通过rbac 认证<code>kind: ServiceAccount</code>调用认证文件(<code>/etc/kubernetes/bootstrap.kubeconfig</code>和<code>/etc/kubernetes/kube-proxy.kubeconfig</code>)，访问的接口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># egrep -i 'image|apiserver|heapster-host|nodeport' kubernetes-dashboard.yaml </span></div><div class="line">        image: k8scn/kubernetes-dashboard-amd64:v1.8.0</div><div class="line">          - --apiserver-host=http://192.168.1.195:8080</div><div class="line">          - --heapster-host=http://172.16.202.225</div><div class="line">  <span class="built_in">type</span>: NodePort</div><div class="line">      <span class="comment">#nodePort: 38443    #k8s kind:server 可以使用nodePort指定端口</span></div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl create -f kubernetes-dashboard.yaml</span></div></pre></td></tr></table></figure>
<p>查看kubernetes-dashboard分配的Node<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get pod -o wide -n kube-system</span></div><div class="line">NAME                                    READY     STATUS    RESTARTS   AGE       IP            NODE</div><div class="line">kubernetes-dashboard-666fbbf977-v9vsh   1/1       Running   0          49s       172.30.41.4   192.168.1.199</div></pre></td></tr></table></figure></p>
<p>查看kubernetes-dashboard分配的 NodePort<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get svc -o wide -n kube-system</span></div><div class="line">NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE       SELECTOR</div><div class="line">kubernetes-dashboard   NodePort    172.16.59.24     &lt;none&gt;        443:8847/TCP    39m       k8s-app=kubernetes-dashboard</div></pre></td></tr></table></figure></p>
<ul>
<li>NodePort 8847映射到 dashboard pod 80端口  </li>
</ul>
<p>检查 controller<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl get deployment kubernetes-dashboard  -n kube-system</span></div><div class="line">NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</div><div class="line">kubernetes-dashboard   1         1         1            1           15m</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>获取集群服务地址列表<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@master1 ~]<span class="comment"># kubectl cluster-info</span></div><div class="line">Kubernetes master is running at https://192.168.1.195:6443</div><div class="line">Heapster is running at https://192.168.1.195:6443/api/v1/namespaces/kube-system/services/heapster/proxy</div><div class="line">KubeDNS is running at https://192.168.1.195:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</div><div class="line">monitoring-grafana is running at https://192.168.1.195:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</div><div class="line">monitoring-influxdb is running at https://192.168.1.195:6443/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy</div><div class="line"></div><div class="line">To further debug and diagnose cluster problems, use <span class="string">'kubectl cluster-info dump'</span>.</div><div class="line">[root@master1 ~]<span class="comment">#</span></div></pre></td></tr></table></figure></p>
<h5 id="访问dashboard"><a href="#访问dashboard" class="headerlink" title="访问dashboard"></a>访问dashboard</h5><p>kubernetes-dashboard 服务暴露了 NodePort，可以使用 <a href="http://NodeIP:nodePort" target="_blank" rel="external">http://NodeIP:nodePort</a> 地址访问 dashboard  </p>
<p>访问 <a href="https://192.168.1.199:8847" target="_blank" rel="external">https://192.168.1.199:8847</a> 访问k8s dashboard，经测试火狐浏览器可以，但360浏览器打不开，报404<br><img src="https://note.youdao.com/yws/api/personal/file/045DBC6BEB45413AAA3C638456310A02?method=download&amp;shareKey=f5da2b7cde024379197726cbd1cf86a5" alt="k8s-dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/C62DEA5E39544B70A3DE0F3C4286EAB6?method=download&amp;shareKey=0d549ebb275806b21dcfe84bfec751c2" alt="k8s-dashboard"><br><img src="https://note.youdao.com/yws/api/personal/file/64C58F5AE8284F75BEB4A3DACC7C9A8A?method=download&amp;shareKey=c60811cb5ac18d5a9bd86c903fdef485" alt="k8s-dashboard">  </p>
<p>如果不安装<code>Heapster/influxdb</code>等插件，k8s-dashboard不能展示<code>Pod</code>，<code>Nodes</code>的<code>CPU</code>，<code>内存</code>等 metric 图形  </p>
<h5 id="保存镜像"><a href="#保存镜像" class="headerlink" title="保存镜像"></a>保存镜像</h5><p>导出镜像<br><a href="https://pan.baidu.com/s/1sngFMlZ" target="_blank" rel="external">镜像保存在百度网盘packages目录，密码：nwzk</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">docker save k8scn/kubernetes-dashboard-amd64:v1.8.0 &gt; kubernetes-dashboard-amd64-v1.8.0.tar.gz</div><div class="line">docker save registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-sidecar-amd64:1.14.7 &gt; k8s-dns-sidecar-amd64-1.14.7.tar.gz</div><div class="line">docker save registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-kube-dns-amd64:1.14.7 &gt; k8s-dns-kube-dns-amd64-1.14.7.tar.gz</div><div class="line">docker save registry.cn-hangzhou.aliyuncs.com/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.7 &gt; k8s-dns-dnsmasq-nanny-amd64-1.14.7.tar.gz</div><div class="line">docker save lvanneo/heapster-influxdb-amd64:v1.1.1 &gt; heapster-influxdb-amd64-v1.1.1.tar.gz</div><div class="line">docker save lvanneo/heapster-grafana-amd64:v4.0.2 &gt; heapster-grafana-amd64-v4.0.2.tar.gz</div><div class="line">docker save lvanneo/heapster-amd64:v1.3.0-beta.1 &gt; heapster-amd64-v1.3.0-beta.1.tar.gz</div><div class="line">docker save k8scn/kubernetes-dashboard-amd64:v1.8.0 &gt; kubernetes-dashboard-amd64-v1.8.0.tar.gz</div></pre></td></tr></table></figure></p>
<p>导入镜像<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker load -i kubernetes-dashboard-amd64-v1.8.0.tar.gz</div></pre></td></tr></table></figure></p>
<p>参考：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster" target="_blank" rel="external">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/02/23/部署TLS-k8s/">http://www.yfshare.vip/2018/02/23/部署TLS-k8s/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效(powerful)，Kubernetes提供了应用部署，规划，更新，维护的一种机制。Kubernetes一个核心的特点就是能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="k8s" scheme="http://www.yfshare.vip/tags/k8s/"/>
    
      <category term="kubernetes" scheme="http://www.yfshare.vip/tags/kubernetes/"/>
    
      <category term="TLS" scheme="http://www.yfshare.vip/tags/TLS/"/>
    
  </entry>
  
  <entry>
    <title>自建docker私有仓库(Vmware Harbor)</title>
    <link href="http://www.yfshare.vip/2018/01/11/%E8%87%AA%E5%BB%BAdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93-Vmware-Harbor/"/>
    <id>http://www.yfshare.vip/2018/01/11/自建docker私有仓库-Vmware-Harbor/</id>
    <published>2018-01-11T12:08:25.000Z</published>
    <updated>2019-01-20T08:25:48.818Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中，确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。<br><a id="more"></a></p>
<ul>
<li>基于角色的访问控制 - 用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。  </li>
<li>镜像复制 - 镜像可以在多个Registry实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。  </li>
<li>图形化用户界面 - 用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。  </li>
<li>AD/LDAP 支持 - Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。  </li>
<li>审计管理 - 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。  </li>
<li>国际化 - 已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。  </li>
<li>RESTful API - RESTful API 提供给管理员对于Harbor更多的操控, 使得与其它管理软件集成变得更容易。  </li>
<li>部署简单 - 提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。  </li>
</ul>
<p>官网地址：<a href="https://vmware.github.io/harbor/cn/" target="_blank" rel="external">https://vmware.github.io/harbor/cn/</a><br>官方下载地址：<a href="https://github.com/vmware/harbor/releases" target="_blank" rel="external">https://github.com/vmware/harbor/releases</a><br><a href="https://pan.baidu.com/s/1gfeZEOb" target="_blank" rel="external">vmware harbor v1.3.0-rc4百度网盘，密码：m2mi</a><br>安装向导：<a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="external">https://github.com/vmware/harbor/blob/master/docs/installation_guide.md</a><br>用户使用指南：<a href="https://github.com/vmware/harbor/blob/master/docs/user_guide.md" target="_blank" rel="external">https://github.com/vmware/harbor/blob/master/docs/user_guide.md</a><br>https配置：<a href="https://github.com/vmware/harbor/blob/master/docs/configure_https.md" target="_blank" rel="external">https://github.com/vmware/harbor/blob/master/docs/configure_https.md</a>  </p>
<h4 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker-compose"></a>安装docker-compose</h4><p>docker-compose版本需要大于<code>1.7.1+</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose</span></div><div class="line"><span class="comment"># chmod +x /usr/local/bin/docker-compose</span></div></pre></td></tr></table></figure></p>
<h4 id="生成证书"><a href="#生成证书" class="headerlink" title="生成证书"></a>生成证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p /data/cert</div><div class="line"><span class="built_in">cd</span> /data/cert</div><div class="line">openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt</div><div class="line">openssl req -newkey rsa:4096 -nodes -sha256 -keyout server.key -out server.csr</div><div class="line"><span class="built_in">echo</span> subjectAltName = IP:192.168.1.196 &gt; extfile.cnf</div><div class="line">openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -extfile extfile.cnf -out server.crt</div></pre></td></tr></table></figure>
<h4 id="部署Vmware-Harbor"><a href="#部署Vmware-Harbor" class="headerlink" title="部署Vmware Harbor"></a>部署Vmware Harbor</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tar -zxf harbor-offline-installer-v1.3.0-rc4.tgz</span></div><div class="line"><span class="comment"># cd harbor</span></div><div class="line"><span class="comment"># grep -iv '^#' harbor.cfg | grep -iv '^$'</span></div><div class="line">hostname = 192.168.1.196          <span class="comment">#如果用购买的证书需要写域名</span></div><div class="line">ui_url_protocol = https           <span class="comment">#http改成https</span></div><div class="line">db_password = root123             <span class="comment">#密码不能改，否则有些容器起不来</span></div><div class="line">max_job_workers = 3</div><div class="line">customize_crt = on</div><div class="line">ssl_cert = /data/cert/server.crt  <span class="comment">#指定证书路径</span></div><div class="line">ssl_cert_key = /data/cert/server.key   <span class="comment">#同上，下面全默认即可</span></div><div class="line">secretkey_path = /data</div><div class="line">admiral_url = NA</div><div class="line">clair_db_password = password</div><div class="line">log_rotate_count = 50</div><div class="line">log_rotate_size = 200M</div><div class="line">email_identity = </div><div class="line">email_server = smtp.mydomain.com</div><div class="line">email_server_port = 25</div><div class="line">email_username = sample_admin@mydomain.com</div><div class="line">email_password = abc</div><div class="line">email_from = admin &lt;sample_admin@mydomain.com&gt;</div><div class="line">email_ssl = <span class="literal">false</span></div><div class="line">email_insecure = <span class="literal">false</span></div><div class="line">harbor_admin_password = Harbor12345</div><div class="line">auth_mode = db_auth</div><div class="line">ldap_url = ldaps://ldap.mydomain.com</div><div class="line">ldap_basedn = ou=people,dc=mydomain,dc=com</div><div class="line">ldap_uid = uid </div><div class="line">ldap_scope = 3 </div><div class="line">ldap_timeout = 5</div><div class="line">self_registration = on</div><div class="line">token_expiration = 30</div><div class="line">project_creation_restriction = everyone</div><div class="line">db_host = mysql</div><div class="line">db_port = 3306</div><div class="line">db_user = root</div><div class="line">uaa_endpoint = uaa.mydomain.org</div><div class="line">uaa_clientid= id</div><div class="line">uaa_clientsecret= secret</div><div class="line">uaa_ca_root= /path/to/uaa_ca.pem</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<p>Docker版本大于<code>1.6.0</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cat /etc/docker/daemon.json</span></div><div class="line">&#123; <span class="string">"insecure-registries"</span>:[<span class="string">"192.168.1.196"</span>] &#125;           <span class="comment">#如果harbor.cfg里填的是域名，这里要保持一致</span></div><div class="line"><span class="comment">#</span></div><div class="line">systemctl <span class="built_in">enable</span> docker</div><div class="line">systemctl start docker</div><div class="line">./install.sh            <span class="comment">#安装Vmware Harbor</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># docker images | grep -i vmware</span></div><div class="line">vmware/harbor-log                                                 v1.3.0-rc4          58aa9393b1<span class="built_in">cd</span>        3 weeks ago         207MB</div><div class="line">vmware/harbor-jobservice                                          v1.3.0-rc4          b3664e837ab8        3 weeks ago         197MB</div><div class="line">vmware/harbor-ui                                                  v1.3.0-rc4          5f6e4c4b41da        3 weeks ago         211MB</div><div class="line">vmware/harbor-adminserver                                         v1.3.0-rc4          a907519f7baf        3 weeks ago         174MB</div><div class="line">vmware/harbor-db                                                  v1.3.0-rc4          83b013940805        3 weeks ago         586MB</div><div class="line">vmware/photon                                                     1.0                 7b154bf6f104        3 weeks ago         130MB</div><div class="line">vmware/clair                                                      v2.0.1-photon       7a633033c5b1        5 weeks ago         365MB</div><div class="line">vmware/postgresql                                                 9.6.5-photon        a5c79b0473d9        6 weeks ago         285MB</div><div class="line">vmware/registry                                                   2.6.2-photon        c38af846a0da        6 weeks ago         240MB</div><div class="line">vmware/mariadb-photon                                             10.2.10             eaaae71dea19        6 weeks ago         586MB</div><div class="line">vmware/notary-photon                                              signer-0.5.1        064b309ad822        6 weeks ago         246MB</div><div class="line">vmware/notary-photon                                              server-0.5.1        b8cc51024379        6 weeks ago         247MB</div><div class="line">vmware/nginx-photon                                               1.11.13             2971c92cc1ae        6 weeks ago         200MB</div><div class="line">vmware/harbor-db-migrator                                         1.3                 6cac2b89f086        6 weeks ago         1.11GB</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@localhost ~]<span class="comment"># docker ps -a | grep -i vmware</span></div><div class="line">ea6b2d79bd4b    vmware/nginx-photon:1.11.13           <span class="string">"nginx -g 'daemon of…"</span>   About a minute ago   Up About a minute        0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp   nginx</div><div class="line">3e1e767e3095    vmware/harbor-jobservice:v1.3.0-rc4   <span class="string">"/harbor/start.sh"</span>       About a minute ago   Up About a minute (healthy)  harbor-jobserviced</div><div class="line">08ae8e864d9     vmware/harbor-ui:v1.3.0-rc4           <span class="string">"/harbor/start.sh"</span>       About a minute ago   Up About a minute (healthy)  harbor-ui</div><div class="line">e2abd8a9f45d    vmware/harbor-db:v1.3.0-rc4           <span class="string">"/usr/local/bin/dock…"</span>   About a minute ago   Up About a minute (healthy)   3306/tcp    harbor-db</div><div class="line">5337d7d2cb0a    vmware/harbor-adminserver:v1.3.0-rc4  <span class="string">"/harbor/start.sh"</span>       About a minute ago   Up About a minute (healthy)   harbor-adminserver</div><div class="line">cf85ac001f1f    vmware/registry:2.6.2-photon          <span class="string">"/entrypoint.sh serv…"</span>   About a minute ago   Up About a minute (healthy)   5000/tcp    registry</div><div class="line">d6075f9aa12c    vmware/harbor-log:v1.3.0-rc4          <span class="string">"/bin/sh -c /usr/loc…"</span>   About a minute ago   Up About a minute (healthy)   127.0.0.1:1514-&gt;10514/tcp  harbor-log</div><div class="line">[root@localhost ~]<span class="comment">#</span></div></pre></td></tr></table></figure>
<p>登陆时如果报下面错误，则需要修改<code>/etc/docker/daemon.json</code>文件<br><code>Error response from daemon: Get https://192.168.1.196/v2/: x509: certificate signed by unknown authority</code><br>参考：<a href="https://github.com/vmware/harbor/blob/master/docs/user_guide.md#pulling-and-pushing-images-using-docker-client" target="_blank" rel="external">https://github.com/vmware/harbor/blob/master/docs/user_guide.md#pulling-and-pushing-images-using-docker-client</a>  </p>
<h4 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker login 192.168.1.196</span></div><div class="line">Username: admin</div><div class="line">Password: </div><div class="line">Login Succeeded</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># docker tag nginx:latest 192.168.1.196/library/nginx:latest</span></div><div class="line"><span class="comment"># docker push 192.168.1.196/library/nginx:latest</span></div></pre></td></tr></table></figure>
<h4 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker rmi 192.168.1.196/library/nginx nginx</span></div><div class="line"><span class="comment"># docker pull 192.168.1.196/library/nginx:latest</span></div><div class="line"><span class="comment"># docker images | grep -i nginx</span></div><div class="line">192.168.1.196/library/nginx   latest              3f8a4339aadd        2 weeks ago         108MB</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p>访问：<a href="https://192.168.1.196" target="_blank" rel="external">https://192.168.1.196</a> 打开vmware Harbor<br>默认用户名密码：<code>admin</code> / <code>Harbor12345</code><br><img src="https://note.youdao.com/yws/api/personal/file/FBB5CB4E422E45B09399E5629C336CD8?method=download&amp;shareKey=45cd553065ceb3c801443de38c483163" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/4620151E79EC49FA95D46FCFE61B81BB?method=download&amp;shareKey=335eab830fd98b9e0755722d8eb8a7e7" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/1F7BDB6EC9724BCE98FC027EEAE58356?method=download&amp;shareKey=698ec4f96d8d22df4b2aabb1de4f62e6" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/2C45B2D75C624C1E8E25FBB5700B7C17?method=download&amp;shareKey=e14797d51254c589aa2fbfba31fa4d4f" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/E9BC596839A8449E9BAB09B30E07D8E8?method=download&amp;shareKey=64519e8e82850773141cc6071cd2ddae" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/788558B5E45F4900B5E71109B1C6A7F7?method=download&amp;shareKey=13a6c8be4dc3ea68649e31a0ae91fc85" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/AD8A2E095F4A48EE8725E2036F521282?method=download&amp;shareKey=d79ca2bb925c020ee84ea719cc9c8637" alt="vmware harbor"><br><img src="https://note.youdao.com/yws/api/personal/file/354AC86784DB4B76AC72172F6AFDAF20?method=download&amp;shareKey=97c1afb9dddacd63de33499ce98a980d" alt="vmware harbor">  </p>
<h4 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># pwd</span></div><div class="line">/root/harbor</div><div class="line"><span class="comment"># docker-compose -f docker-compose.yml down</span></div></pre></td></tr></table></figure>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2018/01/11/自建docker私有仓库-Vmware-Harbor/">http://www.yfshare.vip/2018/01/11/自建docker私有仓库-Vmware-Harbor/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中，确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Docker" scheme="http://www.yfshare.vip/tags/Docker/"/>
    
      <category term="Docker Registry" scheme="http://www.yfshare.vip/tags/Docker-Registry/"/>
    
  </entry>
  
  <entry>
    <title>The Docker of Jira7.2.0 and Confluence6.0.0</title>
    <link href="http://www.yfshare.vip/2017/12/20/The-Docker-of-Jira7-2-0-and-Confluence6-0-0/"/>
    <id>http://www.yfshare.vip/2017/12/20/The-Docker-of-Jira7-2-0-and-Confluence6-0-0/</id>
    <published>2017-12-20T13:40:58.000Z</published>
    <updated>2019-01-20T08:25:18.996Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>JIRA是一个缺陷跟踪管理系统，开发者是 Atlassian；<br>Confluence 是一个专业的企业知识管理与协同软件，可以用于构建企业wiki。<br>这里我把Jira7.2.0 和 Confluence6.0.0根据实际需求把它们封装成两个Docker容器，方便我们快速部署它们。<br><a id="more"></a></p>
<h4 id="主要变化"><a href="#主要变化" class="headerlink" title="主要变化"></a>主要变化</h4><ul>
<li>添加官方JIRA和Confluence容器宿主机debian9 中文支持</li>
<li>修改Mysql字符集为UTF8，默认是latin1</li>
</ul>
<h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p>如果想分别单独部署 JIRA 和 Confluence，可以参考：<br>注：需要使用我提供的三个docker image，否则官网的image不支持中文  </p>
<ul>
<li>mysql：<a href="https://hub.docker.com/r/yfshare/mysql/" target="_blank" rel="external">https://hub.docker.com/r/yfshare/mysql/</a>  </li>
<li>JIRA：<a href="https://hub.docker.com/r/yfshare/jira/" target="_blank" rel="external">https://hub.docker.com/r/yfshare/jira/</a>  </li>
<li>Confluence：<a href="https://hub.docker.com/r/yfshare/confluence/" target="_blank" rel="external">https://hub.docker.com/r/yfshare/confluence/</a>  </li>
</ul>
<p>部署分两种方法：</p>
<ul>
<li>手动部署，按照步骤一步步来，可以参考：<ul>
<li>JIRA：<a href="http://www.yfshare.vip/2017/05/09/%E9%83%A8%E7%BD%B2JIRA-7-2-2-for-Linux/">http://www.yfshare.vip/2017/05/09/%E9%83%A8%E7%BD%B2JIRA-7-2-2-for-Linux/</a>  </li>
<li>Confluence：<a href="http://www.yfshare.vip/2017/06/25/%E9%83%A8%E7%BD%B2atlassian-confluence-6-1-2/">http://www.yfshare.vip/2017/06/25/%E9%83%A8%E7%BD%B2atlassian-confluence-6-1-2/</a>  </li>
</ul>
</li>
<li>Pull Docker<ul>
<li><code>docker pull yfshare/mysql:5.6</code>  </li>
<li><code>docker pull yfshare/jira:7.2.0</code>  </li>
<li><code>docker pull yfshare/confluence:6.0.0</code>  </li>
</ul>
</li>
</ul>
<p>这里提供了Docker-Compose，把JIRA和Confluence整合在一起了，和前面不同的是，这里只使用了一个数据库，前面直接拉会出现两个数据库。<br>但，由于The Docker of Mysql环境变量问题，所以Confluence需要手动初始化数据库，JIRA可以在Docker Compose里直接定义。  </p>
<h5 id="Docker-Compose"><a href="#Docker-Compose" class="headerlink" title="Docker Compose"></a>Docker Compose</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cat docker-jira-confluence-compose.yml </span></div><div class="line">version: <span class="string">'2'</span></div><div class="line">services:</div><div class="line">  db:</div><div class="line">    image: yfshare/mysql:5.6</div><div class="line">    container_name: atlassian_mysql</div><div class="line">    environment:</div><div class="line">      MYSQL_ROOT_PASSWORD: <span class="string">'20180223@Julend.com'</span></div><div class="line">      MYSQL_DATABASE: <span class="string">'jiradb'</span></div><div class="line">      MYSQL_USER: <span class="string">'jira'</span></div><div class="line">      MYSQL_PASSWORD: <span class="string">'jirapass@20180223'</span></div><div class="line">    ports:</div><div class="line">      - 3306:3306</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/atlassian_mysql:/var/lib/mysql</div><div class="line">      - /etc/localtime:/etc/localtime:ro</div><div class="line">    restart: always</div><div class="line"></div><div class="line">  JIRA:</div><div class="line">    image: yfshare/jira:7.2.0</div><div class="line">    container_name: JIRA</div><div class="line">    ports:</div><div class="line">      - 8080:8080</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/atlassian/Jira_Home:/var/atlassian/jira</div><div class="line">      - /etc/localtime:/etc/localtime:ro</div><div class="line">    links:</div><div class="line">      - db</div><div class="line">    restart: always</div><div class="line"></div><div class="line"><span class="comment">#Confluence数据需要自己手动创建</span></div><div class="line"><span class="comment">#mkdir -p /data/docker_mount/atlassian/&#123;Jira_Home,Confluence_Home&#125; &amp;&amp; chmod 777 /data/docker_mount/atlassian/ -R</span></div><div class="line"><span class="comment">#create database confluencedb default character set utf8 collate utf8_general_ci;</span></div><div class="line"><span class="comment">#create user 'confluenceuser'@'%' identified by 'a86b6913dd';</span></div><div class="line"><span class="comment">#grant all privileges on confluencedb.* to 'confluenceuser'@'%';</span></div><div class="line"><span class="comment">#flush privileges;</span></div><div class="line">  Confluence:</div><div class="line">    image: yfshare/confluence:6.0.0</div><div class="line">    container_name: Confluence</div><div class="line">    ports:</div><div class="line">      - 8090:8090</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/atlassian/Confluence_Home:/var/atlassian/confluence</div><div class="line">      - /etc/localtime:/etc/localtime:ro</div><div class="line">    links:</div><div class="line">      - db</div><div class="line">    restart: always</div></pre></td></tr></table></figure>
<h5 id="手动初始化confluence数据库"><a href="#手动初始化confluence数据库" class="headerlink" title="手动初始化confluence数据库"></a>手动初始化confluence数据库</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir -p /data/docker_mount/atlassian/&#123;Jira_Home,Confluence_Home&#125; &amp;&amp; chmod 777 /data/docker_mount/atlassian/ -R</div><div class="line">mysql -uroot -p<span class="string">'password'</span></div><div class="line">&gt; create database confluencedb default character <span class="built_in">set</span> utf8 collate utf8_general_ci;</div><div class="line">&gt; create user <span class="string">'confluenceuser'</span>@<span class="string">'%'</span> identified by <span class="string">'a86b6913dd'</span>;</div><div class="line">&gt; grant all privileges on confluencedb.* to <span class="string">'confluenceuser'</span>@<span class="string">'%'</span>;</div><div class="line">&gt; flush privileges;</div></pre></td></tr></table></figure>
<h5 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker-compose <span class="_">-f</span> docker-jira-confluence-compose.yml up <span class="_">-d</span></div></pre></td></tr></table></figure>
<h5 id="初始化配置"><a href="#初始化配置" class="headerlink" title="初始化配置"></a>初始化配置</h5><p>JIRA访问：<code>http://localhost:8080</code><br>Confluence访问：<code>http://localhost:8090</code>  </p>
<h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p><img src="https://note.youdao.com/yws/api/personal/file/40545D4B08214DFC9C24CEA80BD99057?method=download&amp;shareKey=09a20fe6e1d02d17c571dc81cb085994" alt="jira_login"><br><img src="https://note.youdao.com/yws/api/personal/file/67E688566D8742B8909EC22497EC5113?method=download&amp;shareKey=a348a3215c8edc63a8eb1b0f9096ca4a" alt="jira_issue">  </p>
<p><img src="https://note.youdao.com/yws/api/personal/file/44878F806D6D41C29ADE42983698F665?method=download&amp;shareKey=2f3342ab2a85e08cce3e896ab381ee31" alt="confluence_login"><br><img src="https://note.youdao.com/yws/api/personal/file/223D0600A1804DCDB3E1C23C0BE8E670?method=download&amp;shareKey=9bb3b45acf19a6f74e1982cd9c06a7e2" alt="confluence_issue">  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2017/12/20/The-Docker-of-Jira7-2-0-and-Confluence6-0-0/">http://www.yfshare.vip/2017/12/20/The-Docker-of-Jira7-2-0-and-Confluence6-0-0/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;JIRA是一个缺陷跟踪管理系统，开发者是 Atlassian；&lt;br&gt;Confluence 是一个专业的企业知识管理与协同软件，可以用于构建企业wiki。&lt;br&gt;这里我把Jira7.2.0 和 Confluence6.0.0根据实际需求把它们封装成两个Docker容器，方便我们快速部署它们。&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Docker" scheme="http://www.yfshare.vip/tags/Docker/"/>
    
      <category term="Jira" scheme="http://www.yfshare.vip/tags/Jira/"/>
    
      <category term="Confluence" scheme="http://www.yfshare.vip/tags/Confluence/"/>
    
  </entry>
  
  <entry>
    <title>自建docker私有仓库(Registry)</title>
    <link href="http://www.yfshare.vip/2017/12/20/%E8%87%AA%E5%BB%BAdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93-Registry/"/>
    <id>http://www.yfshare.vip/2017/12/20/自建docker私有仓库-Registry/</id>
    <published>2017-12-20T12:59:07.000Z</published>
    <updated>2019-01-20T08:25:05.410Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>Docker Hub 存放着 Docker 及其组件的所有资源。它可以提供：  </p>
<ul>
<li>Docker 镜像主机</li>
<li>用户认证</li>
<li>自动镜像构建和工作流程工具，如构建触发器和 web hooks</li>
<li>整合了 GitHub 和 BitBucket</li>
</ul>
<p>但，有些场景我们需要一个私有仓库来管理自己的镜像，可以通过Registry来实现此目的。Registry作为Docker的核心组件之一负责镜像内容的存储与分发，客户端的docker pull以及push命令都将直接与registry进行交互。  </p>
<p>环境：<br>　　Docker 17.12.0-ce-rc4<br>　　Centos 7.3  </p>
<h4 id="部署Docker"><a href="#部署Docker" class="headerlink" title="部署Docker"></a>部署Docker</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># yum remove docker docker-common docker-selinux docker-engine -y</span></div><div class="line"><span class="comment"># yum install -y yum-utils device-mapper-persistent-data lvm2</span></div><div class="line"><span class="comment"># yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span></div><div class="line"><span class="comment"># yum-config-manager --enable docker-ce-edge</span></div><div class="line"><span class="comment"># yum-config-manager --enable docker-ce-test</span></div><div class="line"><span class="comment"># yum install docker-ce -y</span></div></pre></td></tr></table></figure>
<h4 id="自建证书"><a href="#自建证书" class="headerlink" title="自建证书"></a>自建证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># mkdir -p conf</span></div><div class="line"><span class="comment"># openssl req -new -newkey rsa:4096 -days 365 -subj "/CN=localhost" -nodes -x509 -keyout conf/auth.key -out conf/auth.cert</span></div></pre></td></tr></table></figure>
<h4 id="registry容器配置文件"><a href="#registry容器配置文件" class="headerlink" title="registry容器配置文件"></a>registry容器配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 启动registry容器需要用到</span></div><div class="line"><span class="comment"># cat registry-srv.yml</span></div><div class="line">version: 0.1</div><div class="line"><span class="built_in">log</span>:</div><div class="line">  fields:</div><div class="line">    service: registry</div><div class="line"></div><div class="line">storage:</div><div class="line">  delete:</div><div class="line">    enabled: <span class="literal">true</span></div><div class="line">  cache:</div><div class="line">    blobdescriptor: inmemory</div><div class="line">  filesystem:</div><div class="line">    rootdirectory: /var/lib/registry</div><div class="line"></div><div class="line">http:</div><div class="line">  addr: 0.0.0.0:5000   </div><div class="line">  headers:</div><div class="line">    X-Content-Type-Options: [nosniff]</div><div class="line"></div><div class="line">health:</div><div class="line">  storagedriver:</div><div class="line">    enabled: <span class="literal">true</span></div><div class="line">    interval: 10s</div><div class="line">    threshold: 3</div><div class="line"></div><div class="line">auth:</div><div class="line">  token:</div><div class="line">    <span class="comment"># external url to docker-web authentication endpoint</span></div><div class="line">    realm: http://registry-web:8080/api/auth</div><div class="line">    <span class="comment"># should be same as registry.name of registry-web</span></div><div class="line">    service: registry-srv:5000</div><div class="line">    <span class="comment"># should be same as registry.auth.issuer of registry-web</span></div><div class="line">    issuer: <span class="string">'my issuer'</span></div><div class="line">    <span class="comment"># path to auth certificate</span></div><div class="line">    rootcertbundle: /etc/docker/registry/auth.cert</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="registry-web容器配置文件"><a href="#registry-web容器配置文件" class="headerlink" title="registry-web容器配置文件"></a>registry-web容器配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 启动registry-web容器需要用到</span></div><div class="line"><span class="comment"># cat registry-web.yml </span></div><div class="line">registry:</div><div class="line">  <span class="comment"># Docker registry url</span></div><div class="line">  url: http://registry-srv:5000/v2</div><div class="line">  <span class="comment"># Docker registry fqdn</span></div><div class="line">  name: registry-srv:5000</div><div class="line">  <span class="comment"># To allow image delete, should be false</span></div><div class="line">  <span class="built_in">readonly</span>: <span class="literal">false</span></div><div class="line">  auth:</div><div class="line">    <span class="comment"># Enable authentication</span></div><div class="line">    enabled: <span class="literal">true</span></div><div class="line">    <span class="comment"># Token issuer</span></div><div class="line">    <span class="comment"># should equals to auth.token.issuer of docker registry</span></div><div class="line">    issuer: <span class="string">'my issuer'</span></div><div class="line">    <span class="comment"># Private key for token signing</span></div><div class="line">    <span class="comment"># certificate used on auth.token.rootcertbundle should signed by this key</span></div><div class="line">    key: /conf/auth.key</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tree docker-registry-web/</span></div><div class="line">docker-registry-web/</div><div class="line">└── conf</div><div class="line">    ├── auth.cert</div><div class="line">    ├── auth.key</div><div class="line">    ├── registry-srv.yml</div><div class="line">    └── registry-web.yml</div><div class="line"></div><div class="line">1 directory, 4 files</div></pre></td></tr></table></figure>
<h4 id="启动registry"><a href="#启动registry" class="headerlink" title="启动registry"></a>启动registry</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># /root/docker-registry-web/conf需要有上述四个文件(auth.cert,auth.key,registry-srv.yml,registry-web.yml)，文件内容见上面</span></div><div class="line"><span class="comment"># 如果配置文件改成了域名，在创建容器时CONTAINER NAME最好与配置文件一致</span></div><div class="line">docker run -v /root/docker-registry-web/conf/registry-srv.yml:/etc/docker/registry/config.yml:ro -v /root/docker-registry-web/conf/auth.cert:/etc/docker/registry/auth.cert:ro -v /data:/var/lib/registry/ -p 5000:5000 --restart=always --name registry-srv <span class="_">-d</span> registry:2.6.2</div></pre></td></tr></table></figure>
<h4 id="启动registry-web"><a href="#启动registry-web" class="headerlink" title="启动registry-web"></a>启动registry-web</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># /root/docker-registry-web/conf需要有上述四个文件(auth.cert,auth.key,registry-srv.yml,registry-web.yml)，文件内容见上面</span></div><div class="line"><span class="comment"># 如果配置文件改成了域名，在创建容器时CONTAINER NAME最好与配置文件一致</span></div><div class="line">docker run <span class="_">-d</span> -v /root/docker-registry-web/conf/registry-web.yml:/conf/config.yml:ro -v /root/docker-registry-web/conf/auth.key:/conf/auth.key -v /root/docker-registry-web/db:/data -it -p 8080:8080 --link registry-srv --restart=always --name registry-web hyper/docker-registry-web</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker images</span></div><div class="line">REPOSITORY                            TAG                 IMAGE ID            CREATED             SIZE</div><div class="line">docker.io/registry                    2.6.2               177391bcf802        2 weeks ago         33.26 MB</div><div class="line">docker.io/hyper/docker-registry-web   latest              0db5683824d8        14 months ago       598.6 MB</div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"><span class="comment"># docker ps -a</span></div><div class="line">CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS                    NAMES</div><div class="line">1b4c0efd8465        hyper/docker-registry-web   <span class="string">"start.sh"</span>               58 minutes ago      Up 58 minutes       0.0.0.0:8080-&gt;8080/tcp   registry-web</div><div class="line">2d475b4d0603        registry:2.6.2              <span class="string">"/entrypoint.sh /etc/"</span>   58 minutes ago      Up 17 minutes       0.0.0.0:5000-&gt;5000/tcp   registry-srv</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="hosts解析"><a href="#hosts解析" class="headerlink" title="hosts解析"></a>hosts解析</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tail -2 /etc/hosts</span></div><div class="line">192.168.1.61 registry-srv</div><div class="line">192.168.1.61 registry-web</div></pre></td></tr></table></figure>
<h4 id="忽略认证"><a href="#忽略认证" class="headerlink" title="忽略认证"></a>忽略认证</h4><p>Question：如果报下面的错误<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker login http://registry-srv:5000</span></div><div class="line">Username (admin): admin</div><div class="line">Password: </div><div class="line">Error response from daemon: Get https://registry-srv:5000/v2/: http: server gave HTTP response to HTTPS client</div></pre></td></tr></table></figure></p>
<p>Answer：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker 1.12.6需要在这里修改</span></div><div class="line"><span class="comment"># grep -iv '^#' /etc/sysconfig/docker | grep -iv '^$'</span></div><div class="line">OPTIONS=<span class="string">'--selinux-enabled --log-driver=journald --signature-verification=false --insecure-registry registry-srv:5000'</span></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;DOCKER_CERT_PATH&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">    DOCKER_CERT_PATH=/etc/docker</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#docker 17.12.0-ce-rc4需要这里修改</span></div><div class="line"><span class="comment"># cat /etc/docker/daemon.json </span></div><div class="line">&#123; <span class="string">"insecure-registries"</span>:[<span class="string">"registry-srv:5000"</span>] &#125;</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># systemctl restart docker</span></div></pre></td></tr></table></figure>
<h4 id="上传镜像到docker-registry"><a href="#上传镜像到docker-registry" class="headerlink" title="上传镜像到docker registry"></a>上传镜像到docker registry</h4><p>Question：在<code>docker 17.12.0-ce-rc4</code>中登陆成功后，如果报下面的错误<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker push registry-srv:5000/mysql:5.6</span></div><div class="line">The push refers to a repository [registry-srv:5000/mysql]</div><div class="line"></div><div class="line">67ab9337620e: Preparing </div><div class="line">388e5e8563d4: Preparing </div><div class="line">000529f48f17: Preparing </div><div class="line">07d0b57bb93e: Preparing </div><div class="line">d59453e8d7bb: Waiting </div><div class="line">19aa284e9bf3: Waiting </div><div class="line">889744378e18: Waiting </div><div class="line">ae12d30e1dfc: Waiting </div><div class="line">4bcdffd70da2: Waiting </div><div class="line">unauthorized: authentication required</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p>Answer：需要登陆registry-web给当前登陆的用户授权<br><img src="https://note.youdao.com/yws/api/personal/file/31325BB5F8004B7EBA8475EEE9EB2D52?method=download&amp;shareKey=187c16347a2a2fc53c43bf618824e074" alt="Registry_Permissions"><br><img src="https://note.youdao.com/yws/api/personal/file/99A6312328FB498FA01065FC943CFED4?method=download&amp;shareKey=0a178de17ed865950e5f3aa12c5220a6" alt="Registry_Roles"><br>然后再登陆<code>docker login http://registry-srv:5000</code>即可成功上传  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker login registry-srv:5000</span></div><div class="line">Username (admin): admin</div><div class="line">Password: </div><div class="line">Login Succeeded</div><div class="line"><span class="comment"># docker tag docker.io/mysql:5.6 registry-srv:5000/mysql:5.6</span></div><div class="line"><span class="comment"># docker images</span></div><div class="line">REPOSITORY                TAG                 IMAGE ID            CREATED             SIZE</div><div class="line">registry-srv:5000/mysql   5.6                 1c7972822e0c        8 days ago          299 MB</div><div class="line">docker push registry-srv:5000/mysql:5.6</div><div class="line">The push refers to a repository [registry-srv:5000/mysql]</div><div class="line">67ab9337620e: Pushed </div><div class="line">388e5e8563d4: Pushed </div><div class="line">000529f48f17: Pushed </div><div class="line">07d0b57bb93e: Pushed </div><div class="line">324a3796c59a: Pushed </div><div class="line">d59453e8d7bb: Pushed </div><div class="line">19aa284e9bf3: Pushed </div><div class="line">889744378e18: Pushed </div><div class="line">ae12d30e1dfc: Pushed </div><div class="line">4bcdffd70da2: Pushed </div><div class="line">5.6: digest: sha256:92<span class="built_in">cd</span>157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd size: 2409</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ls /data/</span></div><div class="line">docker</div><div class="line"><span class="comment"># du -sh /data/</span></div><div class="line">101M	/data/</div><div class="line"><span class="comment"># tree docker/</span></div><div class="line">docker/</div><div class="line">└── registry</div><div class="line">    └── v2</div><div class="line">        ├── blobs</div><div class="line">        │   └── sha256</div><div class="line">        │       ├── 1c</div><div class="line">        │       │   └── 1c7972822e0cfe7af284610af68fd23ab1c6e36566070199a2ecae0c540a6213</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 28</div><div class="line">        │       │   └── 28dd7bab809dc36871733509f298775d6e7e9a7b48411969fd40bbc5d42d4872</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 60</div><div class="line">        │       │   └── 60b597896d30e83b6451b5d287503c6ad5b966afcfe983beaac073<span class="built_in">cd</span>14d3327e</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 67</div><div class="line">        │       │   └── 67ee8c6f60b5ee191862ae0beee2e27<span class="built_in">fc</span>242c1548e724d42491aff9599783f14</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 74</div><div class="line">        │       │   └── 74616d0d8b72cce832e728b721a055ee94112f55d9152ea75c0c11df9255e5fe</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 78</div><div class="line">        │       │   └── 78032de49d65ab1151d278821068401fa7a8964c16b2f4441a3ef9ac8dd02229</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 83</div><div class="line">        │       │   └── 837546b20bc4af04c4<span class="built_in">cd</span>0b34ac6cb74418f0400fa80045d02d341aecbc70f928</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 8b</div><div class="line">        │       │   └── 8b95be8b8d363b4fd0d3de912d206a4a83f9f445e7a0761c61e4225b55aa3f6a</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 92</div><div class="line">        │       │   └── 92<span class="built_in">cd</span>157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd</div><div class="line">        │       │       └── data</div><div class="line">        │       ├── 9b</div><div class="line">        │       │   ├── 9b7ad7dfbf08cb21ae35a041aeceb634a80f6145d371fb793e18c9be75b491ce</div><div class="line">        │       │   │   └── data</div><div class="line">        │       │   └── 9b8316af6cc601a268bccfd58f93c2598e4a5f8a6b101cb9ffe365bcd467cb8e</div><div class="line">        │       │       └── data</div><div class="line">        │       └── f4</div><div class="line">        │           └── f49cf87b52c10aa83b4f4405800527a74400fb19ea1821d209293bc4d53966aa</div><div class="line">        │               └── data</div><div class="line">        └── repositories</div><div class="line">            └── mysql</div><div class="line">                ├── _layers</div><div class="line">                │   └── sha256</div><div class="line">                │       ├── 1c7972822e0cfe7af284610af68fd23ab1c6e36566070199a2ecae0c540a6213</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 28dd7bab809dc36871733509f298775d6e7e9a7b48411969fd40bbc5d42d4872</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 60b597896d30e83b6451b5d287503c6ad5b966afcfe983beaac073<span class="built_in">cd</span>14d3327e</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 67ee8c6f60b5ee191862ae0beee2e27<span class="built_in">fc</span>242c1548e724d42491aff9599783f14</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 74616d0d8b72cce832e728b721a055ee94112f55d9152ea75c0c11df9255e5fe</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 78032de49d65ab1151d278821068401fa7a8964c16b2f4441a3ef9ac8dd02229</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 837546b20bc4af04c4<span class="built_in">cd</span>0b34ac6cb74418f0400fa80045d02d341aecbc70f928</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 8b95be8b8d363b4fd0d3de912d206a4a83f9f445e7a0761c61e4225b55aa3f6a</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 9b7ad7dfbf08cb21ae35a041aeceb634a80f6145d371fb793e18c9be75b491ce</div><div class="line">                │       │   └── link</div><div class="line">                │       ├── 9b8316af6cc601a268bccfd58f93c2598e4a5f8a6b101cb9ffe365bcd467cb8e</div><div class="line">                │       │   └── link</div><div class="line">                │       └── f49cf87b52c10aa83b4f4405800527a74400fb19ea1821d209293bc4d53966aa</div><div class="line">                │           └── link</div><div class="line">                ├── _manifests</div><div class="line">                │   ├── revisions</div><div class="line">                │   │   └── sha256</div><div class="line">                │   │       └── 92<span class="built_in">cd</span>157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd</div><div class="line">                │   │           └── link</div><div class="line">                │   └── tags</div><div class="line">                │       └── 5.6</div><div class="line">                │           ├── current</div><div class="line">                │           │   └── link</div><div class="line">                │           └── index</div><div class="line">                │               └── sha256</div><div class="line">                │                   └── 92<span class="built_in">cd</span>157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd</div><div class="line">                │                       └── link</div><div class="line">                └── _uploads</div><div class="line"></div><div class="line">53 directories, 26 files</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="从docker-registry下载镜像"><a href="#从docker-registry下载镜像" class="headerlink" title="从docker registry下载镜像"></a>从docker registry下载镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker pull registry-srv:5000/mysql:5.6</span></div><div class="line">Trying to pull repository registry-srv:5000/mysql ... </div><div class="line">Get http://registry-srv:5000/v2/mysql/manifests/5.6: unauthorized: authentication required</div><div class="line"><span class="comment"># docker login registry-srv:5000 -uadmin -padmin</span></div><div class="line">Login Succeeded</div><div class="line"><span class="comment"># docker pull registry-srv:5000/mysql:5.6</span></div><div class="line">Trying to pull repository registry-srv:5000/mysql ... </div><div class="line">5.6: Pulling from registry-srv:5000/mysql</div><div class="line">f49cf87b52c1: Pull complete </div><div class="line">78032de49d65: Pull complete </div><div class="line">837546b20bc4: Pull complete </div><div class="line">9b8316af6cc6: Pull complete </div><div class="line">28dd7bab809d: Pull complete </div><div class="line">8b95be8b8d36: Pull complete </div><div class="line">67ee8c6f60b5: Pull complete </div><div class="line">74616d0d8b72: Pull complete </div><div class="line">9b7ad7dfbf08: Pull complete </div><div class="line">60b597896d30: Pull complete </div><div class="line">Digest: sha256:92<span class="built_in">cd</span>157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd</div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># docker images</span></div><div class="line">REPOSITORY                TAG                 IMAGE ID            CREATED             SIZE</div><div class="line">registry-srv:5000/mysql                               5.6                 1c7972822e0c        8 days ago          299 MB</div></pre></td></tr></table></figure>
<h4 id="删除docker-registry仓库镜像"><a href="#删除docker-registry仓库镜像" class="headerlink" title="删除docker registry仓库镜像"></a>删除docker registry仓库镜像</h4><p>在2.4版本中对这一问题进行了解决，增加了一个垃圾回收命令，删除未被引用的层数据，操作如下：  </p>
<ul>
<li>在启动仓库时，需在配置文件中的storage配置中增加delete=true配置项，允许删除镜像  <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#完整配置文件请参考 registry-srv.yml</span></div><div class="line">storage:</div><div class="line">  delete:</div><div class="line">    enabled: <span class="literal">true</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>先在registry-web上执行删除操作<br><img src="https://note.youdao.com/yws/api/personal/file/FB458A3AB2EA4EA5A58C3E0D76B959EF?method=download&amp;shareKey=b1cb0288408e3525dd20fc356e12d91a" alt="register_del"><br><img src="https://note.youdao.com/yws/api/personal/file/EC0B62B8C074467DB81C44B75AF8C940?method=download&amp;shareKey=5501ff8ed5e3e1139116a282c481178c" alt="register_del">  </p>
<p>这时数据并未完全删除，需要执行垃圾回收<br>命令：<code>registry garbage-collect config.yml</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 垃圾回收前</span></div><div class="line"><span class="comment"># du -sh *</span></div><div class="line">101M	docker</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"># docker exec -it registry-srv registry garbage-collect /etc/docker/registry/config.yml</div><div class="line">mysql</div><div class="line"></div><div class="line"><span class="number">0</span> blobs marked, <span class="number">13</span> blobs eligible <span class="keyword">for</span> deletion</div><div class="line">blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">9</span>b8316af6cc601a268bccfd58f93c2598e4a5f8a6b101cb9ffe365bcd467cb8e</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">9</span><span class="keyword">b</span>/<span class="number">9</span>b8316af6cc601a268bccfd58f93c2598e4a5f8a6b101cb9ffe365bcd467cb8e  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">67</span>ee8c6f60b5ee191862ae0beee2e27fc242c1548e724d42491aff9599783f14</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">67</span>/<span class="number">67</span>ee8c6f60b5ee191862ae0beee2e27fc242c1548e724d42491aff9599783f14  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">78032</span>de49d65ab1151d278821068401fa7a8964c16b2f4441a3ef9ac8dd02229</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">78</span>/<span class="number">78032</span>de49d65ab1151d278821068401fa7a8964c16b2f4441a3ef9ac8dd02229  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">60</span>b597896d30e83b6451b5d287503c6ad5b966afcfe983beaac073cd14d3327e</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">60</span>/<span class="number">60</span>b597896d30e83b6451b5d287503c6ad5b966afcfe983beaac073cd14d3327e  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">74616</span>d0d8b72cce832e728b721a055ee94112f55d9152ea75c0c11df9255e5fe</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">74</span>/<span class="number">74616</span>d0d8b72cce832e728b721a055ee94112f55d9152ea75c0c11df9255e5fe  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">837546</span>b20bc4af04c4cd0b34ac6cb74418f0400fa80045d02d341aecbc70f928</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">83</span>/<span class="number">837546</span>b20bc4af04c4cd0b34ac6cb74418f0400fa80045d02d341aecbc70f928  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">8</span>b95be8b8d363b4fd0d3de912d206a4a83f9f445e7a0761c61e4225b55aa3f6a</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">8</span><span class="keyword">b</span>/<span class="number">8</span>b95be8b8d363b4fd0d3de912d206a4a83f9f445e7a0761c61e4225b55aa3f6a  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">92</span>cd157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">92</span>/<span class="number">92</span>cd157a4d73a00a56993bce76d467ae170a86b264d24536648834d7f7501cdd  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">9</span>b7ad7dfbf08cb21ae35a041aeceb634a80f6145d371fb793e18c9be75b491ce</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">9</span><span class="keyword">b</span>/<span class="number">9</span>b7ad7dfbf08cb21ae35a041aeceb634a80f6145d371fb793e18c9be75b491ce  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">1</span>c7972822e0cfe7af284610af68fd23ab1c6e36566070199a2ecae0c540a6213</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">1</span><span class="keyword">c</span>/<span class="number">1</span>c7972822e0cfe7af284610af68fd23ab1c6e36566070199a2ecae0c540a6213  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:<span class="number">28</span>dd7bab809dc36871733509f298775d6e7e9a7b48411969fd40bbc5d42d4872</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/<span class="number">28</span>/<span class="number">28</span>dd7bab809dc36871733509f298775d6e7e9a7b48411969fd40bbc5d42d4872  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/a3/a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430blob eligible <span class="keyword">for</span> deletion: <span class="built_in">sha256</span>:f49cf87b52c10aa83b4f4405800527a74400fb19ea1821d209293bc4d53966aa</div><div class="line">INFO[<span class="number">0000</span>] Deleting blo<span class="variable">b:</span> /docker/registry/v2/blobs/<span class="built_in">sha256</span>/f4/f49cf87b52c10aa83b4f4405800527a74400fb19ea1821d209293bc4d53966aa  <span class="keyword">go</span>.<span class="keyword">version</span>=go1.<span class="number">7.6</span> instance.id=<span class="number">1</span>cb0944e-c80c</div><div class="line">-<span class="number">4111</span>-<span class="number">9758</span>-df3ed7b72430</div><div class="line">#</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 垃圾回收后</span></div><div class="line"><span class="comment"># du -sh *</span></div><div class="line">204K	docker</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<p>注：在执行垃圾回收后，需要重启registry-srv，否则当再次上传相同IMAGE时，将无法成功上传<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># docker restart registry-srv</span></div></pre></td></tr></table></figure></p>
<h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p><img src="https://note.youdao.com/yws/api/personal/file/A798B6B8F24448C8BDDB28FC5613BFA7?method=download&amp;shareKey=b6384831be5d8a0bab732694c1fb4481" alt="register_login"><br><img src="https://note.youdao.com/yws/api/personal/file/9CBFFA18A90544D3A2E96A25B5C5FE2C?method=download&amp;shareKey=7d870422ba75dc1499af97e17cd7f464" alt="register_repositories"><br><img src="https://note.youdao.com/yws/api/personal/file/2FDDDEE2F7B14EAF92DFB78729596EB6?method=download&amp;shareKey=50afa53fa7c84b43e07dd132bb0f8d28" alt="register_tags"><br><img src="https://note.youdao.com/yws/api/personal/file/36A659C4EC7145C6800FCB6704BD2C55?method=download&amp;shareKey=bd67c52c0696a8f00c77a62b2eaed683" alt="register_image">  </p>
<p>参考：<br><a href="https://hub.docker.com/r/library/registry/" target="_blank" rel="external">https://hub.docker.com/r/library/registry/</a><br><a href="https://github.com/mkuchin/docker-registry-web" target="_blank" rel="external">https://github.com/mkuchin/docker-registry-web</a><br><a href="https://hub.docker.com/r/hyper/docker-registry-web/" target="_blank" rel="external">https://hub.docker.com/r/hyper/docker-registry-web/</a><br><a href="http://www.widuu.com/chinese_docker/index.html" target="_blank" rel="external">http://www.widuu.com/chinese_docker/index.html</a>  </p>
<p>附件：<br><a href="https://note.youdao.com/yws/api/personal/file/B44119A67B2644AEA0212B7DF58B88F8?method=download&amp;shareKey=5502a8f7d9e924c901d618ca7466ce83" target="_blank" rel="external">docker-registry-web.tar.gz</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2017/12/20/自建docker私有仓库-Registry/">http://www.yfshare.vip/2017/12/20/自建docker私有仓库-Registry/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;/assets/js/DPlayer.min.js&quot;&gt; &lt;/script&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot;&gt; &lt;/script&gt;&lt;p&gt;Docker Hub 存放着 Docker 及其组件的所有资源。它可以提供
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Docker" scheme="http://www.yfshare.vip/tags/Docker/"/>
    
      <category term="Docker Registry" scheme="http://www.yfshare.vip/tags/Docker-Registry/"/>
    
      <category term="Registry-web" scheme="http://www.yfshare.vip/tags/Registry-web/"/>
    
  </entry>
  
  <entry>
    <title>The Redmine_svn of docker</title>
    <link href="http://www.yfshare.vip/2017/12/14/The-Redmine-svn-of-docker/"/>
    <id>http://www.yfshare.vip/2017/12/14/The-Redmine-svn-of-docker/</id>
    <published>2017-12-14T14:51:08.000Z</published>
    <updated>2019-01-20T08:25:31.831Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p>The redmine + svn + apache2 of Docker 应用基于Debian系统部署，经改造后只需要 PULL+Compose后即可正常工作<br><a id="more"></a></p>
<h2 id="Supported-tags-and-respective-Dockerfile-links"><a href="#Supported-tags-and-respective-Dockerfile-links" class="headerlink" title="Supported tags and respective Dockerfile links"></a>Supported tags and respective Dockerfile links</h2><ul>
<li><code>latest</code>(<a href="https://github.com/yfshare/Docker/blob/master/Redmine_svn/Dockerfile" target="_blank" rel="external">latest/Dockerfile</a>)</li>
<li><code>3.4</code>(<a href="https://github.com/yfshare/Docker/blob/master/Redmine_svn/Dockerfile_3.4" target="_blank" rel="external">3.4/Dockerfile</a>)</li>
</ul>
<p>镜像地址：<a href="https://hub.docker.com/r/yfshare/redmine_svn/" target="_blank" rel="external">https://hub.docker.com/r/yfshare/redmine_svn/</a>  </p>
<h3 id="The-redmine-3-4-svn-apache2-of-Docker"><a href="#The-redmine-3-4-svn-apache2-of-Docker" class="headerlink" title="The redmine 3.4+svn+apache2 of Docker"></a>The redmine 3.4+svn+apache2 of Docker</h3><p>基于Docker hub里redmine 3.4（docker pull redmine:3.4）（<a href="https://hub.docker.com/_/redmine/" target="_blank" rel="external">https://hub.docker.com/_/redmine/</a> ）改造而成。</p>
<h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><p>使用到的环境变量有：  </p>
<ul>
<li><code>MYSQL_ROOT_PASSWORD</code>：MYSQL的ROOT密码，需要和REDMINE_DB_PASSWORD变量值一致</li>
<li><code>MYSQL_DATABASE</code>：MYSQL上创建存储redmine的库名</li>
<li><code>SVN_ROOT</code>：存储SVN REPO的父目录</li>
<li><code>SVN_DIR</code>：SVN REPO项目目录，一般放在变量SVN_ROOT下</li>
<li><code>REDMINE_DB_MYSQL</code>：存储REDMINE的MYSQL DOCKER容器名，建议不要修改；如果修改它同时也需要修改docker-compose</li>
<li><code>REDMINE_DB_PASSWORD</code>：REDMINE连接MYSQL Docker的密码，REDMINE_DB_PASSWORD需要和MYSQL_ROOT_PASSWORD密码一致</li>
<li><code>REDMINE_DB_ENCODING</code>：新建MYSQL库REDMINE的字符集</li>
<li><code>REDMINE_MAIL_ADDRESS</code>：发送REDMINE的邮件地址（configuration.yml）</li>
<li><code>REDMINE_MAIL_DOMAIN</code>：发送REDMINE的邮件域名（configuration.yml）</li>
<li><code>REDMINE_MAIL_USER</code>：发送REDMINE的邮件用户名（configuration.yml）</li>
<li><code>REDMINE_MAIL_PASSWD</code>：发送REDMINE的邮件密码（configuration.yml）</li>
<li>以上变量是在Dockerfile自定义的，如需要其他变量，请参考（<a href="https://hub.docker.com/_/redmine/" target="_blank" rel="external">https://hub.docker.com/_/redmine/</a> ） ，因调用的redmine:3.4，因此也支持它的变量</li>
</ul>
<h4 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cat Dockerfile</span></div><div class="line">FROM docker.io/redmine:3.4</div><div class="line">RUN apt-get update \</div><div class="line">    &amp;&amp; apt-get install -y subversion subversion-tools apache2 libapache2-svn apache2-utils vim \</div><div class="line">    &amp;&amp; useradd apache</div><div class="line"></div><div class="line">RUN <span class="built_in">echo</span> <span class="string">''</span> &gt;/tmp/main</div><div class="line">COPY configure /</div><div class="line"></div><div class="line">VOLUME /usr/src/redmine/files</div><div class="line"></div><div class="line">RUN <span class="built_in">echo</span> <span class="string">"[ ! -d \$&#123;SVN_DIR&#125; ] &amp;&amp; svnadmin create \$&#123;SVN_DIR&#125;"</span> &gt;&gt;/tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"[ -d /Authconf ] &amp;&amp; mv /Authconf \$&#123;SVN_ROOT&#125;/ &amp;&amp; sed -i \"s#^password-db \= passwd#password-db \= \$&#123;SVN_ROOT&#125;\/Authconf\/passwd#g\" \$&#123;SVN_ROOT&#125;/Authconf/svnserve.conf &amp;&amp; sed -i \"s#^authz-db \= authz#authz-db \= \$&#123;SVN_ROOT&#125;\/Authconf\/authz#g\" \$&#123;SVN_ROOT&#125;/Authconf/svnserve.conf &amp;&amp; \mv \$&#123;SVN_ROOT&#125;/Authconf/svnserve.conf \$&#123;SVN_DIR&#125;/conf/"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"[ -f /dav_svn.conf ] &amp;&amp; mv /dav_svn.conf /etc/apache2/mods-enabled/ &amp;&amp; sed -i \"s#SVNPath SVN_DIR#SVNPath \$&#123;SVN_DIR&#125;#g\" /etc/apache2/mods-enabled/dav_svn.conf &amp;&amp; sed -i \"s#Location \/svn#Location \/\`echo \$&#123;SVN_DIR&#125; | awk -F '/' '&#123;print \$NF&#125;'\`#g\" /etc/apache2/mods-enabled/dav_svn.conf &amp;&amp; sed -i \"/Include the virtual host configurations/aServerName localhost\:80\" /etc/apache2/apache2.conf"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"[ -f /svnpass ] &amp;&amp;  mv /svnpass /etc/apache2/.svnpass"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"[ -f /dav_svn.authz ]&amp;&amp; mv /dav_svn.authz /etc/apache2/ &amp;&amp; sed -i \"s#SVN_DIR#\`echo \$&#123;SVN_DIR&#125; | awk -F '/' '&#123;print \$NF&#125;'\`#g\" /etc/apache2/dav_svn.authz"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"[ -f /configuration.yml ] &amp;&amp; \mv /configuration.yml /usr/src/redmine/config &amp;&amp; sed -i \"s#domain\: 'DOMAIN'#domain\: '\$&#123;REDMINE_MAIL_DOMAIN&#125;'#g\" /usr/src/redmine/config/configuration.yml &amp;&amp; sed -i \"s#address\: \\\"MAIL_ADDRESS\\\"#address\: \\\"\$&#123;REDMINE_MAIL_ADDRESS&#125;\\\"#g\" /usr/src/redmine/config/configuration.yml &amp;&amp; sed -i \"s#user_name\: 'MAILADDR'#user_name\: '\$&#123;REDMINE_MAIL_USER&#125;'#g\" /usr/src/redmine/config/configuration.yml &amp;&amp; sed -i \"s#password\: 'PASSWORD'#password\: '\$&#123;REDMINE_MAIL_PASSWD&#125;'#g\" /usr/src/redmine/config/configuration.yml"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"chown apache:apache \$&#123;SVN_ROOT&#125; -R &amp;&amp; chown apache:apache /etc/apache2 -R"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"/etc/init.d/apache2 start &amp;"</span> &gt;&gt; /tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"svnserve -d -r \$&#123;SVN_ROOT&#125; --log-file \$&#123;SVN_ROOT&#125;/svn.log &amp;"</span> &gt;&gt;/tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"sleep 5 &amp;&amp; /docker-entrypoint.sh rails server -b 0.0.0.0 &amp;"</span> &gt;&gt;/tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; <span class="built_in">echo</span> <span class="string">"tail -f /tmp/main"</span> &gt;&gt;/tmp/redmine_svn.sh \</div><div class="line">    &amp;&amp; chmod +x /tmp/redmine_svn.sh</div><div class="line"></div><div class="line">EXPOSE 3000 3690 80</div><div class="line"></div><div class="line">ENTRYPOINT [<span class="string">"/bin/sh"</span>,<span class="string">"/tmp/redmine_svn.sh"</span>]</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure>
<h4 id="Docker-Compose"><a href="#Docker-Compose" class="headerlink" title="Docker Compose"></a>Docker Compose</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">version: <span class="string">'2'</span></div><div class="line">services:</div><div class="line">  db:</div><div class="line">    image: mysql:5.7</div><div class="line">    container_name: mysql</div><div class="line">    environment:</div><div class="line">      MYSQL_ROOT_PASSWORD: <span class="string">'example12'</span></div><div class="line">      MYSQL_DATABASE: redmine</div><div class="line">    ports:</div><div class="line">      - 3306:3306</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/mysql:/var/lib/mysql</div><div class="line">    restart: always</div><div class="line"></div><div class="line">  redmine_svn:</div><div class="line">    image: yfshare/redmine_svn:3.4</div><div class="line">    container_name: redmine_svn</div><div class="line">    ports:</div><div class="line">      - 3000:3000</div><div class="line">      - 3690:3690</div><div class="line">      - 80:80</div><div class="line">    links:</div><div class="line">      - db</div><div class="line">    environment:</div><div class="line">      SVN_ROOT: <span class="string">'/usr/src/SvnRepos'</span></div><div class="line">      SVN_DIR: <span class="string">'/usr/src/SvnRepos/Soros'</span></div><div class="line">      REDMINE_DB_MYSQL: db</div><div class="line">      REDMINE_DB_PASSWORD: <span class="string">'example12'</span></div><div class="line">      REDMINE_DB_ENCODING: <span class="string">'utf8'</span></div><div class="line">      REDMINE_MAIL_ADDRESS: <span class="string">'smtp.exmail.qq.com'</span></div><div class="line">      REDMINE_MAIL_DOMAIN: <span class="string">'example.com'</span></div><div class="line">      REDMINE_MAIL_USER: <span class="string">'username'</span></div><div class="line">      REDMINE_MAIL_PASSWD: <span class="string">'password'</span></div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/redmine/files:/usr/src/redmine/files</div><div class="line">      - /data/docker_mount/SvnRepos:/usr/src/SvnRepos</div><div class="line">    restart: always</div></pre></td></tr></table></figure>
<h4 id="部署Docker"><a href="#部署Docker" class="headerlink" title="部署Docker"></a>部署Docker</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker pull yfshare/redmine_svn</div><div class="line">docker-compose <span class="_">-f</span> docker-redmine_svn_mysql.yml up <span class="_">-d</span></div></pre></td></tr></table></figure>
<h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><ul>
<li><p>在redmine载入默认配置时，请选择“English“，而不是”简体中文“ 否则报下面错误  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">无法载入默认设置：Mysql2::Error: Incorrect string value: <span class="string">'\xE7\xAE\xA1\xE7\x90\x86...'</span> <span class="keyword">for</span> column <span class="string">'name'</span> at row 1: INSERT INTO `roles` (`name`, `issues_visibility`, `position`) VALUES (<span class="string">'管理人员'</span>, <span class="string">'all'</span>, 1)</div></pre></td></tr></table></figure>
</li>
<li><p>redmine默认用户名密码为：<code>admin</code> / <code>admin</code></p>
</li>
<li>TortoiseSVN访问SVN默认用户名密码为：<code>svnadmin</code> / <code>administrator#!001</code></li>
<li>http访问SVN默认用户名密码为：<code>svnadmin</code> / <code>administrator#!001</code></li>
</ul>
<h3 id="The-redmine-latest-svn-apache2-of-Docker"><a href="#The-redmine-latest-svn-apache2-of-Docker" class="headerlink" title="The redmine latest+svn+apache2 of Docker"></a>The redmine latest+svn+apache2 of Docker</h3><h4 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h4><ul>
<li>其他与The redmine 3.4+svn+apache2 of Docker相同，不同的是在build时取消了FROM docker.io/redmine版本号，此时会拉取最新的版本，同时docker-compose也要去掉版本号即可</li>
</ul>
<h4 id="Docker-Compose-1"><a href="#Docker-Compose-1" class="headerlink" title="Docker Compose"></a>Docker Compose</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">version: <span class="string">'2'</span></div><div class="line">services:</div><div class="line">  db:</div><div class="line">    image: mysql</div><div class="line">    container_name: mysql</div><div class="line">    environment:</div><div class="line">      MYSQL_ROOT_PASSWORD: <span class="string">'example12'</span></div><div class="line">      MYSQL_DATABASE: redmine</div><div class="line">    ports:</div><div class="line">      - 3306:3306</div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/mysql:/var/lib/mysql</div><div class="line">    restart: always</div><div class="line"></div><div class="line">  redmine_svn:</div><div class="line">    image: yfshare/redmine_svn</div><div class="line">    container_name: redmine_svn</div><div class="line">    ports:</div><div class="line">      - 3000:3000</div><div class="line">      - 3690:3690</div><div class="line">      - 80:80</div><div class="line">    links:</div><div class="line">      - db</div><div class="line">    environment:</div><div class="line">      SVN_ROOT: <span class="string">'/usr/src/SvnRepos'</span></div><div class="line">      SVN_DIR: <span class="string">'/usr/src/SvnRepos/Soros'</span></div><div class="line">      REDMINE_DB_MYSQL: db</div><div class="line">      REDMINE_DB_PASSWORD: <span class="string">'example12'</span></div><div class="line">      REDMINE_DB_ENCODING: <span class="string">'utf8'</span></div><div class="line">      REDMINE_MAIL_ADDRESS: <span class="string">'smtp.exmail.qq.com'</span></div><div class="line">      REDMINE_MAIL_DOMAIN: <span class="string">'example.com'</span></div><div class="line">      REDMINE_MAIL_USER: <span class="string">'username'</span></div><div class="line">      REDMINE_MAIL_PASSWD: <span class="string">'password'</span></div><div class="line">    volumes:</div><div class="line">      - /data/docker_mount/redmine/files:/usr/src/redmine/files</div><div class="line">      - /data/docker_mount/SvnRepos:/usr/src/SvnRepos</div><div class="line">    restart: always</div></pre></td></tr></table></figure>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#编译Dockerfile生成Docker images</span></div><div class="line">docker build -t yfshare/redmine_svn <span class="_">-f</span> Dockerfile .</div><div class="line"></div><div class="line"><span class="comment">#拉取镜像</span></div><div class="line">docker pull yfshare/redmine_svn</div><div class="line"></div><div class="line"><span class="comment">#使用docker-compose 生成docker容器</span></div><div class="line">docker-compose <span class="_">-f</span> docker-redmine_svn_mysql.yml up <span class="_">-d</span></div></pre></td></tr></table></figure>
<p>然后</p>
<ul>
<li>通过 <code>http://localhost:3000</code>访问Redmine</li>
<li><code>http://localhost/SvnRepos</code>通过WEB界面访问SVN</li>
<li>通过 TortoiseSVN <code>svn://localhost:3690/SvnRepos</code> 访问SVN</li>
<li>通过<code>3306</code>端口访问mysql  </li>
</ul>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2017/12/14/The-Redmine-svn-of-docker/">http://www.yfshare.vip/2017/12/14/The-Redmine-svn-of-docker/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The redmine + svn + apache2 of Docker 应用基于Debian系统部署，经改造后只需要 PULL+Compose后即可正常工作&lt;br&gt;
    
    </summary>
    
      <category term="K8S" scheme="http://www.yfshare.vip/categories/K8S/"/>
    
    
      <category term="Mysql" scheme="http://www.yfshare.vip/tags/Mysql/"/>
    
      <category term="Docker" scheme="http://www.yfshare.vip/tags/Docker/"/>
    
      <category term="Redmine" scheme="http://www.yfshare.vip/tags/Redmine/"/>
    
      <category term="Svn" scheme="http://www.yfshare.vip/tags/Svn/"/>
    
      <category term="Apache" scheme="http://www.yfshare.vip/tags/Apache/"/>
    
  </entry>
  
  <entry>
    <title>关于kernel: nf_conntrack: table full, dropping packet问题</title>
    <link href="http://www.yfshare.vip/2017/12/05/%E5%85%B3%E4%BA%8Ekernel-nf-conntrack-table-full-dropping-packet%E9%97%AE%E9%A2%98/"/>
    <id>http://www.yfshare.vip/2017/12/05/关于kernel-nf-conntrack-table-full-dropping-packet问题/</id>
    <published>2017-12-05T14:01:25.000Z</published>
    <updated>2017-12-05T14:06:57.191Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/DPlayer.min.js"> </script><script src="/assets/js/APlayer.min.js"> </script><p><code>/var/log/messages</code>报错信息：<code>kernel: nf_conntrack: table full, dropping packet</code><br><a id="more"></a><br>问题背景：公司线上活动（可以理解为秒杀）瞬时流量比平时高好几倍，目前最高瞬时流量为25W/秒(requests)，最低时瞬时流量为6W/秒(requests)<br>而<code>/var/log/messages</code>日志一直报上述错误，直到活动结束  </p>
<p>当时，深刻了解到iptables的这个报错，会造成拒绝服务的问题。查资料后，需要关闭iptables，但服务器在阿里云VPC网络内，使用的是阿里云的安全组策略，故iptables没有打开。后排查，虽iptables和firewalld没开，但是Centos7 还是加载了这个<code>nf_conntrack</code>模块，而导致出现了上面这个问题。  </p>
<p>网上说有三种方法，分别为：  </p>
<ul>
<li>修改参数法</li>
<li>使用RAW表，跳过记录法</li>
<li>移除模块法</li>
</ul>
<p>因服务器没有使用iptables和firewalld，故采用的是<code>移除模块法</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># /sbin/lsmod | egrep 'ip_tables|conntrack'</span></div><div class="line">nf_conntrack_ipv4      19108  2 </div><div class="line">nf_defrag_ipv4         12729  1 nf_conntrack_ipv4</div><div class="line">xt_conntrack           12760  1 </div><div class="line">nf_conntrack          111302  5 nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_ipv4</div><div class="line">ip_tables              27115  2 iptable_filter,iptable_nat</div><div class="line"><span class="comment">#</span></div></pre></td></tr></table></figure></p>
<p><code>state</code>模块和<code>nf_conntrack</code>之间是有依赖关系，想要卸载<code>nf_conntrack</code>模块的话，必须也要把<code>state</code>模块移除，不然，其会自动启用nf_conntrack模块。  </p>
<p>卸载模块前需要先关闭iptables和firewalld，如果发现firewalld或iptables没有启用，但是卸载模块时，还是提示模块正在使用，则把iptables或firewalld启动后再关闭，再尝试卸载模块。  </p>
<p>操作方法如下：  </p>
<ul>
<li>先将/etc/sysconfig/iptables 中包含state的语句移除，并restart iptables</li>
<li>执行语句<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#这里是参考文档作者的</span></div><div class="line">modprobe -r xt_NOTRACK nf_conntrack_netbios_ns nf_conntrack_ipv4 xt_state</div><div class="line">modprobe -r nf_conntrack</div><div class="line"></div><div class="line"><span class="comment">#这里是我实际卸载的模块</span></div><div class="line">modprobe -r nf_conntrack_netbios_ns xt_state</div><div class="line">modprobe -r nf_conntrack</div></pre></td></tr></table></figure>
</li>
</ul>
<p>执行完查看/proc/net/ 下面如果没用了 nf_conntrack ，就证明模块移除成功了  </p>
<p>总结（以下摘录原文作者）：<br>以上三种方法种，如果像<code>web这样的操作访问量并发不大</code>的情况下，建议通过第一种方法实现。因为nf_conntrack模块的作用不仅仅只用于记录状态，iptables还可以通过对该模块的使有达到动态过滤的作用。如我在用ab动测试的一台服务器上进行并发模拟时，在/var/log/message里发现如下的日志<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Apr 22 15:21:46 localhost kernel: possible SYN flooding on port 80. Sending cookies.</div><div class="line">Apr 22 15:22:46 localhost kernel: possible SYN flooding on port 80. Sending cookies.</div></pre></td></tr></table></figure></p>
<p>而此时iptables会智能的将发动SYN flood攻击的IP暂时拒绝掉<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ab -c 500 -n 5000 "http://192.168.10.177/"</span></div><div class="line">This is ApacheBench, Version 2.0.40-dev &lt;<span class="variable">$Revision</span>: 1.146 $&gt; apache-2.0</div><div class="line">Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/</div><div class="line">Copyright 2006 The Apache Software Foundation, http://www.apache.org/</div><div class="line">Benchmarking 192.168.10.177 (be patient)</div><div class="line">apr_socket_recv: Connection reset by peer (104)</div><div class="line">Total of 68 requests completed</div></pre></td></tr></table></figure></p>
<p>如上所以，我用ab操作时，其就会收到apr_socket_recv 的错误提示 。我在网上查询到其具体实现的原理如下：  </p>
<ul>
<li><p>传统的防火墙只能进行静态过滤，而 iptables 除了这个基本的功能之外还可以进行动态过滤，即可以对连接状态进行跟踪，通常称为 conntrack 。 但这不意味着它只能对 TCP 这样的面向连接的协议有效，它还可以对 UDP， ICMP 这种无连接的协议进行跟踪。  </p>
</li>
<li><p>iptables 中的连接跟踪是通过 state 模块来实现的，是在PREROUTING 链中完成的，除了本地主机产生的数据包，它们是在 OUTPUT 链中完成。 它把“连接”划分为四种状态：NEW， ESTABLISHED， RELATED 和 INVALID。连接跟踪当前的所有连接状态可以通过 /proc/net/nf_conntrack 来查看（注意，在一些稍微旧的 Linux 系统上是 /proc/net/ip_conntrack）  </p>
</li>
<li>当 conntrack 第一次看到相关的数据包时，就会把状态标记为 NEW ，比如 TCP 协议中收到第一个 SYN 数据包。当连接的双方都有数据包收发并且还将继续匹配到这些数据包时，连接状态就会变为 ESTABLISHED 。而 RELATED 状态是指一个新的连接，但这个连接和某个已知的连接有关系，比如 FTP 协议中的数据传输连接。INVALID 状态是说数据包和已知的任何连接都不匹配</li>
</ul>
<p>当然，仅仅利用iptables conntrack自动实现syn flood 等DDOS攻击时很弱的。而现成的动态过滤和DDOS防护的方法是很多的。比如netstat脚本实现，iptalbes限制每秒进行连接数，nginx/apache的连接数限制模块及fail2ban日志分析法………… ，所以在具有以上防护的情况下，非常推荐将web 、squid/varnish等应用所在的服务器配置为RAW方式 。我在现网一台150M/S 的cache server上将80和3128两个端口全部NOTRACK之后，conntrack hash表由瞬满直线下降到只有几百条。<br>最后，最不推荐使用的第三种方法，因为第三种方法会将state模块也一块儿移除掉。  </p>
<p>参考：<br><a href="http://www.361way.com/ip_conntrack-tablefull/1717.html" target="_blank" rel="external">http://www.361way.com/ip_conntrack-tablefull/1717.html</a><br><a href="http://www.361way.com/%E5%86%8D%E7%9C%8Bnf_conntrack-table-full%E9%97%AE%E9%A2%98/2404.html" target="_blank" rel="external">http://www.361way.com/%E5%86%8D%E7%9C%8Bnf_conntrack-table-full%E9%97%AE%E9%A2%98/2404.html</a><br><a href="http://jaseywang.me/2012/08/16/%E8%A7%A3%E5%86%B3-nf_conntrack-table-full-dropping-packet-%E7%9A%84%E5%87%A0%E7%A7%8D%E6%80%9D%E8%B7%AF/" target="_blank" rel="external">http://jaseywang.me/2012/08/16/%E8%A7%A3%E5%86%B3-nf_conntrack-table-full-dropping-packet-%E7%9A%84%E5%87%A0%E7%A7%8D%E6%80%9D%E8%B7%AF/</a><br><a href="https://wiki.khnet.info/index.php/Conntrack_tuning" target="_blank" rel="external">https://wiki.khnet.info/index.php/Conntrack_tuning</a><br><a href="http://blog.zol.com.cn/2608/article_2607945.html" target="_blank" rel="external">http://blog.zol.com.cn/2608/article_2607945.html</a><br><a href="http://blog.csdn.net/dog250/article/details/7262619" target="_blank" rel="external">http://blog.csdn.net/dog250/article/details/7262619</a>  </p>
<hr>
<p>本作品采用<a href="https://creativecommons.org/licenses/by/2.5/cn/" target="_blank" rel="external">知识共享署名 2.5 中国大陆许可协议</a>进行许可，欢迎转载，但转载请注明来自<a href="http://www.yfshare.vip">Jack Wang Blog</a>，并保持转载后文章内容的完整。本人保留所有版权相关权利。<br><img src="http://note.youdao.com/yws/api/personal/file/FE3C6F68961F4541AF284E5F346FC3CA?method=download&amp;shareKey=c5e4c3ca81daa6e908d5630f8c6ec242" alt="打赏"><br>本文出自”Jack Wang Blog”：<a href="http://www.yfshare.vip/2017/12/05/关于kernel-nf-conntrack-table-full-dropping-packet问题/">http://www.yfshare.vip/2017/12/05/关于kernel-nf-conntrack-table-full-dropping-packet问题/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;/var/log/messages&lt;/code&gt;报错信息：&lt;code&gt;kernel: nf_conntrack: table full, dropping packet&lt;/code&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://www.yfshare.vip/categories/Linux/"/>
    
    
      <category term="Kernel" scheme="http://www.yfshare.vip/tags/Kernel/"/>
    
      <category term="nf_conntrack" scheme="http://www.yfshare.vip/tags/nf-conntrack/"/>
    
      <category term="dropping packet" scheme="http://www.yfshare.vip/tags/dropping-packet/"/>
    
  </entry>
  
</feed>
